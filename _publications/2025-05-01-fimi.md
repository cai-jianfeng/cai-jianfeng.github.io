---
title: "Bias Fitting to Mitigate Length Bias of Reward Model in RLHF"
collection: publications
state: under review
permalink: /publication/2025-05-01-fimi
excerpt: 'To accurately model the intricate nature of length bias and facilitate more effective bias mitigation, it proposes FiMi-RM (Bias Fitting to Mitigate Length Bias of Reward Model in RLHF), a framework that autonomously learns and corrects underlying bias patterns.'
date: 2025-05-01
venue: 'Neural Information Processing Systems'
paperurl: 'https://arxiv.org/abs/2505.12843'
citation: 'Zhao Kangwen, Cai Jianfeng. (2025). &quot;Bias Fitting to Mitigate Length Bias of Reward Model in RLHF.&quot; <i>arXiv preprint arXiv: 2505.12843, 2025</i>.'
---
<p style="text-align:justify; text-justify:inter-ideograph;">Reinforcement Learning from Human Feedback relies on reward models to align large language models with human preferences. However, RLHF often suffers from reward hacking, wherein policy learning exploits flaws in the trained reward model to maximize reward scores without genuinely aligning with human preferences. A significant example of such reward hacking is length bias, where reward models usually favor longer responses irrespective of actual response quality. Previous works on length bias have notable limitations, these approaches either mitigate bias without characterizing the bias form, or simply assume a linear length-reward relation. To accurately model the intricate nature of length bias and facilitate more effective bias mitigation, we propose FiMi-RM (Bias Fitting to Mitigate Length Bias of Reward Model in RLHF), a framework that autonomously learns and corrects underlying bias patterns. Our approach consists of three stages: First, we train a standard reward model which inherently contains length bias. Next, we deploy a lightweight fitting model to explicitly capture the non-linear relation between length and reward. Finally, we incorporate this learned relation into the reward model to debias. Experimental results demonstrate that FiMi-RM achieves a more balanced length-reward distribution. Furthermore, when applied to alignment algorithms, our debiased reward model improves length-controlled win rate and reduces verbosity without compromising its performance.</p>

[Download paper from here](https://arxiv.org/abs/2505.12843)

<p style="text-align:justify; text-justify:inter-ideograph;">Zhao Kangwen, Cai Jianfeng. (2025). &quot;Bias Fitting to Mitigate Length Bias of Reward Model in RLHF.&quot; <i>arXiv preprint arXiv: 2505.12843, 2025</i>.</p>