---
title: 'The Basic Knowledge of RLHF Training Pipeline'
date: 25-05-26
update: 25-06-07
permalink: /posts/2025/05/blog-rlhf-train-pipeline/
star: superior
tags:
  - 深度学习基本知识
---

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;">这篇博客主要讲解 RLHF 具体训练的框架 (DeepSpeedChat，OpenRLHF，verl) 的具体细节，包括每个框架的整体架构，架构内的各部分细节 (包括逻辑细节和代码细节)。(建议先阅读我之前关于 RLHF 的博客 <a href="https://cai-jianfeng.github.io/posts/2024/04/blog-rlhf/" target="_blank">The Basic Knowledge of RLHF (Reinforce Learning with Human Feedback)</a>)</p>

<h1 id="RLHF pipeline">RLHF 的算法流程</h1>

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;">在<a href="https://cai-jianfeng.github.io/posts/2024/04/blog-rlhf/" target="_blank">之前的博客</a>中，我们讲解了 RLHF 的三个阶段：SFT (预训练 LLM 模型 $M_\theta$)，Reward Modeling (预训练奖励模型 $r_\theta$) 和最后的 RL 训练 (使用 PPO 微调 $M_\theta$)。对于前两个阶段而言，其只存在一个模型，因此可以使用 Deepspeed，FSDP，Megatron，甚至是 Transformers 的内置 Trainer 等<b>单模型</b>训练框架直接进行分布式训练 (关于单模型训练框架，可以参考我<a href="https://cai-jianfeng.github.io/posts/2025/06/blog-distributed-train-pipeline/" target="_blank">之后的博客</a>)。对于第三个阶段而言，其包含多个模型，同时不同模型的作用也不尽相同 (例如，reference model 和 reward model 只用于 infer，policy model 和 value model 用于 train，同时 policy model 还用于 rollout)。因此，需要在 Deepspeed/FSDP/Megatron 这种单模型的训练框架上再进行进一步的搭建以构建<b>多模型</b>训练框架。因此，本文的所有 RLHF 框架其实主要是聚焦于构建第三阶段的多模型训练框架。在下面的讲解中，我将按照目前主流的描述将 policy model 称为 actor model，将 value model 称为 critic model，将 reference model 简称为 ref model。</p>

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;"><span style="color: gray;">题外话：infer 指的是使用 model 进行一次的 forward，例如使用 reward model 输入 prompt + response 只用一次 forward 就能得到 reward；train 指的是 model 还需要进行训练；rollout 指的是 model 需要根据给定的 prompt 进行多次 forward 来生成 response，即 LLM generate。由于 train 和 rollout 的不同，目前大家分别为它们构建了不同的框架，如 train 有 Deepspeed/FSDP/Megatron 等<b>训练引擎</b>，其计算精度较高，但是由于增加额外通信等问题导致速度较慢，主打一个如何增加较少的额外计算/通信使得 model 可以训练，即以时间换空间 (flash attention 除外)；rollout 有 vllm/sglang 等<b>推理引擎</b>，其速度较快，但是计算损失较大,主要通过 kv cache，融合算子等操作来减少计算时间，即以空间换时间。而对于 infer，由于训练引擎和推理引擎都可以胜任，一般为了保证计算精度会使用训练引擎 (可以参考<a href="https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/927d54fbbbf84ae9e7109cf58c279ff959afcb46/rlhf/verl/readme.md?plain=1#L50">这篇博客</a>的分析)。</span></p>

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;">下面，我们以 PPO 为例来了解每个框架的整体架构和每个部分的具体模块 (其他的 RL 算法如 GRPO，REINFORCE++ 等基本上都是在 PPO 的基础上减少某些模块)。首先，如图 <a href="#fig-ppo-pipeline">1</a> 所示，我们先逻辑化整理一下 PPO 的算法流程：</p>

<!-- ![ppo pipeline](/images/PPO_gen_and_learn.png) -->
<figure id="fig-ppo-pipeline">
  <img src="/images/PPO_gen_and_learn.svg" alt="ppo pipeline" style="width:100%">
  <figcaption>图 1：PPO 的生成与训练阶段 (其中<span style="color: red;">红色箭头</span>表示逻辑流；<span style="color: #CCCC00;">黄色模块</span>表示计算模块，计算模块需按照红色箭头顺序执行)</figcaption>
</figure>

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;"><b>A. PPO 的生成阶段：</b>即通过给定的输入 prompt，生成一系列 PPO 所训练的必要的元素，在经典 RL 中也被称作环境交互。</p>

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;">1. 给定 SFT 后的 model，将其复制为 ref model $\pi_{SFT}$ 和需要进一步训练的 actor model $\pi_{RL}$；给定 Reward Modeling 后的 model，将其复制为 reward model $R$ 和 critic model $V$。</p>

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;">2. 给定 prompt $x$，将其输入给 actor model $\pi_{RL}$ 生成对应的 response $y$，得到完整的 sequence $x + y$。(<span style="color: red;">$\pi_{RL}$ rollout</span>)</p>

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;">3. 给定 sequence $x + y$，将其输入给 actor model $\pi_{RL}$ 和 ref model $\pi_{SFT}$ 分别生成 action logits $p_{RL}$ 和 sft logits $p_{SFT}$，并进一步计算 KL divergence $KL$。(<span style="color: red;">$\pi_{RL}$ 和 $\pi_{SFT}$ infer</span>)</p>

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;">4. 给定 sequence $x + y$，将其输入给 reward model $R$ 和 critic model $V$ 分别生成 reward $r$ 和 value $v$。(<span style="color: red;">$R$ 和 $V$ infer</span>)</p>

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;">5. 给定 $KL$ 和 $r$，计算得到 PPO 的 return；并通过给定 $v$，计算得到 PPO 的 advantage $A$。</p>

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;"><b>B. PPO 的训练阶段：</b>即通过 PPO 的生成阶段所得到的元素，进行 PPO 的训练，在经典 RL 中也被称作奖励学习。由于 PPO 的生成阶段的时间成本较高，因此通常对生成阶段得到的元素进行缓存，并进行多次训练。对于第 $t$ 次训练，其具体流程如下：</p>

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;">1. 给定 sequence $x + y$，将其输入给第 $t-1$ 次训练完的 actor model $\pi^t_{RL}$ 生成 new action logits $p^{t}_{RL}$，并和 action logits $p_{RL}$ 计算 ratio $r^{t}(\theta)$。接着和给定的 advantage $A$ 计算 actor loss 用于 actor model 的训练。(<span style="color: red;">$\pi_{RL}$ train</span>)</p>

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;">2. 给定 sequence $x + y$，将其输入给第 $t-1$ 次训练完的 critic model $V^t$ 生成 new value $v^{t}$，并和 value $v$ 计算 clipped value $v_{clip}$。接着和给定的 return 计算 critic loss 用于 critic model 的训练。(<span style="color: red;">$V$ train</span>)</p>

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;"><span style="color: gray;">题外话：通过上述的步骤不难发现需要在 PPO 生成阶段缓存的元素 (用于 PPO 训练阶段的多次训练) 包括：生成的 sequence $x + y$，action logits $p_{RL}$，value $v$，advantage $A$ 以及 return；但是为了避免后续算法有额外的需求，一般会将 sft logits $p_{SFT}$，KL divergence $KL$ 以及 reward $r$ 一起缓存。</span></p>

<h1 id="DeepSpeedChat pipeline">DeepSpeedChat</h1>

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;">可以看到，上述的流程涉及到 actor model $\pi_{RL}$ 的 rollout，actor model $\pi_{RL}$ 和 ref model $\pi_{SFT}$ 的 infer，reward model $R$ 和 critic model $V$ 的 infer，以及 actor model $\pi_{RL}$ 和 critic model $V$ 的 train。<b>最直接的实现方式是，按照上述流程的逻辑编写 PPO 训练的架构，通过简单扩展单模型训练框架得到多模型训练框架。</b>如图 <a href="#fig-deepspeedchat-pipeline">2</a> 所示，DeepSpeedChat 就是按照这种思路扩展 DeepSpeed 框架来实现 PPO 的训练的。(下面讲解的 DeepSpeedChat 的版本为 bd47e5bc38d292f44bf183e7bda992cde36a769b)</p>

<!-- <figure id="fig-deepspeedchat-pipeline">
  <embed src="/images/deepspeedchat.pdf" type="application/pdf" width="100%" height="600px">
  <figcaption>图 2：DeepSpeedChat 的 PPO 训练框架</figcaption>
</figure> -->

<!-- <figure id="fig-deepspeedchat-pipeline">
  <object data="/images/deepspeedchat.pdf" type="application/pdf" width="100%" height="600px">
    <p>您的浏览器不支持 PDF 显示。您可以 <a href="/images/deepspeedchat.pdf">下载 PDF</a> 查看。</p>
  </object>
  <figcaption>图 2：DeepSpeedChat 的 PPO 训练框架</figcaption>
</figure> -->

<figure id="fig-deepspeedchat-pipeline">
  <img src="/images/deepspeedchat.svg" alt="deepspeedchat pipeline" style="width:100%">
  <figcaption>图 2：DeepSpeedChat 的 PPO 训练框架 (其中<span style="color: red;">红色箭头</span>表示代码执行的顺序；<span style="color: black;">黑色箭头</span>表示模块的扩展描述；<span style="color: green;">绿色模块</span>表示 main.py 内的代码模块；<span style="color: #CCCC00;">黄色模块</span>表示其他文件内的代码模块)</figcaption>
</figure>

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;">接下来，我们讲解 <a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/" target="_blank">DeepSpeedChat</a> 的每个模块的逻辑和代码细节：</p>

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;">与<a href="https://cai-jianfeng.github.io/posts/2025/06/blog-distributed-train-pipeline/" target="_blank">之后的博客</a>中讲解的 DeepSpeed 的分布式训练一致，DeepSpeedChat 通过 <a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/training_scripts/llama2/run_llama2_7b.sh#L27" target="_blank">deepspeed</a> 命令启动分布式训练，并在每张卡上运行 <a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py" target="_blank">main.py</a> 文件。</p>

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;">1. 第一阶段：在 main.py 文件中，首先是使用 <code style="color: #B58900">deepspeed.init_distributed()</code> <a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py#L449-L457" target="_blank">初始化分布式环境</a>，<code style="color: #B58900">load_hf_tokenizer()</code> <a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py#L471-L475" target="_blank">加载 tokenizer</a>，<code style="color: #B58900">create_datasets()</code> <a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py#L477-L478" target="_blank">加载数据集</a>。接着，<a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py#L481-L486" target="_blank">初始化</a> <code style="color: #B58900"><a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/rlhf/rlhf_engine.py#L40" target="_blank">DeepSpeedRLHFEngine</a></code>，包括初始化 <a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/rlhf/rlhf_engine.py#L48-L49" target="_blank">actor model</a>，<a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/rlhf/rlhf_engine.py#L50-L51" target="_blank">ref model</a>，<a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/rlhf/rlhf_engine.py#L58-L59" target="_blank">reward model</a> 和 <a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/rlhf/rlhf_engine.py#L56-L57" target="_blank">value model</a>，其中 actor model 和 critic model 是通过 <code style="color: #B58900"><a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/utils/ds_utils.py#L13-L75" target="_blank">get_train_ds_config()</a></code> 获取 train 的 config 实现 DeepSpeedEngine 的初始化，而 ref model 和 critic model 是通过 <code style="color: #B58900"><a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/utils/ds_utils.py#L78-L105" target="_blank">get_eval_ds_config()</a></code> 获取 infer 的 config 实现 DeepSpeedEngine 的初始化。具体而言，以 actor model 初始化为例，其是调用 DeepSpeedRLHFEngine 的 <code style="color: #B58900"><a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/rlhf/rlhf_engine.py#L63" target="_blank">_init_actor()</a></code> 方法，该方法主要包括通过 <code style="color: #B58900"><a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/utils/ds_utils.py#L13-L75" target="_blank">get_train_ds_config()</a></code> 来<a href="applications/DeepSpeed-Chat/dschat/rlhf/rlhf_engine.py#L67-L81" target="_blank">获取</a> <code style="color: #B58900">ds_config</code>；通过 <code style="color: #B58900"><a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/utils/model/model_utils.py#L85" target="_blank">create_hf_model()</a></code> 来<a href="applications/DeepSpeed-Chat/dschat/rlhf/rlhf_engine.py#L90-L95" target="_blank">加载</a> HF 格式的 <code style="color: #B58900">actor_model</code>；构建 <a href="applications/DeepSpeed-Chat/dschat/rlhf/rlhf_engine.py#L108-L114" target="_blank">optimizer</a> 和 <a href="applications/DeepSpeed-Chat/dschat/rlhf/rlhf_engine.py#L117-L122" target="_blank">lr scheduler</a>；最后使用 <code style="color: #B58900"><a href="applications/DeepSpeed-Chat/dschat/rlhf/rlhf_engine.py#L26-L129" target="_blank">deepspeed.initialize()</a></code> 将三者封装为 <code style="color: #B58900">DeepSpeedEngine</code>。然后，初始化 <a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py#L495-L496" target="_blank">ppo_trainer</a>，这是一个 <code style="color: #B58900"><a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py#L44" target="_blank">DeepSpeedPPOTrainer</a></code> 训练类，里面包含了所有关于 PPO 计算模块的函数。其<a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py#L46-L72" target="_blank">初始化的过程</a>就是简单的对 PPO 所需的各个 model 和系数等进行赋值。最后，初始化 <a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py#L495-L496" target="_blank">exp_mini_dataset</a>，其是一个 <code style="color: #B58900"><a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py#L486" target="_blank">MiniDataset</a></code> 类，用于缓存后续 PPO 生成阶段的结果，并提供给 PPO 训练阶段使用。</p>

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;">2. 第二阶段：在完成初始化和数据的准备后，下一步便开始 PPO 的生成和训练阶段。<a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py#L528-L530" target="_blank">PPO 的生成阶段</a> 主要由 <code style="color: #B58900"><a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py#L137" target="_blank">DeepSpeedPPOTrainer.generate_experience()</a></code> 实现：通过给定准备好的 <a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py#L520" target="_blank">prompt</a>，首先使用 <code style="color: #B58900"><a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py#L137" target="_blank">DeepSpeedPPOTrainer._generate_sequence()</a></code> 来<a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py#L140" target="_blank">生成</a> response，其内部调用了最原始的 <code style="color: #B58900"><a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py#L86-L93" target="_blank">model.generate()</a></code> 的方式。接着，将生成好的 sequence (prompt + response) 输入给 actor model <a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py#L153" target="_blank">生成</a> $p_{RL}$，输入给 ref model <a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py#L154" target="_blank">生成</a> $p_{SFT}$，输入给 reward model <a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py#L155-L158" target="_blank">生成</a> $r$，最后输入给 critic model <a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py#L159-L160" target="_blank">生成</a> $v$，最终将生成的所有结果通过 <code style="color: #B58900"><a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py#L86-L93" target="_blank">add()</a></code> <a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py#L540" target="_blank">缓存</a>到 <code style="color: #B58900">MiniDataset</code> 中。</p>

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;">3. 第三阶段：完成 PPO 的生成阶段，便是 PPO 的训练阶段，其主要是通过 <code style="color: #B58900"><a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py#L196" target="_blank">DeepSpeedPPOTrainer.train_rlhf()</a></code> 实现。可以发现，为了更好地利用 PPO 生成阶段所生成的结果，一般会使用其进行<a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py#L550" target="_blank">多次训练</a>，对应代码中的 <code style="color: #B58900">args.ppo_epochs</code>。对于<a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py#L553" target="_blank">第 $t$ 次训练</a>，首先是根据 PPO 生成阶段的 $p_{RL}$，$p_{SFT}$ 和 $r$ 使用 <code style="color: #B58900"><a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py#L181" target="_blank">compute_rewards()</a></code> <a href="applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py#L212-L214" target="_blank">计算</a> old_rewards (这里可能会有些奇怪，在上述的 <a href="#RLHF pipeline">RLHF 的算法流程</a>中似乎没有这个流程，其实这里就是<a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py#L184" target="_blank">计算</a> $KL$，并将其<a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py#L185-L192" target="_blank">融入到</a> $r$ 中进行后续的计算而已)。接着便是通过 <code style="color: #B58900"><a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py#L308" target="_blank">get_advantages_and_returns()</a></code> 使用标准的 <a href="https://nn.labml.ai/rl/ppo/gae.html" target="_blank">GAE</a> <a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py#L221-L222" target="_blank">生成</a> <a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py#L310-L318" target="_blank">$A$</a> 和 <a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py#L319" target="_blank">return</a>。对于 actor model，接着<a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py#L226-L227" target="_blank">计算</a> $p^t_{RL}$，并和 $p_{RL}$ <a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py#L284-L285" target="_blank">计算</a> $r^t(\theta)$ (这里需要注意，因为 actor model 已经经过了 $t-1$ 次的更新，因此计算得到的 $p^t_{RL}$ 和 $p_{RL}$ 不相等，如果 <code style="color: #B58900">args.ppo_epochs</code> 等于 $1$，那么其二者会一直相等，即 $r^t(\theta)$ 恒等于 $1$)。最后通过 <code style="color: #B58900"><a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py#L282" target="_blank">actor_loss_fn()</a></code> 计算 <a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py#L228-L230" target="_blank">actor loss</a>，并进行 <a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py#L231" target="_blank">backward</a> 和 actor model <a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py#L234" target="_blank">参数更新</a>。而对于 critic model，接着<a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py#L236-L238" target="_blank">计算</a> $v^t$，并和 $v$ <a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py#L294-L298" target="_blank">计算</a> $v_{clip}$ (这里需要注意，因为 critic model 也已经经过了 $t-1$ 次的更新，因此计算得到的 $v^t$ 和 $v$ 不相等)。最后通过 <code style="color: #B58900"><a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py#L292" target="_blank">critic_loss_fn()</a></code> 和 return 计算 <a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py#L302-L305" target="_blank">critic loss</a>，并进行 <a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py#L242" target="_blank">backward</a> 和 critic model <a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py#L267" target="_blank">参数更新</a>。</p>

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;"><mark><b>总结：</b>DeepSpeedChat 使用 deepspeed 作为所有 model 的分布式进程组的“类” (OpenRLHF 和 verl 专门定义了 <code style="color: #B58900">*Group</code> 类)，所有 model 使用一个分布式进程组，每个分布式进程的所有 model 使用 DeepSpeedEngine 进行封装。通过将所有 model 放置在相同的 GPU 资源上，同时按照 PPO 逻辑顺序编写代码实现类似 single controller 的代码模式 (single controller 的解释参考下文)。</mark></p>

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;">至此，DeepSpeedChat 的逻辑和代码细节便已讲解完毕。可以发现，DeepSpeedChat 的 PPO 训练框架的构建和上述的 <a href="#RLHF pipeline">PPO 的算法流程</a>的逻辑是一致的，因此，DeepSpeedChat 的 PPO 训练框架的构建是相对容易的。同时，其将所有 model 都分配到了相同的设备上，即共用相同的 GPU 资源，如图 <a href="#fig-collocate-pipeline">3</a> 所示。这种做法的好处是可以简化代码，但是其缺点是无法实现不同 model 的并行计算，即每类 model 只能顺序执行计算，如只能 actor model infer 完后再进行 ref model infer，即便它们没有数据依赖关系。其次，对于每个 model 的不同阶段，其所需要的分布式优化不同，例如 actor model 在 rollout 时可能需要 vllm 的分布式优化，而在 train 时需要 DeepSpeed 的分布式优化。而 DeepSpeedChat 统一使用 DeepSpeed (虽然其存在 HybridEngine)，导致在 rollout 时 GPU 资源利用率不高。这对于单机多卡而言，由于卡间通信较快，其可以一定程度上缓解，但是对于多机多卡而言，其资源利用率会非常低。</p>

<figure id="fig-collocate-pipeline">
  <img src="/images/collocate_all_model.png" alt="collocate pipeline" style="width:100%">
  <figcaption>图 3：DeepSpeedChat 的各个 model 分布 (其中<span style="color: green;">绿色</span>表示每个 GPU 的内存；<span style="color: blue;">蓝色</span>表示每个 model。其中，actor model 和 critic model 由于需要 train 一般使用 Zero $3$，而 ref model 和 reward model 由于只需要 infer 一般使用 Zero $0$)</figcaption>
</figure>

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;">我们将 DeepSpeedChat 这种将所有 model 都分配到相同的 GPU 资源上的结构称为 <b>collocate all models</b>。而在理想的情况下，各个 model 在 GPU 资源上的结构应该如图 <a href="#fig-collocate-pipeline">4</a> 所示，称为 distribute all models (这是我自己瞎起的😎)。首先，将 actor model 复制为 $2$ 份，一份用于 train，使用 TrainEngine (如 DeepSpeed, FSDP, Megatron) 进行优化，称为 $\pi_{train}$；而另一份用于 rollout，使用 InferEngine (如 vllm, sglang) 进行优化，称为 $\pi_{rollout}$，并在每次 PPO 训练阶段完成后，下一次 PPO 生成阶段开始前，将更新后的 $\pi_{train}$ 的参数同步给 $\pi_{rollout}$。这样做的目的是可以更好地利用目前开源的各个 train/infer engine，提升各个阶段的效率。其次，将每个 model 分配到不同的 GPU 资源上，使其独占给定的 GPU 资源，这样，图 <a href="#fig-ppo-pipeline">1</a> 中那些没有数据依赖关系的计算模块就可以并行，从而节省整体的时间开销。</p>

<figure id="fig-scattered-pipeline">
  <img src="/images/scattered_pipeline.png" alt="scattered pipeline" style="width:100%">
  <figcaption>图 4：理想情况下 RLHF 的各个 model 分布 (其中<span style="color: green;">绿色</span>表示每个 GPU 的内存；<span style="color: blue;">蓝色</span>表示每个 model)</figcaption>
</figure>

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;"><span style="color: gray;">题外话 ：要使得没有数据依赖关系的计算模块可以并行除了需要将 model 分配到不同 GPU 资源上，另一个关键是要实现每个计算模块的<b>异步调用</b>，而不是像 DeepSpeedChat 那样前一个计算模块完成后才会启动下一个计算模块；同时将每个 model 分配到不同的 GPU 资源上可以避免在一个 GPU 上开太多的进程导致神秘 bug 的等一系列新的问题🥲。</span></p>

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;">在将每个 model 都分配到不同的 GPU 资源之后，RLHF 的整个流程就可以引入计算模块并行推理，形成如图 <a href="#fig-RLHF-parallel-pipeline">5</a> 所示的逻辑流程。可以看到，大多数的计算模块都可以并行进行，与图 <a href="#fig-ppo-pipeline">1</a> 相比，其可以节省大量的时间开销。下面要讲的 OpenRLHF 和 verl 框架都是使用这种并行的逻辑流程来编写代码的，当然其也包括 collocate all models 的逻辑流程编写的代码。</p>

<figure id="fig-RLHF-parallel-pipeline">
  <img src="/images/RLHF-parallel-pipeline.svg" alt="RLHF parallel pipeline" style="width:100%">
  <figcaption>图 5：理想情况下 RLHF 的逻辑流程 (其中<span style="color: red;">红色箭头</span>表示逻辑流；<span style="color: black;">黑色箭头</span>表示数据流。<span style="color: #CCCC00;">黄色模块</span>表示计算模块；<span style="color: blue;">蓝色模块</span>表示由计算模块生成的数据模块，同一层内的计算模块表示其可以并行)</figcaption>
</figure>

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;"><span style="color: gray;">题外话：虽然上面说了那么多 distribute all models 的好处，但是理论上，在不考虑通信的情况下，collocate all models 才是最优解。在给定 GPU 资源的情况下 (假设给定的 GPU 算力为 $N$)，首先，由图 <a href="#fig-RLHF-parallel-pipeline">5</a> 可知，每个 model 的计算模块不是每时每刻都可以并行 (如 actor model (rollout) 在 rollout 时，其余 model 只能等待)，这必然会导致 GPU 资源的浪费，因此 distribute all models 架构所利用的 GPU 算力的上限 $<N$；而 collocate all models 架构将每个 model 都分布在所有的 GPU 资源上，无论是哪个 model 的计算模块在执行都可以利用所有的 GPU 算力，因此其所利用的 GPU 算力的上限为 $N$，即 <b>collocate all models 所能利用的 GPU 资源上限高</b>。其次，假设 distribute all models 给 $M$ ($M < N$) 个 model 均分 GPU 资源，那么每个 model 所能得到的最大的 GPU 显存为 $M * 单个 GPU 显存$；而 collocate all models 通过 offload 技术，可以实现每个 model 所能得到的最大的 GPU 显存为 $N * 单个 GPU 显存$，因此 <b>collocate all models 加载 model 的上限高</b>。综上所述，collocate all models 架构理论上不仅更快，而且还能训练更大的 model。</span></p>

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;"><span style="color: gray;">因此，collocate all models 架构在 GPU 资源较少的情况下会比 distribute all models 架构更快，<a href="https://arxiv.org/abs/2409.19256" target="_blank">HybridFlow</a> 论文中的实验也证明了这一点。这也是为什么 OpenRLHF 和 verl 都实现了 collocate all models 架构的主要原因，而 distribute all models 架构发挥作用的场景在 GPU 资源较多的情况下。举个极端的例子，假设你有 $5$ 台 $8$ 卡 A$100$ 准备训练 $1.5$B 的 model，这时候如果将 $1.5$B model 使用 Megatron/FSDP 等单模型分布式框架分布到 $5 * 8$ 张卡上，由于机间通信等问题，导致大部分时间都在传数据，而不是计算，其时间不一定会比使用 $1$ 台 $8$ 卡 A$100$ 进行训练要短。此时使用 distribute all models 架构，将 5 个 model 各自分配在 $1$ 台 $8$ 卡 A$100$ 上，每个 model 基本上只在机内通信，大大降低了通信时间，增加了计算效率。因此，distribute all models 架构属于是追求极致时间性能的最优解，即给定如下任务：训练一个 $xx$B 的 model，给你无限的卡，只要求训练时间尽可能短。这时候使用 distribute all models 架构比 collocate all models 架构好。图 <a href="#fig-collocate_distribute_performance" target="_blank">题外话-1</a> 是我猜测的两个架构随着给定 GPU 资源增加的情况下的训练时间的变化。所以对于我这种“穷人家的孩子”，没有什么计算资源，还是老老实实用 collocate all models 架构才是最佳的选择😅。</span></p>

<figure id="fig-collocate_distribute_performance">
  <img src="/images/collocate_distribute_performance.svg" alt="collocate_distribute_performance" style="width:100%">
  <figcaption>图 题外话-1：(猜测的) collocate all models 架构和 distribute all models 架构的训练时间与 GPU 资源的关系</figcaption>
</figure>

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;">与 DeepSpeedChat 一开始就使用 deepspeed 命令启动分布式，并在每个子进程中运行 main.py 不同。关于图 <a href="#fig-RLHF-parallel-pipeline">5</a> 所示的逻辑流程的代码编写，由于其需要模块并行，即每个 model 的分布式进程组执行的模块不同 (例如 actor model 的分布式进程组在生成 action logits 时，ref model 的分布式进程组在同时生成 sft logits)，因此最直观，也是最具扩展性的方式是使用一个<b>主进程</b>来编写 PPO 的整体计算逻辑 (这个主进程也被称为 single controller)，在遇到分布式初始化/计算时，则异步启动/调用各个 model 的分布式进程组，然后继续主进程的下一步计算逻辑，并在之后需要原先分布式进程组结果的时候获取它。因此，整体的代码训练框架如图 <a href="#fig-RLHF-parallel-pipeline">6</a> 所示 (由于篇幅限制，这里只展示一小部分代码逻辑)。

<figure id="fig-RLHF-parallel-code-pipeline">
  <img src="/images/RLHF-parallel-code-pipeline.svg" alt="RLHF parallel code pipeline" style="width:100%">
  <figcaption>图 6：理想情况下 RLHF 的训练框架 (其中<span style="color: black;">黑色箭头</span>表示初始化/调用不同 model 的分布式进程组。<span style="color: green;">绿色模块</span>表示 model 的分布式进程组；<span style="color: #CCCC00;">黄色模块</span>表示 model 的分布式进程组的每个进程)</figcaption>
</figure>

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;"><span style="color: gray;">题外话：原本的 OpenRLHF 的代码不是 single controller 的模式，而是将 PPO 的计算逻辑分散到各个 model 的分布式进程组中，导致其较难扩展。不过好在现在已经<a href="https://github.com/OpenRLHF/OpenRLHF/pull/972" target="_blank">重构</a>为 single controller 的模式了。(有空可以补充 OpenRLHF 旧版本的 multi controller 的代码讲解🤔)</span></p>

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;">那么如何异步地启动/调用不同 model 的分布式进程组呢？目前 OpenRLHF 和 verl 都采用了 <a href="https://github.com/ray-project/ray" target="_blank"> ray </a>来实现这一目的。ray 有些类似于计算集群管理和调度的软件，通过 <code style="color: #B58900">ray start</code> 或者 <code style="color: #B58900">ray.init()</code> 来启动 ray，并指定集群所拥有的 CPU 数，GPU 数等计算资源。接着使用装饰符 <code style="color: #B58900">@ray.remote()</code> 将某个函数/类装饰为一个 Task/Actor (可以初略地理解为任务)，则在后续调用该任务时，ray 会自动将其异步地调度到目前可用的计算资源上，从而减轻我们编写异步代码的难度。</p>

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;">由于 OpenRLHF 和 verl 都是基于 ray 来构建，同时构建逻辑遵循图 <a href="#fig-RLHF-parallel-pipeline">5</a>，因此其代码结构有些相似。但是不同的是 OpenRLHF 的分布式进程组的后端使用的是 DeepSpeed 和 vllm；而 verl 的分布式进程组的后端使用的是 FSDP/Megatron 和 vllm/sglang。同时，OpenRLHF 使用 <code style="color: #B58900">PPORayActorGroup</code> 封装每个 model 的分布式进程组；接着通过统一的 <code style="color: #B58900">async_run_method_batch()</code> 来调用每个 model 的统一接口 <code style="color: #B58900">execute_batch(method_name, ...)</code>，根据提供的 <code style="color: #B58900">method_name</code> 的不同来调用不同 model 的具体方法。而 verl 则是使用 <code style="color: #B58900">WorkerDict</code> 封装所有 model 的分布式进程，并使用 <code style="color: #B58900">_bind_workers_method_to_parent()</code> 将所有 model 的特有方法 (含有 <code style="color: #B58900">MAGIC_ATTR</code> 属性的方法)绑定到 <code style="color: #B58900">WorkerDict</code> 自身上，并进一步使用 <code style="color: #B58900">RayWorkerGroup()</code> 封装 <code style="color: #B58900">WorkerDict</code> 的分布式进程组，并使用 <code style="color: #B58900">_bind_worker_method()</code> 将绑定到 <code style="color: #B58900">WorkerDict</code> 上的方法进一步绑定到 <code style="color: #B58900">RayWorkerGroup()</code> 上，从而通过直接调用 <code style="color: #B58900">RayWorkerGroup()</code> 自身的方法来调用不同 model 的具体方法。</p>

<h1 id="OpenRLHF pipeline">OpenRLHF</h1>

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;">接下来，我们讲解 OpenRLHF 的每个模块的逻辑和代码细节：(下面讲解的 OpenRLHF 的版本为 494850f50342ed38d5ae76ef45a3207f3523b582)</p>

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;">如图 <a href="#fig-OpenRLHF-pipeline">7</a> 所示 (这里直接盗用 <a href="https://arxiv.org/abs/2405.11143" target="_blank">OpenRLHF</a> 的图片🥳)，OpenRLHF 的每个模块的逻辑细节和图 <a href="#fig-RLHF-parallel-pipeline">5</a> 类似。在启动时，通过 <a href="https://github.com/OpenRLHF/OpenRLHF?tab=readme-ov-file#pporeinforce-with-ray-and-vllm" target="_blank">ray job submit</a> 的方式向 ray 提交运行主进程的 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/cli/train_ppo_ray.py" target="_blank">train_ppo_ray.py</a> 文件。在后续的代码分析中，我们先聚焦于每个 model 各自分配不同的 GPU 资源，即 distribute all models 的实现，后面有空再讲解 collocate all models 的实现。关于 OpenRLHF 代码的完整流程图可以查看 Appendix <a href="#appendix B">B</a>。train_ppo_ray.py 包括 $3$ 个阶段。<b>第一阶段：</b>首先，实例化 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/cli/train_ppo_ray.py#L24" target="_blank">strategy</a>，其是一个 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/utils/deepspeed/deepspeed.py#L36" target="_blank"><code style="color: #B58900">DeepspeedStrategy</code></a> 类，主要用于<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/utils/deepspeed/deepspeed.py#L222" target="_blank">构建 model 的 DeepSpeed 分布式进程组</a>，<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/utils/deepspeed/deepspeed.py#L143" target="_blank">model 的 backward</a> 和<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/utils/deepspeed/deepspeed.py#L148" target="_blank">参数更新</a>等一系列与分布式初始化/计算以及通信操作有关的内容。接着是构建 <a href="https://github.com/vllm-project/vllm" target="_blank"><code style="color: #B58900">vllm</code></a> 封装的 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/cli/train_ppo_ray.py#L60-L74" target="_blank">actor model</a> 用于 rollout，以及构建 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/launcher.py#L190" target="_blank"><code style="color: #B58900">PPORayActorGroup</code></a> 封装的 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/cli/train_ppo_ray.py#L76C19-L83" target="_blank">actor model</a> (用于 train 和 infer)，<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/cli/train_ppo_ray.py#L88-L95" target="_blank">ref model</a> (用于 infer)，<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/cli/train_ppo_ray.py#L112-L119" target="_blank">critic model</a> (用于 train 和 infer)，和 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/cli/train_ppo_ray.py#L126-L133" target="_blank">reward model</a> (用于 infer)。以 actor model 为例，<code style="color: #B58900">PPORayActorGroup</code> 的<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/launcher.py#L205-L226" target="_blank">初始化</a>主要涉及初始化 actor model 的<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/launcher.py#L228-L278" target="_blank">分布式进程组的预处理</a>。先是使用 ray 为分布式进程组<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/launcher.py#L233-L240" target="_blank">预分配</a> GPU 等计算资源，接着初始化 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/launcher.py#L242-L249" target="_blank">master actor</a>，得到 master actor 的 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/launcher.py#L260" target="_blank">master addr 和 master port</a> 后，基于此继续初始化剩下的 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/launcher.py#L263-L271" target="_blank">work actor</a>。以 master actor 的初始化为例，主要是<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/launcher.py#L18-L36" target="_blank">初始化 world size，rank，master addr，master port 等环境变量</a>，为后面真正的 model 的分布式进程组初始化做准备。</p>

<figure id="fig-OpenRLHF-pipeline">
  <img src="/images/OpenRLHF-pipeline.png" alt="OpenRLHF pipeline" style="width:100%">
  <figcaption>图 7: OpenRLHF 的 PPO 训练框架 (source: https://arxiv.org/abs/2405.11143)</figcaption>
</figure>

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;"><b>第二阶段：</b>首先是<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/cli/train_ppo_ray.py#L143-L160" target="_blank">初始化</a> <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ppo_trainer.py#L360" target="_blank"><code style="color: #B58900">PPOTrainer</code></a>，它是 PPO 训练和生成阶段的主要类，其初始化主要包括通过 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/utils/utils.py#L24" target="_blank"><code style="color: #B58900">get_tokenizer()</code></a> 初始化 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ppo_trainer.py#L43" target="_blank">tokenizer</a>，将之前初始化的各个 model 的分布式进程组<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ppo_trainer.py#L44-L49" target="_blank">赋值给自身的变量</a>，以及 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ppo_utils/experience_maker.py#L237" target="_blank"><code style="color: #B58900">SamplesGenerator</code></a> 的<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ppo_trainer.py#L406-L411" target="_blank">初始化</a> (用于使用 vllm 封装的 actor model 进行 rollout)，<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ppo_utils/experience_maker.py#L406" target="_blank"><code style="color: #B58900">RemoteExperienceMaker</code></a> 的<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ppo_trainer.py#L413-L422" target="_blank">初始化</a> (用于 PPO 的生成阶段)，通过 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ppo_trainer.py#L307" target="_blank"><code style="color: #B58900">prepare_datasets()</code></a> <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ppo_trainer.py#L424" target="_blank">构建数据集</a>，以及其他参数/日志的初始化等。接着是 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/cli/train_ppo_ray.py#L168" target="_blank">actor model</a>，<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/cli/train_ppo_ray.py#L167" target="_blank">ref model</a>，<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/cli/train_ppo_ray.py#L176" target="_blank">critic model</a> 和 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/cli/train_ppo_ray.py#L170" target="_blank">reward model</a> 的分布式进程组的初始化。以 actor model 为例，其主要是<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/launcher.py#L280-L290" target="_blank">调用</a>每个分布式进程的 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/ppo_actor.py#L360" target="_blank"><code style="color: #B58900">init_model_from_pretrained()</code></a> 方法。在该方法中，首先<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/ppo_actor.py#L373" target="_blank">调用</a> strategy 的 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/utils/deepspeed/deepspeed.py#L79" target="_blank"><code style="color: #B58900">setup_distributed()</code></a> 来初始化分布式进程组的<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/utils/deepspeed/deepspeed.py#L97" target="_blank">通信后端</a>和分布式进程组的 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/utils/deepspeed/deepspeed.py#L102-L104" target="_blank">device mesh</a>；接着<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/ppo_actor.py#L375-L388" target="_blank">初始化</a> <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/models/actor.py#L17"><code style="color: #B58900">Actor</code></a> 类，其主要是通过 <code style="color: #B58900">AutoModelForCausalLM.from_pretrained()</code> 加载指定的 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/models/actor.py#L87-L94" target="_blank">hf model</a>；然后初始化 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/ppo_actor.py#L392-L394" target="_blank">tokenizer</a>，<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/ppo_actor.py#L409-L411" target="_blank">优化器</a>，<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/ppo_actor.py#L413-L419" target="_blank">学习率 scheduler</a>，并<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/ppo_actor.py#L427-L429" target="_blank">通过</a> strategy 的 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/utils/deepspeed/deepspeed.py#L205-L250" target="_blank"><code style="color: #B58900">prepare()</code></a> 方法将 model，优化器和学习率 scheduler 使用 deepspeed 后端进行<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/ppo_actor.py#L427-L430" target="_blank">封装</a>；最后是<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/ppo_actor.py#L453-L464" target="_blank">构建</a> <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/ppo_actor.py#L33" target="_blank"><code style="color: #B58900">ActorPPOTrainer</code></a> 用于 actor model 的 train。其他 model 的初始化过程与 actor model 的初始化过程类似。至此，PPO 的所有初始化便结束了。</p>

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;"><b>第三阶段：</b>这一阶段主要是<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/cli/train_ppo_ray.py#L180" target="_blank">使用</a> PPOTrainer 的 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ppo_trainer.py#L433" target="_blank"><code style="color: #B58900">fit()</code></a> 方法执行 PPO 的生成和训练阶段。在该方法中，首先是<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ppo_trainer.py#L439-L454" target="_blank">加载</a> checkpoint (这里的 checkpoint 指的是整个训练环境，包括 model，dataloader 等的 checkpoint state)。接着开始 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ppo_trainer.py#L456"><code style="color: #B58900">args.num_episodes</code></a> 次 epoch 的训练。对于<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ppo_trainer.py#L465" target="_blank">每一次</a>训练，首先是<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ppo_trainer.py#L467-L469" target="_blank">使用</a> <code style="color: #B58900">SamplesGenerator</code> 的 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ppo_utils/experience_maker.py#L246" target="_blank"><code style="color: #B58900">generate_samples()</code></a> 生成 response，其主要是<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ppo_utils/experience_maker.py#L259" target="_blank">调用</a> <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ppo_utils/experience_maker.py#L288" target="_blank"><code style="color: #B58900">_generate_vllm()</code></a> 方法，将给定的 prompt <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ppo_utils/experience_maker.py#L327" target="_blank">均匀分配</a>给每一个 <code style="color: #B58900">vllm</code> 包装的 actor model，并通过 <code style="color: #B58900">vllm</code> 的 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ppo_utils/experience_maker.py#L328" target="_blank"><code style="color: #B58900">add_requests()</code></a> 和 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ppo_utils/experience_maker.py#L334" target="_blank"><code style="color: #B58900">get_responses()</code></a> 来请求和返回生成的 response，最后将其<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ppo_utils/experience_maker.py#L335-L370" target="_blank">整理</a>并使用 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ppo_utils/experience_maker.py#L372-L379" target="_blank"><code style="color: #B58900">Experience</code></a> 类进行存放。得到 response 后，接着便<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ppo_trainer.py#L505" target="_blank">使用</a> <code style="color: #B58900">RemoteExperienceMaker</code> 的 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ppo_utils/experience_maker.py#L436"><code style="color: #B58900">make_experience_batch()</code></a> 方法执行 PPO 的生成阶段的剩下部分，包括将 response <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ppo_utils/experience_maker.py#L450-L452" target="_blank">组成 batch</a>，<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ppo_utils/experience_maker.py#L456" target="_blank">调用</a> <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ppo_utils/experience_maker.py#L463" target="_blank"><code style="color: #B58900">make_experience()</code></a> 方法生成 $p_{RL}$，$p_{SFT}$，$v$，$r$ 以及计算 $KL$，最后<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ppo_utils/experience_maker.py#L459" target="_blank">调用</a> <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ppo_utils/experience_maker.py#L614" target="_blank"><code style="color: #B58900">compute_advantages_and_returns()</code></a> 方法生成 $A$ 和 return。如前所述，<code style="color: #B58900">make_experience()</code> 和 <code style="color: #B58900">compute_advantages_and_returns()</code> 这两个方法是通过调用各个 model 的分布式进程组的统一的 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/launcher.py#L307" target="_blank"><code style="color: #B58900">async_run_method_batch()</code></a> 接口，并传入具体需要调用的 model 方法的名字来实现的。在 <code style="color: #B58900">make_experience()</code> 方法中，首先是调用 reward model 的 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/launcher.py#L171" target="_blank"><code style="color: #B58900">forward()</code></a> 方法生成 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ppo_utils/experience_maker.py#L496C46-L501" target="_blank">reward $r$</a>，然后是调用 actor model 的 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/ppo_actor.py#L486" target="_blank"><code style="color: #B58900">forward()</code></a> 方法获取 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ppo_utils/experience_maker.py#L509-L514" target="_blank">action logits $p_{RL}$</a>，接着是调用 critic model 的 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/ppo_critic.py#L222" target="_blank"><code style="color: #B58900">forward()</code></a> 方法获取 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ppo_utils/experience_maker.py#L527-L532">value $v$</a>，调用 ref model 的 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/launcher.py#126" target="_blank"><code style="color: #B58900">forward()</code></a> 方法获取 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ppo_utils/experience_maker.py#L527-L532">sft logits $p_{SFT}$</a>，最后利用 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/models/utils.py#L7" target="_blank"><code style="color: #B58900">compute_approx_kl()</code></a> 方法 (其中包括 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/models/utils.py#L22" target="_blank">k1</a>，<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/models/utils.py#L30-L31" target="_blank">k2</a>，<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/models/utils.py#L22" target="_blank">k3</a> 三种方式近似计算 $KL$) 计算 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/models/utils.py#L36-L38" target="_blank">KL divergence $KL$</a>。在 <code style="color: #B58900">compute_advantages_and_returns()</code> 方法中，首先是对 reward $r$ 的<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ppo_utils/experience_maker.py#L627-L649" target="_blank">后处理</a> (主要是 PPO 外的其他 RL 算法需要)，接着是使用 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/models/utils.py#L43" target="_blank"><code style="color: #B58900">compute_reward()</code></a> 方法将 $KL$ <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ppo_utils/experience_maker.py#L653-L659" target="_blank">融入到</a> reward $r$ 中，然后是使用 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ppo_utils/experience_maker.py#L722" target="_blank"><code style="color: #B58900">get_advantages_and_returns()</code></a> 方法计算 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ppo_utils/experience_maker.py#L662-L668" target="_blank">advantage $A$ 和 return</a>，最后对 $A$ 进行<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ppo_utils/experience_maker.py#L717" target="_blank">归一化</a>处理。而 <code style="color: #B58900">get_advantages_and_returns()</code> 方法里的内容和 <a href="#DeepSpeedChat pipeline">DeepSpeedChat</a>类似，是使用标准的 GAE 计算 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ppo_utils/experience_maker.py#L759-L764" target="_blank">advantage $A$</a> 和 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ppo_utils/experience_maker.py#L765" target="_blank">return</a> 的过程。</p>

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;">在使用 <code style="color: #B58900">RemoteExperienceMaker</code> 的 <code style="color: #B58900">make_experience_batch()</code> 方法完成 PPO 的生成阶段后，接下来便是 PPO 的训练阶段。首先是<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ppo_trainer.py#L510-L514" target="_blank">调用</a> actor model 的 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/ppo_actor.py#L512" target="_blank"><code style="color: #B58900">append()</code></a> 方法和 critic model 的 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/ppo_critic.py#L243" target="_blank"><code style="color: #B58900">append()</code></a> 方法将 PPO 生成阶段的结果存到每个分布式进程的 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ppo_utils/replay_buffer.py#L137" target="_blank"><code style="color: #B58900">NaiveReplayBuffer</code></a> 中，接着便<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ppo_trainer.py#L517">使用</a> <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ppo_trainer.py#L118"><code style="color: #B58900">ppo_train()</code></a> 方法执行 PPO 训练阶段，其内部是调用了 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ppo_trainer.py#L127" target="_blank">critic model</a> 的 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/ppo_critic.py#L247" target="_blank"><code style="color: #B58900">fit()</code></a> 方法和 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ppo_trainer.py#L139" target="_blank">actor model</a> 的 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/ppo_actor.py#L466" target="_blank"><code style="color: #B58900">fit()</code></a> 方法分别进行 actor model 和 critic model 的 train，最后将更新的 actor model 参数使用 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ppo_trainer.py#L155" target="_blank"><code style="color: #B58900">_broadcast_to_vllm()</code></a> 方法<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ppo_trainer.py#L147" target="_blank">同步到</a> vllm 封装的 actor model 中。在 critic model 的 <code style="color: #B58900">fit()</code> 方法中，主要是<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/ppo_critic.py#L251">调用</a>第二阶段<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/ppo_critic.py#L213-L220">初始化</a>的 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/ppo_critic.py#L24" target="_blank"><code style="color: #B58900">CriticPPOTrainer</code></a> 的 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/ppo_critic.py#L251"><code style="color: #B58900">ppo_train()</code></a> 方法。而 <code style="color: #B58900">CriticPPOTrainer.ppo_train()</code> 方法内主要包括将 <code style="color: #B58900">NaiveReplayBuffer</code> 中的 PPO 生成阶段的结果<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/ppo_critic.py#L62-L69">封装</a>为 <code style="color: #B58900">DataLoader</code>，对于 <code style="color: #B58900">Dataloader</code> 中的<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/ppo_critic.py#L80">每个 batch</a>，<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/ppo_critic.py#L82">调用</a> <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/ppo_critic.py#L99"><code style="color: #B58900">training_step()</code></a> 方法进行 train (包括<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/ppo_critic.py#L110-L118">生成 new value $V^t$</a>，通过 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/models/loss.py#L106"><code style="color: #B58900">critic_loss_fn()</code></a> 来<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/ppo_critic.py#L121-L126">计算 critic loss</a>，<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/ppo_critic.py#L133-L134">调用</a> strategy 的 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/utils/deepspeed/deepspeed.py#L143" target="_blank"><code style="color: #B58900">backward()</code></a> 方法和 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/utils/deepspeed/deepspeed.py#L148" target="_blank"><code style="color: #B58900">optimizer_step()</code></a> 方法进行 backward 和 critic model 的参数更新)。在 actor model 的 <code style="color: #B58900">fit()</code> 方法中，主要是<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/ppo_actor.py#L470">调用</a>第二阶段<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/ppo_actor.py#L453-L264">初始化</a>的 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/ppo_actor.py#L33" target="_blank"><code style="color: #B58900">ActorPPOTrainer</code></a> 的 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/ppo_actor.py#L146"><code style="color: #B58900">ppo_train()</code></a> 方法。而 <code style="color: #B58900">ppo_train()</code> 方法内主要包括将 <code style="color: #B58900">NaiveReplayBuffer</code> 中的 PPO 生成阶段的结果<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/ppo_actor.py#L149-L156">封装</a>为 <code style="color: #B58900">DataLoader</code>，对于 <code style="color: #B58900">Dataloader</code> 中的<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/ppo_actor.py#L167">每个 batch</a>，<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/ppo_actor.py#L169">调用</a> <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/ppo_actor.py#L199"><code style="color: #B58900">training_step()</code></a> 方法进行 train (包括<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/ppo_actor.py#L211-L219">生成 new action logits $p^t_{RL}$</a>，通过 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/models/loss.py#L75"><code style="color: #B58900">actor_loss_fn()</code></a> 来<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/ppo_actor.py#L222-L227">计算 actor loss</a> 和可选的其他<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/ppo_actor.py#L230-L247">辅助 loss</a>，<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/ppo_actor.py#L254-L255">调用</a> strategy 的 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/utils/deepspeed/deepspeed.py#L143" target="_blank"><code style="color: #B58900">backward()</code></a> 方法和 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/utils/deepspeed/deepspeed.py#L148" target="_blank"><code style="color: #B58900">optimizer_step()</code></a> 方法进行 backward 和 actor model 的参数更新)。在 <code style="color: #B58900">_broadcast_to_vllm()</code> 方法中，主要是<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ppo_trainer.py#L161">调用</a> actor model 分布式进程组的 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/ppo_actor.py#L506"><code style="color: #B58900">broadcast_to_vllm()</code></a> 方法，进而<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/ppo_actor.py#L507">调用</a> <code style="color: #B58900">ActorPPOTrainer</code> 的 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/ppo_actor.py#L506"><code style="color: #B58900">broadcast_to_vllm()</code></a> 方法 (这个方法细节见 Appendix <a href="#appendix A-OpenRLHF">A</a>) 来实现两个 actor model 之间的参数同步。</p>

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;">至此，OpenRLHF 的逻辑和代码细节便已讲解完毕。可以发现，通过 ray 来编写整个代码框架非常的便捷，除了前述的资源管理和分配的优势外，在编写代码时只需要在需要调用 model 的分布式进程组的方法时执行 <code style="color: #B58900">func_handler=function.remote()</code> 得到执行结果的句柄 (注意，此时函数真正的结果可能还没执行完毕)，接着在需要得到执行结果时使用 <code style="color: #B58900">ray.get(func_handler)</code> 获得结果，即可实现异步的程序执行。同时，由于执行时只获得句柄，如果需要不同 model 的分布式进程组数据转移的逻辑编写，也只需要将在主进程执行 model_1 的 function_1 <code style="color: #B58900">func_handler=model_1.function_1.remote()</code> 获得句柄，并传递给 model_2 的 function_2 <code style="color: #B58900">model_2.function_2.remote(func_handler)</code>，并在 function_2 里执行 <code style="color: #B58900">ray.get(func_handler)</code> 即可直接实现由 model_1 向 model_2 传递结果，而不需要主进程作为中介 (主进程只起到一个传递句柄的作用，而不是传递真正的数据)。</p>

<h1 id="verl pipeline">verl</h1>

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;">最后，我们讲解 verl，由于 verl 的逻辑细节和 OpenRLHF 的非常一致，因此我们侧重讲解其每个模块的代码细节 (下面讲解的 verl 的版本为 78532923368aeb058f62201489546d013df47710)。关于 verl 代码的完整流程图可以查看 Appendix <a href="#appendix C">C</a>。</p>

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;">相比于 OpenRLHF，verl 的整体架构更加地“工业味”，是一个标准的面向对象开发的 project (即其将所有的东西都视为一个类，包括 model，资源等，同时构造不同的上下文环境管理类来管理不同 model 的上下文)。其也包括 $3$ 个阶段。<b>第一阶段：</b>首先实例化了一个 <code style="color: #B58900">TaskRunner</code> 用于作为 single controller，接着构建 <code style="color: #B58900">model</code> 到 <code style="color: #B58900">model_class</code>，<code style="color: #B58900">资源池名称</code> 到 <code style="color: #B58900">资源数量</code>，<code style="color: #B58900">model</code> 到 <code style="color: #B58900">资源池名称</code> 的映射 (Dict)，然后初始化 reward manager 用于计算 reward，最后初始化训练和测试数据集，以及 sampler。</p>

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;"><b>第二阶段：</b>其主要是初始化每个 model 的分布式进程组。首先是使用 create_resource_pool() 创建资源池，并初始化 <code style="color: #B58900">资源池</code> 到 <code style="color: #B58900">model_name</code> 和 <code style="color: #B58900">model_class</code> 的映射 (Dict)。接着，使用 create_colocated_worker_cls() 初始化统一的 work_dict_class，其的作用是将每个 model 的方法都统一到一个 class 下，并使用前缀来区分不同 model 的方法 (例如 actor model 的forward 方法为 actor_forward)。在 create_colocated_worker_cls() 中，首先是获取所有 model 的基类，并继承该基类构建新 class <code style="color: #B58900">WorkDict</code>，WorkDict 的初始化主要包括实例化每个 model，并通过后续的方法绑定将 WorkDict 的方法调用转化为对每个实例化 model 的方法调用。接着是获取每个 model 的 class，并通过 _bing_workers_method_to_parent() 将每个 model 的包含 MAGIC_ATTR 属性的方法绑定到 WorkDict 中，并通过将不同 model 的名称设置为方法的前缀来区分不同 model 的方法。使用 create_colocated_worker_cls() 生成 WorkDict 后，接下来便是初始化 RayWorkerGroup 生成 wg_dict。在其初始化过程中，首先是使用 _init_with_resource_pool() 初始化 WorkDict 的分布式进程组，其主要包括使用 ray 分配计算资源，获取必要的环境变量，调用 RayClassWithInitArgs.__call() 初始化分布式进程：对于主进程 (master)，其通过 register_center_actor 获取 master_addr 和 master_port；而对于剩下的进程 (worker)，其使用 master 获取到的 master_addr 和 master_port。初始化分布式进程后，通过 _bind_worker_method() 方法将 WorkDict 的方法绑定到 RayWorkerGroup 上，这次绑定和 WorkDict 中不同的是其额外获取了 dispatch，execute 和 block 等模式用于定义分配方法的输入，执行方法，以及收集方法的输出的模式。在初始化 wg_dict 后，接着使用 spawn() 方法将其为每个 model 复制一份，其主要包括使用 from_detached() 复制 wg_dict 中的 WorkDict 实例；将 WorkDict 的方法绑定到 RayWorkerGroup 上；根据 model 的名称来去除 RayWorkerGroup 方法中的前缀并进行重新绑定 (例如这个 RayWorkerGroup 是复制给 actor model 的，则将其中含 actor_* 名称的方法全部恢复为 * 的名称)。在复制完成后，每个 model 的分布式进程组便已初始化完毕，最后便是实例化每个 model。以实例化 actor model 为例，其主要是通过 _build_model_optimizer() 方法实例化 model，optimizer 和 lr scheduler。在 _build_model_optimizer() 中，首先是使用 hf_tokenizer() 实例化 tokenizer，接着通过 AutoConfig.frp,_pretrained() 和 get_generation_config() 分别获取 model 的 config 和 generate config。然后初始化上下文环境 (主要是用于指明实例化 model 的位置，CPU or GPU 等)，并通过 AutoModelFroCausalLM.From_pretrained() 实例化 model。接着便是初始化混合精度设置，初始化 auto_wrap_policy，初始化分片策略，并通过这些来构建最终的 FSDP model (关于构建 FSDP 的更多细节可以参考我<a href="https://cai-jianfeng.github.io/posts/2025/06/blog-distributed-train-pipeline/" target="_blank">之前的博客</a>)，最后初始化 optimizer 和 lr scheduler。在 _build_model_optimizer() 结束后，进一步使用 DataParallelPPOActor 将 model 进行进一步封装，并使用 _build_rollout() 初始化 model (rollout) 用于 rollout，包括使用 init_device_mesh() 初始化 device mesh；使用 vLLmRollout 构建 vllm 封装的 model；以及使用 FSDPVLLMShardingManager 构建 rollout_sharding_manager 作为 model rollout 时的上下文管理器。构建完 model (rollout) 后，最后便是通过 FSDPCheckpointManager 构建 checkpoint_manager 来管理 model 的 checkpoint。</p>

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;"><b>第三阶段：</b>该阶段主要是 PPO 的生成和训练阶段。在 PPO 生成阶段，首先是给定被构造为 DataProto 的 prompt 并通过 generate_sequences() 生成 response。在 generate_sequences() 中，首先是通过 rollout_sharding_manager 上下文管理器获取 actor model 的参数并使用 sync_model_weights() (这个方法细节见 Appendix <a href="#appendix A-verl">A</a>) 将其同步给 vllm 封装的 actor model，接着使用 reprocess_data() 收集进程组的所有 prompt，并通过 generate_sequences() 生成 response，最后使用 postprocess_data() 将生成完的 response 进行分块，并退出 rollout_sharding_manager 上下文管理器。生成 response 后，接下来是使用 compute_rm_score() 生成 reward $R$，其主要包括通过 ulysses_sharding_manager 上下文管理器设置 sequence parallel；使用 preprocess_data() 收集进程组的所有 rm_data 和 data，并通过 split() 将其划分为 micro batch 大小；使用 reward_module 的 forward() 生成 $R$；最后使用 postprocess_data() 将生成完的 response 进行分块，并退出 ulysses_sharding_manager 上下文管理器。然后使用 compute_log_prob() 生成 action logtis $p_{RL}$，其主要包括通过 ulysses_sharding_manager 上下文管理器设置 sequence parallel；使用 preprocess_data() 收集进程组的所有 data，并使用 DataParallelPPOActor.compute_log_prob() 生成 $p_{RL}$；最后退出 ulysses_sharding_manager 上下文管理器。接着使用和生成 action logtis $p_{RL}$ 相同的步骤生成 sft logtis $p_{SFT}$。然后使用 compute_values() 生成 value $V$，其步骤与生成 action logtis $p_{RL}$ 相同，除了将中间的 使用 DataParallelPPOActor.compute_log_prob() 生成 $p_{RL}$ 替换为使用 DataParallelCritic.compute_values() 生成 $V$。最后是使用 apply_kl_penalty() 计算 KL divergence $KL$ 并将其融入到 $R$ 中，并通过 compute_advantage() 生成 advantage $A$ 和 return。在 PPO 训练阶段，其主要是调用 critic model 的 updata_critic() 方法和 actor model 的 update_actor() 来训练两个 model。在 updata_critic() 中，除与之前一致的上下文环境管理外，主要是使用 DataParallelPPOCritic.update_critic() 训练 critic model，其中包括生成 new value $V^t$；使用 core_algos.compute_value_loss() 计算 critic loss；最后进行 backward 并使用 DataParallelPPOCritic._optimizer_step() 更新 critic model 参数。而在 update_actor() 中，除与之前一致的上下文环境管理外，主要是使用 ActorRolloutRefWorker.update_actor() 训练 actor model，其中包括生成 new action logits $p^t_{RL}$；使用 core_algos.compute_policy_loss() 计算 actor loss；最后进行 backward 并使用 DataParallelPPOActor._optimizer_step() 更新 actor model 参数。</p>

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;">至此，verl 的代码细节便已讲解完毕。可以发现，verl 的代码非常擅于利用装饰器和上下文管理器来实现预处理步骤以及逻辑的封装，甚至能够利用 <code style="color: #B58900">__new__</code> 的初始化来编写自身的代码 (例如 <code style="color: #B58900">register_center_actor</code> 的实例化便是在 <code style="color: #B58900">Worker</code> 类的 <code style="color: #B58900">__new__</code> 中)。这种方式的优点是可以实现很强的代码复用，以及很容易进行模块化扩展，但是缺点就是代码的逻辑阅读性可能不是很好。不过感觉公司的项目应该很喜欢这种风格的代码，更符合多人协作的范式，大家还是要多多熟悉 (我也要多多熟悉😉)。</p>

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;"><span style="color: white;">TODO List: 1. verl 的 url 引用；2. Appendix A 的 OpenRLHF 和 verl 的 vllm 与 actor model 的参数同步；3. R 和 V 表示的是 model，后续却将其表示为 值，需要统一。</p>

<h1 id="appendix A">Appendix A: Actor Model 与 vllm 的参数同步的实现细节</h1>

敬请期待🤪

<h2 id="appendix A-OpenRLHF">OpenRLHF 的 <code style="color: #B58900">broadcast_to_vllm()</code></h2>

<h2 id="appendix A-verl">verl 的 <code style="color: #B58900">sync_model_weights()</code></h2>

<h1 id="appendix B">Appendix B: OpenRLHF 代码流程图</h1>

<figure id="fig-OpenRLHF-train_ppo_ray">
  <img src="/images/OpenRLHF_train_ppo_ray.svg" alt="OpenRLHF train_ppo_ray" style="width:100%">
  <figcaption>图 B-1: OpenRLHF 的 <code style="color: #B58900">train_ppo_ray.py</code> 的代码流程</figcaption>
</figure>

<figure id="fig-OpenRLHF-create_vllm_engines">
  <img src="/images/OpenRLHF_create_vllm_engines.svg" alt="OpenRLHF create_vllm_engines" style="width:100%">
  <figcaption>图 B-2: OpenRLHF 的 <code style="color: #B58900">create_vllm_engines()</code> 的代码流程</figcaption>
</figure>

<figure id="fig-OpenRLHF-PPORayActorGroup">
  <img src="/images/OpenRLHF_PPORayActorGroup.svg" alt="OpenRLHF PPORayActorGroup" style="width:100%">
  <figcaption>图 B-3: OpenRLHF 的 <code style="color: #B58900">PPORayActorGroup()</code> 的代码流程</figcaption>
</figure>

<figure id="fig-OpenRLHF-PPOTrainer">
  <img src="/images/OpenRLHF_PPOTrainer.svg" alt="OpenRLHF PPOTrainer" style="width:100%">
  <figcaption>图 B-4: OpenRLHF 的 <code style="color: #B58900">PPOTrainer()</code> 的代码流程</figcaption>
</figure>

<figure id="fig-OpenRLHF-PPORayActorGroup_async_init_model_from_pretrained">
  <img src="/images/OpenRLHF_PPORayActorGroup_async_init_model_from_pretrained.svg" alt="OpenRLHF PPORayActorGroup_async_init_model_from_pretrained" style="width:100%">
  <figcaption>图 B-5: OpenRLHF 的 <code style="color: #B58900">PPORayActorGroup.async_init_model_from_pretrained()</code> 的代码流程</figcaption>
</figure>

<figure id="fig-OpenRLHF-PPOTrainer_fit_generate">
  <img src="/images/OpenRLHF_PPOTrainer_fit_generate.svg" alt="OpenRLHF PPOTrainer_fit_generate" style="width:100%">
  <figcaption>图 B-6: OpenRLHF 的 <code style="color: #B58900">PPOTrainer.fit()</code> 的<b>预处理</b>和<b>PPO 生成阶段</b>的代码流程</figcaption>
</figure>

<figure id="fig-OpenRLHF-PPOTrainer_fit_train">
  <img src="/images/OpenRLHF_PPOTrainer_fit_train.svg" alt="OpenRLHF PPOTrainer_fit_train" style="width:100%">
  <figcaption>图 B-7: OpenRLHF 的 <code style="color: #B58900">PPOTrainer.fit()</code> 的<b>预处理</b>和<b>PPO 训练阶段</b>的代码流程</figcaption>
</figure>

<h1 id="appendix C">Appendix C: verl 代码流程图</h1>

<figure id="fig-verl-main_ppo">
  <img src="/images/verl_main_ppo.svg" alt="verl main_ppo" style="width:100%">
  <figcaption>图 C-1: verl 的 <code style="color: #B58900">main_ppo.py</code> 的<code style="color: #B58900">TaskRunner.run()</code>的代码流程</figcaption>
</figure>

<figure id="fig-verl-load_reward_manager">
  <img src="/images/verl_load_reward_manager.svg" alt="verl load_reward_manager" style="width:100%">
  <figcaption>图 C-2: verl 的 <code style="color: #B58900">load_reward_manager()</code> 的代码流程</figcaption>
</figure>

<figure id="fig-verl-RayPPOTrainer">
  <img src="/images/verl_RayPPOTrainer.svg" alt="verl RayPPOTrainer" style="width:100%">
  <figcaption>图 C-3: verl 的 <code style="color: #B58900">RayPPOTrainer()</code> 的代码流程</figcaption>
</figure>

<figure id="fig-verl-init_workers">
  <img src="/images/verl_init_workers.svg" alt="verl init_workers" style="width:100%">
  <figcaption>图 C-4: verl 的 <code style="color: #B58900">init_workers()</code> 的代码流程</figcaption>
</figure>

<figure id="fig-verl-RayPPOTrainer_fit">
  <img src="/images/verl_RayPPOTrainer_fit.svg" alt="verl RayPPOTrainer_fit" style="width:100%">
  <figcaption>图 C-5: verl 的 <code style="color: #B58900">fit()</code> 的代码流程</figcaption>
</figure>

<h1 id="appendix D">Appendix D: verl 代码额外细节</h1>

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;">1. 在 <code style="color: #B58900">Worker</code> 的 <code style="color: #B58900">__new__</code> 方法中，<code style="color: #B58900">*_register_center</code> 的实例化需要满足 <code style="color: #B58900">int(os.environ.get("DISABLE_WORKER_INIT", 0)) == 0</code>，<code style="color: #B58900">None not in [rank, worker_group_prefix]</code> 和 <code style="color: #B58900">"ActorClass(" not in cls.__name__</code>，而在 <code style="color: #B58900">WorkerDict</code> 的 <code style="color: #B58900">__init__</code> 中，使用 <code style="color: #B58900">with patch.dict(os.environ, {"DISABLE_WORKER_INIT": "1"})</code> 的上下文环境避免实例化各个 model 时生成 <code style="color: #B58900">*_register_center</code>；而在 <code style="color: #B58900">remote_cls = ray.remote(WorkerDict)</code> 时，会调用 <code style="color: #B58900">WorkDict</code> 的 <code style="color: #B58900">__new__</code> 方法，此时 <code style="color: #B58900">"ActorClass(" not in cls.__name__</code> 条件避免生成 <code style="color: #B58900">*_register_center</code>；最后在 <code style="color: #B58900">RayWorkerGroup._init_with_resource_pool()</code> 时，<code style="color: #B58900">worker = ray_cls_with_init(...)</code> 会调用 <code style="color: #B58900">self.cls.options(**options).remote(...)</code>，这个 <code style="color: #B58900">self.cls</code> 就是 <code style="color: #B58900">WorkDict</code>，此时将其实例化会再次调用 <code style="color: #B58900">Worker</code> 的 <code style="color: #B58900">__new__</code>，且其的 <code style="color: #B58900">cls.__name__</code> 为 <code style="color: #B58900">WorkDict</code>，没有外套 <code style="color: #B58900">ActorClass</code>，因此会成功调用 <code style="color: #B58900">Worker._configure_before_init()</code> 实例化 <code style="color: #B58900">*_register_center</code>。因此 <code style="color: #B58900">int(os.environ.get("DISABLE_WORKER_INIT", 0)) == 0</code>，<code style="color: #B58900">None not in [rank, worker_group_prefix]</code> 和 <code style="color: #B58900">"ActorClass(" not in cls.__name__</code> 这三个条件都是为了防止其他位置不合时宜地实例化 <code style="color: #B58900">*_register_center</code> (<del>感觉应该是在写代码的过程中发现了这些漏洞一个个加上的 (bushi)</del>)。(所以我比较疑惑 <code style="color: #B58900">ray.remote()</code> 封装时调用了类的 <code style="color: #B58900">__new__</code> 的作用是什么？🤔)</p>