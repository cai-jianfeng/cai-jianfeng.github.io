---
title: 'The Basic Knowledge of RLHF Training Pipeline'
date: 25-05-26
update: 25-05-26
permalink: /posts/2025/05/blog-rlhf-train-pipeline/
star: superior
tags:
  - 深度学习基本知识
---

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;">这篇博客主要讲解 RLHF 具体训练的框架 (DeepSpeedChat，OpenRLHF，verl) 的具体细节，包括每个框架的整体架构，架构内的各部分细节 (包括逻辑细节和代码细节)。(建议先阅读我之前关于 RLHF 的博客 <a href="https://cai-jianfeng.github.io/posts/2024/04/blog-rlhf/" target="_blank">The Basic Knowledge of RLHF (Reinforce Learning with Human Feedback)</a>)</p>

<h1 id="RLHF pipeline">RLHF 的算法流程</h1>

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;">在<a href="https://cai-jianfeng.github.io/posts/2024/04/blog-rlhf/" target="_blank">之前的博客</a>中，我们讲解了 RLHF 的三个阶段：SFT (预训练 LLM 模型 $M_\theta$)，Reward Modeling (预训练奖励模型 $r_\theta$) 和最后的 RL 训练 (使用 PPO 微调 $M_\theta$)。对于前两个阶段而言，其只存在一个模型，因此可以使用 Deepspeed，FSDP，Megatron，甚至是 Transformers 的内置 Trainer 等<b>单模型</b>训练框架直接进行分布式训练 (关于单模型训练框架，可以参考我<a href="https://cai-jianfeng.github.io/posts/2025/06/blog-distributed-train-pipeline/" target="_blank">之前的博客</a>)。对于第三个阶段而言，其包含多个模型，同时不同模型的所处状态也不尽相同 (例如，reference model 和 reward model 只用于 infer，policy model 和 value model 用于 train，同时 policy model 还用于 rollout)。因此，需要在 Deepspeed/FSDP/Megatron 这种单模型的训练框架上再进行进一步的搭建以构建<b>多模型</b>训练框架。因此，本文的所有 RLHF 框架其实主要是聚焦于构建第三阶段的多模型训练框架。</p>

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;">下面，我们以 PPO 为例来了解每个框架的整体架构和每个部分的具体模块 (其他的 RL 算法如 GRPO，REINFORCE++ 等基本上都是在 PPO 的基础上减少某些模块)。首先，如图 <a href="#fig-ppo-pipeline">1</a> 所示，我们先逻辑化整理一下 PPO 的算法流程：</p>

<!-- ![ppo pipeline](/images/PPO_gen_and_learn.png) -->
<figure id="fig-ppo-pipeline">
  <img src="/images/PPO_gen_and_learn.svg" alt="ppo pipeline" style="width:100%">
  <figcaption>图 1：PPO 的生成与训练阶段 (其中<span style="color: red;">红色箭头</span>表示逻辑流；<span style="color: yellow;">黄色模块</span>表示计算模块，计算模块需按照红色箭头顺序执行)</figcaption>
</figure>

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;"><b>A. PPO 的生成阶段：</b>即通过给定的输入，生成一系列 PPO 所训练的必要的元素，在经典 RL 中也被称作环境交互。</p>

1. <p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;">给定 SFT 后得到的 model，将其复制为 reference model $\pi_{SFT}$ 和需要进一步训练的 actor model $\pi_{RL}$；给定 Reward Modeling 后得到的 model，将其复制为 reward model $R$ 和 critic model $V$。</p>

2. <p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;">给定 prompt $x$，将其输入给 actor model $\pi_{RL}$ 生成对应的 response $y$，得到完整的 sequence $x + y$。(<span style="color: red;">$\pi_{RL}$ rollout</span>)</p>

3. <p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;">给定 sequence $x + y$，将其输入给 actor model $\pi_{RL}$ 和 reference model $\pi_{SFT}$ 分别生成 action logits $p_{RL}$ 和 sft logits $p_{SFT}$，并进一步计算 KL divergence $KL$。(<span style="color: red;">$\pi_{RL}$ 和 $\pi_{SFT}$ infer</span>)</p>

4. <p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;">给定 sequence $x + y$，将其输入给 reward model $R$ 和 critic model $V$ 分别生成 reward $r$ 和 value $v$。(<span style="color: red;">$R$ 和 $V$ infer</span>)</p>

5. <p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;">给定 $KL$ 和 $r$，计算得到 PPO 的 return；并通过给定 $v$，计算得到 PPO 的 advantage $A$。</p>

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;"><b>B. PPO 的训练阶段：</b>即通过 PPO 的生成阶段所得到的元素，进行 PPO 的训练，在经典 RL 中也被称作奖励学习。由于 PPO 的生成阶段的时间成本较高，因此通常对生成阶段得到的元素进行缓存，并进行多次训练。对于第 $t$ 次训练，其具体流程如下：</p>

1. <p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;">给定 sequence $x + y$，将其输入给第 $t-1$ 次训练完的 actor model $\pi_{RL}$ 生成 new action logits $p^{t}_{RL}$，并和 action logits $p_{RL}$ 计算 ratio $r^{t}(\theta)$。接着和给定的 advantage $A$ 计算 Actor loss 用于 actor model 的训练。</p>

2. <p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;">给定 sequence $x + y$，将其输入给第 $t-1$ 次训练完的 critic model $V$ 生成 new value $v^{t}$，并和 value $v$ 计算 clipped value $v_{clip}$。接着和给定的 return 计算 Critic loss 用于 critic model 的训练。</p> 

<h1 id="DeepSpeedChat pipeline">DeepSpeedChat</h1>


<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;">可以看到，上述的流程涉及到 actor model $\pi_{RL}$ 的 rollout，actor model $\pi_{RL}$ 和 $\pi_{SFT}$ 的 infer，reward model $R$ 和 critic model $V$ 的 infer，以及 actor model $\pi_{RL}$ 和 critic model $V$ 的 train。<b>最直接的实现方式是，按照上述流程的逻辑编写 PPO 训练的架构，通过简单扩展单模型训练框架得到多模型训练框架。</b>如图 <a href="#fig-deepspeedchat-pipeline">2</a> 所示，DeepSpeedChat 就是按照这种思路扩展 DeepSpeed 框架来实现 PPO 的训练的。(下面讲解的 DeepSpeedChat 的版本为 bd47e5bc38d292f44bf183e7bda992cde36a769b)</p>

<!-- <figure id="fig-deepspeedchat-pipeline">
  <embed src="/images/deepspeedchat.pdf" type="application/pdf" width="100%" height="600px">
  <figcaption>图 2：DeepSpeedChat 的 PPO 训练框架</figcaption>
</figure> -->

<!-- <figure id="fig-deepspeedchat-pipeline">
  <object data="/images/deepspeedchat.pdf" type="application/pdf" width="100%" height="600px">
    <p>您的浏览器不支持 PDF 显示。您可以 <a href="/images/deepspeedchat.pdf">下载 PDF</a> 查看。</p>
  </object>
  <figcaption>图 2：DeepSpeedChat 的 PPO 训练框架</figcaption>
</figure> -->

<figure id="fig-deepspeedchat-pipeline">
  <img src="/images/deepspeedchat.svg" alt="deepspeedchat pipeline" style="width:100%">
  <figcaption>图 2：DeepSpeedChat 的 PPO 训练框架 (其中<span style="color: red;">红色箭头</span>表示代码执行的顺序；<span style="color: black;">黑色箭头</span>表示模块的扩展描述；<span style="color: green;">绿色模块</span>表示 main.py 内的代码模块；<span style="color: yellow;">黄色模块</span>表示其他文件内的代码模块)</figcaption>
</figure>

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;">接下来，我们讲解 DeepSpeedChat 的每个模块的逻辑和代码细节：</p>

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;">与<a href="https://cai-jianfeng.github.io/posts/2025/06/blog-distributed-train-pipeline/" target="_blank">之前的博客</a>中讲解的 DeepSpeed 的分布式训练一致，DeepSpeedChat 通过 <a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/training_scripts/llama2/run_llama2_7b.sh#L27" target="_blank">deepspeed</a> 命令启动分布式训练，并在每张卡上运行 <a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py" target="_blank">main.py</a> 文件。</p>

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;">1. 第一阶段：在 main.py 文件中，首先是<a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py#L449-L457" target="_blank">初始化分布式环境</a>，<a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py#L471-L475" target="_blank">加载 tokenizer</a>，<a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py#L477-L478" target="_blank">加载数据集</a>。接着，初始化 <a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py#L481-L486" target="_blank">DeepSpeedRLHFEngine</a>，包括初始化 <a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/rlhf/rlhf_engine.py#L48-L49" target="_blank">actor model</a>，<a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/rlhf/rlhf_engine.py#L50-L51" target="_blank">ref model</a>，<a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/rlhf/rlhf_engine.py#L58-L59" target="_blank">reward model</a> 和 <a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/rlhf/rlhf_engine.py#L56-L57" target="_blank">value model</a>，其中 actor model 和 critic model 是通过<code style="color: #B58900"><a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/utils/ds_utils.py#L13-L75" target="_blank">get_train_ds_config()</a></code>获取 train 的 config 实现 DeepSpeedEngine 的初始化，而 ref model 和 critic model是通过<code style="color: #B58900"><a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/utils/ds_utils.py#L78-L105" target="_blank">get_eval_ds_config()</a></code>获取 inferinfer的 config 实现 DeepSpeedEngine 的初始化。然后，初始化 <a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py#L495-L496" target="_blank">DeepSpeedPPOTrainer</a>，这是一个训练类，里面包含了所有关于 PPO 的计算函数。其<a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py#L46-L72" target="_blank">初始化的过程</a>就是简单的对 PPO 所需的各个 model 和系数进行赋值。最后，初始化 <a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py#L495-L496" target="_blank">MiniDataset</a>，其用于缓存后续 PPO 生成阶段的结果，并提供给 PPO 训练阶段使用。</p>

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;">2. 第二阶段：在完成初始化和数据的准备后，下一步便开始 PPO 的生成和训练阶段。在 <a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py#L528-L530" target="_blank">PPO 的生成阶段</a>，给定准备好的 <a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py#L520" target="_blank">prompt</a>，首先<a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py#L140" target="_blank">生成 response</a>，DeepSpeedChat 使用了最原始的 <code style="color: #B58900"><a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py#L86-L93" target="_blank">model.generate()</a></code> 的方式。接着，将生成好的 seq (prompt + response) 输入给 actor model 生成 <a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py#L153" target="_blank">$p_{RL}$</a>，输入给 ref model 生成 <a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py#L154" target="_blank">$p_{SFT}$</a>，输入给 reward model 生成 <a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py#L155-L158" target="_blank">$R$</a>，最后输入给 critic model 生成 <a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py#L159-L160" target="_blank">$V$</a>，最终将生成的所有结果<a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py#L540" target="_blank">缓存</a>到 MiniDataset 中。</p>

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;">3. 第三阶段：完成 PPO 的生成阶段，便是 PPO 的训练阶段。可以发现，为了更好地利用 PPO 生成阶段所生成的结果，一般会使用其进行<a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py#L550" target="_blank">多次训练</a>，对应代码中的 <code style="color: #B58900">args.ppo_epochs</code>。对于<a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py#L553" target="_blank">第 $t$ 次训练</a>，首先是根据 PPO 生成阶段的 $p_{RL}$，$p_{SFT}$ 和 $R$ 计算 <a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py#L553" target="_blank">old_rewards</a> (这里可能会有些奇怪，在上述的<a href="#RLHF pipeline">RLHF 的算法流程</a>中似乎没有这个流程，其实这里就是计算 <a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py#L184" target="_blank">$KL$</a>，并将其<a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py#L185-L192" target="_blank">融入到</a> $R$ 中进行后续的计算而已)。接着便是标准的使用 <a href="https://nn.labml.ai/rl/ppo/gae.html" target="_blank">GAE</a> 生成 <a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py#L310-L318" target="_blank">$A$</a> 和 <a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py#L319" target="_blank">return</a>。对于 actor model，接着计算 <a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py#L226-L227" target="_blank">$p^t_{RL}$</a>，并和 $p_{RL}$ 计算 $r^t(\theta)$ (这里需要注意，因为 actor model 已经经过了 $t-1$ 次的更新，因此计算得到的 $p^t_{RL}$ 和 $p_{RL}$ 不相等，如果 <code style="color: #B58900">args.ppo_epochs</code> 等于 $1$，那么其二者会一直相等，即 $r^t(\theta)$ 恒等于 $1$)。最后计算 <a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py#L228-L230" target="_blank">actor loss</a>，并进行 <a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py#L231" target="_blank">backward</a> 和 actor model <a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py#L234" target="_blank">更新</a>。而对于 critic model，接着计算 <a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py#L236-L238" target="_blank">$v^t$</a>，并和 $v$ 计算 <a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py#L294-L298" target="_blank">$v_{clip}$</a> (这里需要注意，因为 critic model 也已经经过了 $t-1$ 次的更新，因此计算得到的 $v^t$ 和 $v$ 不相等)。最后和 return 计算 <a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py#L302-L305" target="_blank">critic loss</a>，并进行 <a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py#L242" target="_blank">backward</a> 和 critic model <a href="https://github.com/deepspeedai/DeepSpeedExamples/blob/bd47e5bc38d292f44bf183e7bda992cde36a769b/applications/DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py#L267" target="_blank">更新</a>。</p>

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;">至此，DeepSpeedChat 的逻辑和代码细节便已讲解完毕。可以发现，DeepSpeedChat 的 PPO 训练框架的构建和上述的 PPO 的算法流程的逻辑是一致的，因此，DeepSpeedChat 的 PPO 训练框架的构建是相对容易的。同时，其将所有 model 都分配到了相同的设备上，即共用相同的 GPU 资源，如图 <a href="#fig-collocate-pipeline">3</a> 所示。这种做法的好处是可以简化代码，但是其缺点是无法实现不同 model 的并行计算，即每类 model 只能顺序执行计算，如 actor model infer 完后再进行 ref model infer。其次，对于每个 model 的不同阶段，其所需要的分布式优化不同，例如 actor model 在 rollout 时可能需要 vllm 的分布式优化，而在 train 时需要 DeepSpeed 的分布式优化。而 DeepSpeedChat 统一使用 DeepSpeed (虽然其存在 HybridEngine)，导致在 rollout 时 GPU 资源利用率不高。这对于单机多卡而言，由于卡间通信较快，其可以一定程度上缓解，但是对于多机多卡而言，其资源利用率会非常低。</p>

<figure id="fig-collocate-pipeline">
  <img src="/images/collocate_all_model.png" alt="collocate pipeline" style="width:100%">
  <figcaption>图 3：DeepSpeedChat 的各个 model 分布 (其中<span style="color: green;">绿色</span>表示每个 GPU 的内存；<span style="color: blue;">蓝色</span>表示每个 model。其中，actor model 和 critic model 由于需要 train 一般使用 Zero $3$，而 ref model 和 reward model 由于只需要 infer 一般使用 Zero $0$)</figcaption>
</figure>

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;">我们将 DeepSpeedChat 这种将所有 model 都分配到相同的 GPU 资源上的结构称为 <b>collocate all models</b>。而在理想的情况下，各个 model 在 GPU 资源上的结构应该如图 <a href="#fig-collocate-pipeline">4</a> 所示。首先，将 actor model 复制为 $2$ 份，一份用于 train，使用 TrainEngine (如 DeepSpeed, FSDP, Megatron) 进行优化，称为 $\pi_{train}$；而另一份用于 rollout，使用 InferEngine (如 vllm, sglang) 进行优化，称为 $\pi_{rollout}$，并在每次 PPO 训练阶段完成后，下一次 PPO 生成阶段开始前，将更新后的 $\pi_{train}$ 的参数同步给 $\pi_{rollout}$。这样做的目的是可以更好地利用目前开源的各个 train/infer engine，提升各个阶段的效率。其次，将每个 model 分配到不同的 GPU 资源上，使其独占给定的 GPU 资源，这样，图 <a href="#fig-ppo-pipeline">1</a> 中那些没有数据依赖关系的计算模块就可以并行，从而节省整体的时间开销。</p>

<figure id="fig-scattered-pipeline">
  <img src="/images/scattered_pipeline.png" alt="scattered pipeline" style="width:100%">
  <figcaption>图 4：理想情况下 RLHF 的各个 model 分布 (其中<span style="color: green;">绿色</span>表示每个 GPU 的内存；<span style="color: blue;">蓝色</span>表示每个 model)</figcaption>
</figure>

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;">在将每个 model 都分配到不同的 GPU 资源之后，RLHF 的整个流程就可以引入 model 并行推理，形成如图 <a href="#fig-RLHF-parallel-pipeline">5</a> 所示的逻辑流程。可以看到，大多数的计算模块都可以并行进行，与图 <a href="#fig-ppo-pipeline">1</a> 相比，其可以节省大量的时间开销。下面要讲的 OpenRLHF 和 verl 都是使用这种并行的逻辑流程来编写代码的。</p>

<figure id="fig-RLHF-parallel-pipeline">
  <img src="/images/RLHF-parallel-pipeline.svg" alt="RLHF parallel pipeline" style="width:100%">
  <figcaption>图 5：理想情况下 RLHF 的逻辑流程 (其中<span style="color: red;">红色箭头</span>表示逻辑流；<span style="color: black;">黑色箭头</span>表示数据流。<span style="color: yellow;">黄色模块</span>表示计算模块；<span style="color: blue;">蓝色模块</span>表示由计算模块生成的数据模块，同一层内的计算模块表示其可以并行)</figcaption>
</figure>

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;">与 DeepSpeedChat 一开始就使用 deepspeed 命令启动分布式，并在每个子进程中运行 main.py 不同。关于图 <a href="#fig-RLHF-parallel-pipeline">5</a> 所示的逻辑流程的代码编写，由于其需要模块并行，即每个 model 的分布式进程组执行的模块不同 (例如 actor model 的分布式进程组在生成 action logits 时，ref model 的分布式进程组在同时生成 sft logits)。因此最直观，也是最具扩展性的方式是使用一个<b>主进程</b>来编写 PPO 的整体计算逻辑 (这个主进程也被称为 single controller)，在遇到分布式初始化/计算时，则异步启动/调用各个 model 的分布式进程组，然后继续主进程的下一步计算逻辑，并在之后需要原先分布式进程组结果的时候获取它。因此，整体的代码训练框架如图 <a href="#fig-RLHF-parallel-pipeline">6</a> 所示 (由于篇幅限制，这里只展示一小部分代码逻辑)。

<figure id="fig-RLHF-parallel-code-pipeline">
  <img src="/images/RLHF-parallel-code-pipeline.svg" alt="RLHF parallel code pipeline" style="width:100%">
  <figcaption>图 6：理想情况下 RLHF 的训练框架 (其中<span style="color: black;">黑色箭头</span>表示初始化/调用不同 model 的分布式进程组。<span style="color: green;">绿色模块</span>表示 model 的分布式进程组；<span style="color: yellow;">黄色模块</span>表示 model 的分布式进程组的每个进程)</figcaption>
</figure>

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;"><span style="color: gray;">题外话：原本的 OpenRLHF 的代码不是 single controller 的模式，而是将 PPO 的计算逻辑分散到各个 model 的分布式进程中，导致其很难扩展。不过好在现在已经重构为 single controller 的模式了。</span></p>

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;">那么如何异步地启动/调用不同 model 的分布式进程组呢？目前 OpenRLHF 和 verl 都采用了 <a href="https://github.com/ray-project/ray" target="_blank"> ray </a>来实现这一目的。ray 有些类似于计算集群管理和调度的软件，通过 <code style="color: #B58900">ray start</code> 或者 <code style="color: #B58900">ray.init()</code> 来启动 ray，并指定集群所拥有的 CPU 数，GPU 数等计算资源。接着使用装饰符 <code style="color: #B58900">@ray.remote()</code> 将某个函数/类装饰为一个 Task/Actor (可以初略地理解为任务)，则在后续调用该任务时，ray 会自动将其异步地调度到目前可用的计算资源上，从而减轻我们编写异步代码的难度。</p>

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;">由于 OpenRLHF 和 verl 都是基于 ray 来构建，同时构建逻辑遵循图 <a href="#fig-RLHF-parallel-pipeline">5</a>，因此其代码结构有些相似。但是不同的是 OpenRLHF 的分布式进程组的后端使用的是 DeepSpeed 和 vllm；而 verl 的分布式进程组的后端使用的是 DeepSpeed/FSDP 和 vllm/sglang。同时，OpenRLHF 使用 <code style="color: #B58900">PPORayActorGroup</code> 封装每个 model 的分布式进程组；接着通过统一的 <code style="color: #B58900">async_run_method_batch()</code> 来调用每个 model 的统一接口 <code style="color: #B58900">execute_batch(method_name, ...)</code>，根据提供的 <code style="color: #B58900">method_name</code> 的不同来调用不同 model 的具体方法。而 verl 则是使用 <code style="color: #B58900">WorkerDict</code> 封装所有 model 的分布式进程组，并使用 <code style="color: #B58900">_bind_workers_method_to_parent()</code> 将所有 model 的特有方法 (含有 <code style="color: #B58900">MAGIC_ATTR</code> 属性的方法)绑定到 <code style="color: #B58900">WorkerDict</code> 自身上，从而通过直接调用自身的方法来调用不同 model 的具体方法。</p>

<h1 id="OpenRLHF pipeline">OpenRLHF</h1>

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;">接下来，我们讲解 OpenRLHF 的每个模块的逻辑和代码细节：(下面讲解的 OpenRLHF 的版本为 494850f50342ed38d5ae76ef45a3207f3523b582)</p>

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;">如图 <a href="#fig-OpenRLHF-pipeline">7</a> 所示 (这里直接盗用 <a href="https://arxiv.org/abs/2405.11143" target="_blank">OpenRLHF</a> 的图片🥳)，OpenRLHF 的每个模块的逻辑细节和图 <a href="#fig-RLHF-parallel-pipeline">5</a> 类似。在启动时，通过 <a href="https://github.com/OpenRLHF/OpenRLHF?tab=readme-ov-file#pporeinforce-with-ray-and-vllm" target="_blank">ray job submit</a> 的方式向 ray 提交运行主进程的 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/cli/train_ppo_ray.py" target="_blank">train_ppo_ray.py</a> 文件。在后续的代码分析中，我们先聚焦于每个 model 各自分配不同的 GPU 资源，后面再讲解 collocate all models 的实现。train_ppo_ray.py 包括 $3$ 个阶段。<b>第一阶段：</b>首先，实例化 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/cli/train_ppo_ray.py#L24" target="_blank">strategy</a>，其是一个 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/utils/deepspeed/deepspeed.py#L36" target="_blank">DeepspeedStrategy</a> 类，主要用于<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/utils/deepspeed/deepspeed.py#L222" target="_blank">构建 model 的 DeepSpeed 分布式进程组</a>，<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/utils/deepspeed/deepspeed.py#L143" target="_blank">model 的 backward</a> 和<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/utils/deepspeed/deepspeed.py#L148" target="_blank">参数更新</a>等一系列与分布式初始化/计算以及操作有关的内容。接着是构建 <a href="https://github.com/vllm-project/vllm" target="_blank">vllm</a> 封装的 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/cli/train_ppo_ray.py#L60-L74" target="_blank">actor model</a> 用于 rollout，以及构建 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/launcher.py#L190" target="_blank">PPORayActorGroup</a> 封装的 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/cli/train_ppo_ray.py#L76C19-L83" target="_blank">actor model</a> (用于 train 和 infer)，<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/cli/train_ppo_ray.py#L88-L95" target="_blank">ref model</a> (用于 infer)，<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/cli/train_ppo_ray.py#L112-L119" target="_blank">critic model</a> (用于 train 和 infer)，和 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/cli/train_ppo_ray.py#L126-L133" target="_blank">reward model</a> (用于 infer)。以 actor model 为例，PPORayActorGroup 的<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/launcher.py#L205-L226" target="_blank">初始化</a>主要涉及初始化 actor model 的<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/launcher.py#L228-L278" target="_blank">分布式进程组的预处理</a>。先是使用 ray 为分布式进程组<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/launcher.py#L233-L240" target="_blank">预分配</a> GPU 等计算资源，接着初始化 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/launcher.py#L242-L249" target="_blank">master actor</a>，得到 master actor 的 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/launcher.py#L260" target="_blank">master addr 和 master port</a> 后，基于此继续初始化剩下的 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/launcher.py#L263-L271" target="_blank">work actor</a>。以 master actor 的初始化为例，主要是<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/launcher.py#L18-L36" target="_blank">初始化 world size，rank，master addr，master port 等环境变量</a>，为后面真正的 model 的分布式进程组初始化做准备。</p>

<figure id="fig-OpenRLHF-pipeline">
  <img src="/images/OpenRLHF-pipeline.png" alt="OpenRLHF pipeline" style="width:100%">
  <figcaption>图 7: OpenRLHF 的 PPO 训练框架</figcaption>
</figure>

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;"><b>第二阶段：</b>首先是<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/cli/train_ppo_ray.py#L143-L160" target="_blank">初始化</a> <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ppo_trainer.py#L360" target="_blank">PPOTrainer</a>，它是 PPO 训练和生成阶段的主要类，其初始化主要包括初始化 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ppo_trainer.py#L43" target="_blank">tokenizer</a>，将之前初始化的各个 model 的分布式进程组<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ppo_trainer.py#L44-L49" target="_blank">赋值给自身的变量</a>，以及 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ppo_utils/experience_maker.py#L237" target="_blank">SamplesGenerator</a> 的<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ppo_trainer.py#L406-L411" target="_blank">初始化</a> (用于使用 vllm 封装的 actor model 进行 rollout)，<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ppo_utils/experience_maker.py#L406" target="_blank">RemoteExperienceMaker</a> 的<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ppo_trainer.py#L413-L422" target="_blank">初始化</a> (用于 PPO 的生成阶段)，<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ppo_trainer.py#L424" target="_blank">构建数据集</a>等其他参数/日志的初始化。接着是 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/cli/train_ppo_ray.py#L168" target="_blank">actor model</a>，<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/cli/train_ppo_ray.py#L167" target="_blank">ref model</a>，<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/cli/train_ppo_ray.py#L176" target="_blank">critic model</a> 和 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/cli/train_ppo_ray.py#L170" target="_blank">reward model</a> 的分布式进程组的初始化。以 actor model 为例，其主要是<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/launcher.py#L280-L290" target="_blank">调用</a>每个分布式进程的 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/ppo_actor.py#L360" target="_blank">init_model_from_pretrained()</a> 方法。在该方法中，首先<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/ppo_actor.py#L373" target="_blank">调用</a> strategy 的 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/utils/deepspeed/deepspeed.py#L79" target="_blank">setup_distributed()</a> 来初始化分布式进程组的<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/utils/deepspeed/deepspeed.py#L97" target="_blank">通信后端</a>和分布式进程组的 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/utils/deepspeed/deepspeed.py#L102-L104" target="_blank">device mesh</a>。接着<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/ppo_actor.py#L375-L388" target="_blank">初始化</a> <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/models/actor.py#L17">Actor</a> 类，其主要是加载指定的 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/models/actor.py#L87-L94" target="_blank">hf model</a>，接着初始化 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/ppo_actor.py#L392-L394" target="_blank">tokenizer</a>，<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/ppo_actor.py#L409-L411" target="_blank">优化器</a>，<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/ppo_actor.py#L413-L419" target="_blank">学习率 scheduler</a>，并<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/ppo_actor.py#L427-L429" target="_blank">通过</a> strategy 的 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/utils/deepspeed/deepspeed.py#L205-L250" target="_blank">prepare()</a> 方法将 model，优化器和学习率 scheduler 使用 deepspeed 后端进行<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/ppo_actor.py#L427-L430" target="_blank">封装</a>。最后是<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/ppo_actor.py#L453-L464" target="_blank">构建</a> <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/ppo_actor.py#L33" target="_blank">ActorPPOTrainer</a> 用于 actor model 的 train。至此，PPO 的所有初始化便结束了。</p>

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;"><b>第三阶段：</b>这一阶段主要是<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/cli/train_ppo_ray.py#L180" target="_blank">使用</a> PPOTrainer 的 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ppo_trainer.py#L433" target="_blank">fit()</a> 方法执行 PPO 的生成和训练阶段。在该方法中，首先是加载 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ppo_trainer.py#L439-L454" target="_blank">checkpoint</a> (这里的 checkpoint 指的是整个训练环境，包括 model，dataloader等的 checkpoint state)。接着开始 <code style="color: #B58900"><a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ppo_trainer.py#L456">args.num_episodes</a></code> 次 epoch 的训练。对于<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ppo_trainer.py#L465" target="_blank">每一次</a>训练，首先是<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ppo_trainer.py#L467-L469" target="_blank">使用</a> SamplesGenerator 的 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ppo_utils/experience_maker.py#L246" target="_blank">generate_samples()</a> 生成 response，其主要是<a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ppo_utils/experience_maker.py#L259" target="_blank">调用</a> <a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ppo_utils/experience_maker.py#L288" target="_blank">_generate_vllm()</a> 方法，将给定的 prompt <a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ppo_utils/experience_maker.py#L327" target="_blank">均匀分配</a>给每一个 vllm 包装的 actor model，并通过 vllm 的 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ppo_utils/experience_maker.py#L328" target="_blank">add_requests()</a> 和 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ppo_utils/experience_maker.py#L334" target="_blank">get_responses()</a> 来请求和返回生成的 response，最后将其<a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ppo_utils/experience_maker.py#L335-L370" target="_blank">整理</a>并使用 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ppo_utils/experience_maker.py#L372-L379" target="_blank">Experience</a> 类进行存放。得到 response 后，接着便<a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ppo_trainer.py#L505" target="_blank">使用</a> RemoteExperienceMaker 的 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ppo_utils/experience_maker.py#L436">make_experience_batch()</a> 方法执行 PPO 的生成阶段的剩下部分，包括将 response <a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ppo_utils/experience_maker.py#L450-L452" target="_blank">组成 batch</a>，<a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ppo_utils/experience_maker.py#L456" target="_blank">调用</a> <a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ppo_utils/experience_maker.py#L463" target="_blank">make_experience()</a> 方法生成 $p_{RL}$，$p_{SFT}$，$V$，$R$ 以及计算 $KL$，最后<a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ppo_utils/experience_maker.py#L459" target="_blank">调用</a> <a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ppo_utils/experience_maker.py#L614" target="_blank">compute_advantages_and_returns()</a> 方法生成 $A$ 和 return。如前所述，make_experience() 和 compute_advantages_and_returns() 这两个方法是通过调用各个 model 的分布式进程组的方式为使用统一的 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ray/launcher.py#L307" target="_blank">async_run_method_batch()</a> 接口，并传入具体需要调用的 model 方法的名字来实现的。在 make_experience() 方法中，首先是调用 reward model 的 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ray/launcher.py#L171" target="_blank">forward()</a> 方法生成 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ppo_utils/experience_maker.py#L496C46-L501" target="_blank">reward $R$</a>，然后是调用 actor model 的 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ray/ppo_actor.py#L486" target="_blank">forward()</a> 方法获取 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ppo_utils/experience_maker.py#L509-L514" target="_blank">action logits $p_{RL}$</a>，接着是调用 critic model 的 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ray/ppo_critic.py#L222" target="_blank">forward()</a> 方法获取 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ppo_utils/experience_maker.py#L527-L532">value $V$</a>，调用 ref model 的 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ray/launcher.py#126" target="_blank">forward()</a> 方法获取 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ppo_utils/experience_maker.py#L527-L532">sft logits $p_{SFT}$</a>，最后利用 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/models/utils.py#L7" target="_blank">compute_approx_kl()</a> 方法 (其中包括 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/models/utils.py#L22" target="_blank">k1</a>，<a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/models/utils.py#L30-L31" target="_blank">k2</a>，<a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/models/utils.py#L22" target="_blank">k3</a> 三种方式近似计算 $KL$) 计算 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/models/utils.py#L36-L38" target="_blank">KL divergence $KL$</a>。在 compute_advantages_and_returns() 方法中，首先是对 reward 的<a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ppo_utils/experience_maker.py#L627-L649" target="_blank">后处理</a> (主要是 PPO 外的其他 RL 算法需要)，接着是使用 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/models/utils.py#L43" target="_blank">compute_reward()</a> 方法将 $KL$ <a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ppo_utils/experience_maker.py#L653-L659" target="_blank">融入到</a> reward 中，然后是使用 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ppo_utils/experience_maker.py#L722" target="_blank">get_advantages_and_returns()</a> 方法计算 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ppo_utils/experience_maker.py#L662-L668" target="_blank">advantage $A$ 和 return</a>，最后对 $A$ 进行<a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ppo_utils/experience_maker.py#L717" target="_blank">归一化</a>处理。而 get_advantages_and_returns() 方法里的内容和 <a href="#DeepSpeedChat pipeline">DeepSpeedChat</a>类似，是标准的使用 GAE 计算 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ppo_utils/experience_maker.py#L759-L764" target="_blank">advantage $A$</a> 和 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ppo_utils/experience_maker.py#L765" target="_blank">return</a> 的过程。</p>

<p style="text-align: justify; text-justify: inter-ideograph; word-break: break-all;">在使用 RemoteExperienceMaker 的 make_experience_batch() 方法完成 PPO 的生成阶段后，接下来便是 PPO 的训练阶段。首先是<a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ppo_trainer.py#L510-L514" target="_blank">调用</a> actor model 的 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ray/ppo_actor.py#L512" target="_blank">append()</a> 方法和 critic model 的 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ray/ppo_critic.py#L243" target="_blank">append()</a> 方法将 PPO 生成阶段的结果存到每个分布式进程的 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ppo_utils/replay_buffer.py#L137" target="_blank">NaiveReplayBuffer</a> 中，接着便<a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ppo_trainer.py#L517">使用</a> <a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ppo_trainer.py#L118">ppo_train()</a> 方法执行 PPO 训练阶段，其内部是调用了 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ppo_trainer.py#L127" target="_blank">critic model</a> 的 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ray/ppo_critic.py#L247" target="_blank">fit()</a> 方法和 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ppo_trainer.py#L139" target="_blank">actor model</a> 的 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ray/ppo_actor.py#L466" target="_blank">fit()</a> 方法分别进行 actor model 和 critic model 的 train，最后将更新的 actor model 参数使用 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ppo_trainer.py#L155" target="_blank">_broadcast_to_vllm()</a> 方法<a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ppo_trainer.py#L147" target="_blank">同步到</a> vllm 封装的 actor model 中。在 critic model 的 fit() 方法中，主要是<a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ray/ppo_critic.py#L251">调用</a>第二阶段<a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ray/ppo_critic.py#L213-L220">初始化</a>的 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ray/ppo_critic.py#L24" target="_blank">CriticPPOTrainer</a> 的 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ray/ppo_critic.py#L251">ppo_train()</a> 方法。而 ppo_train() 方法内主要包括将 NaiveReplayBuffer 中的 PPO 生成阶段的结果<a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ray/ppo_critic.py#L62-L69">封装</a>为 DataLoader，对于 Dataloader 中的<a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ray/ppo_critic.py#L80">每个 batch</a>，<a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ray/ppo_critic.py#L82">调用</a> <a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ray/ppo_critic.py#L99">training_step()</a> 方法进行 train (包括<a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ray/ppo_critic.py#L110-L118">生成 new value $V^t$</a>，<a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ray/ppo_critic.py#L121-L126">计算 critic loss</a>，<a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ray/ppo_critic.py#L133-L134">调用</a> strategy 的 </a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/utils/deepspeed/deepspeed.py#L143" target="_blank">backward()</a> 方法和 </a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/utils/deepspeed/deepspeed.py#L148" target="_blank">optimizer_step()</a> 方法进行 backward 和 critic model 的参数更新)。在 actor model 的 fit() 方法中，主要是<a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ray/ppo_actor.py#L470">调用</a>第二阶段<a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ray/ppo_actor.py#L453-L264">初始化</a>的 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ray/ppo_actor.py#L33" target="_blank">ActorPPOTrainer</a> 的 <a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ray/ppo_actor.py#L146">ppo_train()</a> 方法。而 ppo_train() 方法内主要包括将 NaiveReplayBuffer 中的 PPO 生成阶段的结果<a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ray/ppo_actor.py#L149-L156">封装</a>为 DataLoader，对于 Dataloader 中的<a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ray/ppo_actor.py#L167">每个 batch</a>，<a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ray/ppo_actor.py#L169">调用</a> <a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ray/ppo_actor.py#L199">training_step()</a> 方法进行 train (包括<a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ray/ppo_actor.py#L211-L219">生成 new action logits $p^t_{RL}$</a>，<a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ray/ppo_actor.py#L222-L227">计算 actor loss</a> 和可选的其他<a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ray/ppo_actor.py#L230-L247">辅助 loss</a>，<a href="https://github.com/OpenRLHF/OpenRLHF/blob/6d500d14f7484b08f055c537c46b3c803917b412/openrlhf/trainer/ray/ppo_actor.py#L254-L255">调用</a> strategy 的 </a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/utils/deepspeed/deepspeed.py#L143" target="_blank">backward()</a> 方法和 </a href="https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/utils/deepspeed/deepspeed.py#L148" target="_blank">optimizer_step()</a> 方法进行 backward 和 actor model 的参数更新)。</p>

<!-- 78532923368aeb058f62201489546d013df47710 -->
敬请期待🤪 (争取端午节放假结束之前完成)