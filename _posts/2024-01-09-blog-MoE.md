---
title: 'The Basic Knowledge of MoE'
date: 24-01-09
permalink: /posts/2023/12/blog-moe/
star: superior
tags:
  - 深度学习基本知识
---

<p style="text-align:justify; text-justify:inter-ideograph;">这篇博客主要讲解了使用 Mixture of Experts (MoE) 将多个模型进行组合的原理。</p>

MoE 的基本原理
===

<p style="text-align:justify; text-justify:inter-ideograph;">Mixture of Experts (MoE)，即<b>混合专家模型</b>，旨在增加少量的计算量和内存量下，尽可能地扩大模型规模，提高模型性能。
一般来说，CNN 的模型规模和计算量/内存量呈线性关系；而 Transformer 的模型规模更是和计算量呈二次关系 (在模型宽度，即输入序列上)。
</p>

MoE 在 Deep Learning 的应用
===

1. <p style="text-align:justify; text-justify:inter-ideograph;"><a href="https://openaccess.thecvf.com/content_cvpr_2017/html/Gross_Hard_Mixtures_of_CVPR_2017_paper.html" target="_blank" title="Hard Mixtures of Experts for Large Scale Weakly Supervised Vision">k-mean</a>：该方法使用 k-mean 聚类的方法来实现 Gating Model。
具体而言，假设有 $K$ 个 expert 模型，分别为 $H_1(x) ~ H_K(x)$ 和 $1$ 个 Gating 分类器 $T(x)$。</p>

References
===

1. [Expert Gate: Lifelong Learning with a Network of Experts](https://openaccess.thecvf.com/content_cvpr_2017/html/Aljundi_Expert_Gate_Lifelong_CVPR_2017_paper.html)

2. [Hash Layers For Large Sparse Models](https://proceedings.neurips.cc/paper_files/paper/2021/hash/92bf5e6240737e0326ea59846a83e076-Abstract.html)

3. [Hard Mixtures of Experts for Large Scale Weakly Supervised Vision](https://openaccess.thecvf.com/content_cvpr_2017/html/Gross_Hard_Mixtures_of_CVPR_2017_paper.html)

4. [Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer](https://arxiv.org/abs/1701.06538)

5. [Switch transformers: scaling to trillion parameter models with simple and efficient sparsity](https://dl.acm.org/doi/abs/10.5555/3586589.3586709)

6. [GLaM: Efficient Scaling of Language Models with Mixture-of-Experts](https://proceedings.mlr.press/v162/du22c.html)

7. [GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding](https://arxiv.org/abs/2006.16668)

8. [Mixture-of-Experts Meets Instruction Tuning:A Winning Combination for Large Language Models](https://arxiv.org/abs/2305.14705)

9. [Finetuned Language Models Are Zero-Shot Learners](https://arxiv.org/abs/2109.01652)

10. [Scaling vision with sparse mixture of experts](https://proceedings.neurips.cc/paper/2021/hash/48237d9f2dea8c74c2a72126cf63d933-Abstract.html)

11. [Mixture-of-Experts with Expert Choice Routing](https://proceedings.neurips.cc/paper_files/paper/2022/hash/2f00ecd787b432c1d36f3de9800728eb-Abstract-Conference.html)

12. [Language Modeling with Gated Convolutional Networks](https://proceedings.mlr.press/v70/dauphin17a.html?ref=https://githubhelp.com)

13. [Gaussian Error Linear Units (GELUs)](https://arxiv.org/abs/1606.08415)

14. [Mixture of Experts: How an Ensemble of AI Models Decide As One - Blog](https://deepgram.com/learn/mixture-of-experts-ml-model-guide)

15. [Mixture-of-Experts with Expert Choice Routing - Blog](https://blog.research.google/2022/11/mixture-of-experts-with-expert-choice.html?m=1)