---
title: 'The Basic Knowledge of MoE'
date: 24-01-09
permalink: /posts/2023/12/blog-moe/
star: superior
tags:
  - 深度学习基本知识
---

<p style="text-align:justify; text-justify:inter-ideograph;">这篇博客主要讲解了使用 Mixture of Experts (MoE) 将多个模型进行组合的原理。</p>

MoE 的基本原理
===

<p style="text-align:justify; text-justify:inter-ideograph;">Mixture of Experts (MoE)，即<b>混合专家模型</b>，是一种集成学习方法，旨在增加少量的计算量和内存量下，尽可能地扩大模型规模，提高模型性能。
一般来说，CNN 的模型规模和计算量/内存量呈线性关系；而 Transformer 的模型规模更是和计算量呈二次关系 (在模型宽度，即输入序列上)。
如果使用 Dense 模型(即对于每个输入数据都使用全部的参数进行计算)，则会始终无法突破这种限制。
因此，MoE 模型使用 Sparse (稀疏性)的方式"绕过"该限制以实现模型性能尽可能地提升。
同时，MoE 模型也引入了在复杂预测建模问题的特定子任务上训练专家的思想，
即对于一个复杂的预测建模任务，可以将其分解为多个特定子任务，并对每个子任务训练一个模型，使得对于不同子任务的输入数据，使用不同的模型进行处理。
具体而言，MoE 模型包括 $N$ 个 expert 模型和 $1$ 个 Gating 模型：每个 expert 模型处理不同的数据分布/不同的子任务下的数据，
而 Gating 模型需要通过输入的数据 $x$，判断其属于哪个范围的数据分布/哪个子任务，并将其分配给对应的 expert 模型进行处理。
(可以简单理解为每个 expert 模型分别处理整个高维数据空间的不同子空间，而 Gating 模型需要判断输入的数据输入哪个子空间，然后送入到对应的 expert 模型进行处理。)
因此，在 Deep Learning 之前的 Machine Learning 时代，首先需要将整个数据进行划分(这一步通常根据不同的任务需求有不同的划分方式)，
然后再针对划分好的数据使用 <a href="https://cai-jianfeng.github.io/posts/2023/12/blog-em_algorithm" target="_blank">EM</a> 算法分别训练 expert 模型和 Gating 模型。
因此如何进行<b>数据划分</b>便是一个主要问题。
而在 Deep Learning 时代，一般将 expert 模型和 Gating 模型都使用 Neural Network，
只需要输入数据与结果，便可使得模型自主学习数据划分、每个 expert 模型对应的数据范围以及 Gating 模型的数据划分预测。
但是，为了保证Gating 模型的数据划分预测的<b>稀疏性</b>，一般需要对 Gating 模型的输出进行显式限制。
同时，由于模型内部学习的不确定性，可能导致大部分数据都分配给少数的 expert 模型，需要对数据分配的平衡性进行限制。
因此，如何限制 Gating 模型输出的稀疏性、如何使用额外的方式保持 expert 模型处理数据的均衡、如何将 MoE 模型嵌入已有模型框架以及更好地融入现有硬件框架进行训练等，
都是目前需要考虑和改进的问题。</p>

MoE 在 Deep Learning 的应用
===

- <p style="text-align:justify; text-justify:inter-ideograph;">目前 MoE 模型主要与 LLM 和 Transformers 进行结合。具体而言，Transformers 中包含 Self-Attention、Cross-Attention 和 FFN $3$ 个子模块。
假设 $1$ 个 Transformers Encoder/Decoder 共有 $L$ 层，通过将每层/隔层的 <b>FFN^l</b> 模块扩展为 $N$ 个 ($\text{FFN}_1^l \sim \text{FFN}_N^l, l = 1,...,L$) 组成 $N$ 个 expert 模型，
并为每层/隔层训练 $1$ 个独立的 Gating 模型 $G^l(·), l=1,...,L$。则每层的输出为：
$$\bar{I}^l = \operatorname{LayerNorm}(\operatorname{Self-Attn}(I^l) + I^l); \\
O^l = \operatorname{LayerNorm}(\sum_{i=1}^NG^l_i(\bar{I}^l)FFN^l_i(\bar{I}^l) + \bar{I}^l)$$</p>

- <p style="text-align:justify; text-justify:inter-ideograph;"><a href="https://openaccess.thecvf.com/content_cvpr_2017/html/Gross_Hard_Mixtures_of_CVPR_2017_paper.html" target="_blank" title="Hard Mixtures of Experts for Large Scale Weakly Supervised Vision">K-means</a>：该方法使用 k-means 聚类的方法来实现 Gating Model。
它将每个 expert 模型所映射的特征空间视为 $1$ 个簇，则 $N$ 个 expert 模型分别表示 $N$ 个簇。
接着通过计算新数据 $x$ 的特征值到各个簇之间的最小距离，即可确定该数据应该由哪个 expert 模型进行处理。
同时，它使用分离式训练方式，先训练 Gating Model，在使用训练完成的 Gating Model 训练 $N$ 个 expert 模型。
具体而言，假设有数据集 $\mathcal{D} = \{x_i, y_i\}_N$。首先使用数据集 $\mathcal{D}$ 训练 $1$ 个 $L$ 层的分类器模型 $T$ (即 Gating Model)。
然后将其最后一层(即 $L-1$ 层)的输出值 $z_i = T^{L-1}(x_i)$ 作为数据 $x_i$ 的特征值，
则对于数据集 $\mathcal{D}$ 中的每个数据 $x_j$ 都能生成一个对应的特征值 $z_j$，组成特征集 $\mathcal{F} = \{x_j, z_j\}_N$。
接着针对特征集 $\mathcal{F}$ 进行 <b>k-means</b> 聚类，聚类个数为 $K$，则最终会获得 $K$ 个聚类中心 $c_1,...c_K$。
这些聚类中心即可视为不同的数据分布，需要不同的 expert 模型进行学习处理，即 $1$ 个聚类中心表示一个 expert 模型。
因此需要 $K$ 个 expert 模型，分别为 $H_1(x) \sim H_K(x)$。
接着对于训练集中的每个数据 $x$，计算其到各个聚类中心 $c_j$ 的距离，并选择最短距离的聚类中心 $c_i$ 所对应的 expert 模型 $H_{i}(x)$ 作为处理 $x$ 的模型。
在具体实现上，可以使用二值函数来实现对每个 expert 模型的选择，即：
$$T(x)_i= \begin{cases}1, & \text { if } i=\operatorname{argmin}_j\left\|T^{L-1}(x)-c_j\right\|_2 \\ 0, & \text { otherwise }\end{cases} \\
O = \sum_{j=1}^KT(x)_jH_j(x) = T(x)_iH_i(x)$$
由于对于每个数据 $x$，$T(x)$ 仅有第 $i$ 个位置为 $1$，其余位置均为 $0$，再使用加权求和方式进行计算，即可表示选择第 $i$ 个 expert 模型 $H_{i}(x)$。
由于<b>显式</b>使用 k-means 进行分类，保证每个 expert 模型 $H_i(·)$ 都只训练特定的数据，与 MoE 最原始的目的相一致。
而在训练完成后，对于一个新数据 $\bar{x}$，首先通过分类器模型 $T$ 输出其特征值：$\bar{z} = T^{L-1}(\bar{x})$。
然后使用公式 $(1)$ 计算需要使用的 expert 模型 $H_{i}(\bar{x})$，并最终获得输出 $\bar{O}$。
<span style="color: red">(补充 shared decoder)</span></p>

![MoE k-means](/images/paper_MoE_k-means.png)

- <p style="text-align:justify; text-justify:inter-ideograph;"><a href="https://proceedings.neurips.cc/paper_files/paper/2021/hash/92bf5e6240737e0326ea59846a83e076-Abstract.html" target="_blank" title="Hash Layers For Large Sparse Models">Hash</a>：该方法使用 hash 的方法来实现 Gating Model。
它使用预定义的 hash 函数，将输入数据 $x$ 映射为 $1 \sim N$ 之间的数字 $i$，表示 $x$ 由第 $i$ 个 expert 模型处理。
具体而言，假设有 $K$ 个 expert 模型，分别为 $H_1(x) \sim H_K(x)$。对于输入数据 $x$，首先使用给定的 hash 函数将其映射为数字 $i$，
然后使用第 $i$ 个 expert 模型 $H_i(x)$ 处理数据 $x$，最终得到输出 $O$：
$$O = H_{hash(x)}(x); hash(x) \sim [1, K]$$<span style="color: red">(补充其在 Transformer 框架的具体应用以及 Multihash-Layer)</span></p>

![MoE hash](/images/paper_Hash-MoE.png)

- <p style="text-align:justify; text-justify:inter-ideograph;"><a href="https://arxiv.org/abs/170-06538" target="_blank" title="Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer">Sparsely-Gated</a>：
假设有 $K$ 个 expert 模型，分别为 $H_1(x) \sim H_K(x)$。
对于输入数据 $x$，最简单的 Gating Model 是使用线性权重 $W_g$ 将数据 $x$ 映射为 $K \times 1$ 的维度，然后使用 $Softmax$ 函数将其转化为 expert 模型的权重，
最后对所有 expert 模型的输出进行加权求和：
$$O = \sum_{i=1}^KG(x)_iH_i(x); G(x) = softmax(x \times W_g)$$这种方法有一个问题，MoE 无法学习到精确的<b>稀疏权重</b>，
使得对于每个输入数据基本上都需要使用全部的 $K$ 个 expert 模型进行计算。这与 MoE 的原始目的相违背。
因此，需要显式地引入<b>稀疏限制</b>。具体而言，首先，对于 Gating Model，为了使其产生的权重稀疏，需要在 softmax 函数之前进行值限制：
只保留 $x \times W_g$ 前 $k$ 个值(从大到小)，将其余值设为 $-\infty$，则在经过 softmax 函数后，仅有前 $k$ 个值所对应的权重 $\neq 0$，而其余值所对应的权重均为 $0$。
此时，对于那些为 $0$ 的权重，就不需要计算其对应的 expert 模型的输出值：
$$O = \sum_{i=1}^KG(x)_iH_i(x); G(x) = softmax(KeepTopK(x \times W_g, k)) \\
\text { KeepTopK }(v, k)_i= \begin{cases}v_i & \text { if } v_i \text { is in the top } k \text { elements of } v \\ -\infty & \text { otherwise. }\end{cases}$$<span style="color: red">(补充 noise term 和 shrinking batch + network bandwidth + balance 问题的改进方法)</span></p>

![Sparsely-Gated Mixture-of-Experts Layer](/images/paper_MoE_layer.png)

- <p style="text-align:justify; text-justify:inter-ideograph;"><a href="https://dl.acm.org/doi/abs/10.5555/3586589.3586709" target="_blank" title="Switch transformers: scaling to trillion parameter models with simple and efficient sparsity">Switch Transformers</a>：
它对 MoE 模型的 Gating Model 进一步进行简化，使得对于每个数据 $x$，只会有 $1$ 个 expert 模型进行处理。
具体而言，它将上述的 Sparsely-Gated 模型中的 $topk$ 值设为 $1$，即只取 $x \times W_g$ 中的最大值，将其余值设为 $-\infty$：
$$O = \sum_{i=1}^KG(x)_iH_i(x); G(x) = softmax(KeepTopK(x \times W_g, 1)) \\
\text { KeepTopK }(v, k)_i= \begin{cases}v_i & \text { if } v_i \text { is in the top } k \text { elements of } v \\ -\infty & \text { otherwise. }\end{cases}$$<span style="color: red">(补充 Load Balancing loss + 改进的 Training and Fine-Tuning 方法)</span></p>

![Switch Transformers](/images/paper_Switch-Transformers.png)

![Switch Transformers Routing](/images/paper_Switch-Transformers_2.png)

- <p style="text-align:justify; text-justify:inter-ideograph;"><a href="https://proceedings.mlr.press/v162/du22c.html" target="_blank" title="GLaM: Efficient Scaling of Language Models with Mixture-of-Experts">GLaM</a>：
它与 Switch Transformers 相似，都是对 MoE 模型的 Gating Model 进一步进行简化。它使得对于每个数据 $x$，只会有 $2$ 个 expert 模型进行处理。
具体而言，它将上述的 Sparsely-Gated 模型中的 $topk$ 值设为 $2$，即只取 $x \times W_g$ 中前 $2$ 个值(从大到小)，将其余值设为 $-\infty$：
$$O = \sum_{i=1}^KG(x)_iH_i(x); G(x) = softmax(KeepTopK(x \times W_g, 2)) \\
\text { KeepTopK }(v, k)_i= \begin{cases}v_i & \text { if } v_i \text { is in the top } k \text { elements of } v \\ -\infty & \text { otherwise. }\end{cases}$$<span style="color: red">(补充 Training Dataset + 改进的 Transformer FFN)</span></p>

![GLaM](/images/paper_GLaM.png)

- <p style="text-align:justify; text-justify:inter-ideograph;"><a href="https://arxiv.org/abs/2006.16668" target="_blank" title="GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding">GShard</a>：
它与 GLaM 相似，都是对 MoE 模型的 Gating Model 进一步进行简化。
它通过利用 Gating Model 的输出值动态调整，使得对于每个数据 $x$，<b>最多</b>仅有 $2$ 个 expert 模型进行处理。
具体而言，它将上述的 Sparsely-Gated 模型中的 $topk$ 值设为 $2$，即只取 $x \times W_g$ 中前 $2$ 个值 $g_1$ 和 $g_2$ (从大到小)，将其余值设为 $0$。
接着将 $g_1, g_2$ 进行归一化，然后将 $g_1$ 所对应的 expert 模型 $e_1$ 对数据 $x$ 进行处理。
同时判断 $g_2$ 是否大于阈值 $T$：如果大于，则将 $g_2$ 所对应的 expert 模型 $e_2$ 对数据 $x$ 进行处理；反之，则丢弃 $g_2$，即设为 $0$：
$$g = softmax(W_g \times x); g_1, g_2 = KeepTopK(g, 2) \\
\text { KeepTopK }(v, k)_i= \begin{cases}v_i & \text { if } v_i \text { is in the top } k \text { elements of } v \\ -\infty & \text { otherwise. }\end{cases} \\
g_1 = g_1/(g_1 + g_2); g_2 = g_2/(g_1 + g_2); g_2 = \begin{cases} g_2 \text { if } g_2 \geq T \\ 0 \text { otherwise. }\end{cases}; T \sim U(0, 1) \\
O = \sum_{i=1}^Kg_iH_i(x)$$<span style="color: red">(补充 Local Group Dispatching 和 Auxiliary loss)</span></p>

![GShard MoE Model](/images/paper_GShard.png)

![Gshard Gating Algorithm](/images/paper_GShard_3.png)

![Gshard Architecture](/images/paper_GShard_2.png)

- <p style="text-align:justify; text-justify:inter-ideograph;"><a href="https://proceedings.neurips.cc/paper/2021/hash/48237d9f2dea8c74c2a72126cf63d933-Abstract.html" target="_blank" title="Scaling vision with sparse mixture of experts">V-MoE</a>：
它将 Sparsely-Gated 应用到 Vision 领域，并对 Gating Model 进一步进行改进。
具体而言，将 $\text{ KeepTopK }$ 函数应用于 $\text{ Softmax }$ 函数之后，同时引入正态分布噪声 $\epsilon$ 增强训练稳定性：
$$g(\mathbf{x})=\operatorname{KeepTopK}(\operatorname{softmax}(\mathbf{W} \mathbf{x}+\epsilon), k); \epsilon \sim \mathcal{N}(0, \frac{1}{E^2})\\
O = \sum_{i=1}^kg_iH_i(x)$$<span style="color: red">(补充 Batch Prioritized Routing (BPR) 和 load balancing losses)</span></p>

![V-MoE](/images/paper_V-MoE.png)

- <p style="text-align:justify; text-justify:inter-ideograph;"><a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/2f00ecd787b432c1d36f3de9800728eb-Abstract-Conference.html" target="_blank" title="Mixture-of-Experts with Expert Choice Routing">Expert Choice Routing</a>：
先前的 MoE 方法中的 Gating Model 都是针对每一个数据 $x$ 分配固定数量的 expert 模型。
这种方法忽视了数据之间的关联性(因为每个数据都是独立经过 Gating Model 进行 expert 模型选择)，
同时会造成计算资源不平衡(即可能导致大多数数据都集中在部分 expert 模型进行学习，而导致其他 expert 模型的空闲)。
因此，Expert Choice Routing 改变了分配策略，显式地平均分配数据至每个 expert 模型，即针对每个 expert 模型分配固定数量的数据 $x$。
具体而言，假设有 $n$ 个 $d$ 维数据 $X \in \mathbb{R}^{n \times d}$，$e$ 个 expert 模型，每个 expert 模型的数据处理容量为 $k$。
首先计算每个数据分配给各个 expert 模型的权重 $S \in \mathbb{R}^{n \times e}$；
然后根据 $S$ 使用 $\text{ KeepTopK }$ 函数选择每个 expert 模型的前 $k$ 个数据；
最后再使用 $S$ 中对应位置的权重对每个数据进行加权求和得到最终输出 $O \in \mathbb{R}^{n \times d}$。
再具体实现上，可以使用索引矩阵 $I \in \mathbb{R}^{e \times k}$ (其中 $(i,j)$ 位置表示第 $i$ 个 expert 模型选择第 $j$ 个数据)，
门控权重矩阵 $G \in \mathbb{R}^{e \times k}$ (其中 $(i,j)$ 位置表示第 $i$ 个 expert 模型选择第 $j$ 个数据的权重) 以及 one-hot 矩阵 $P \in \mathbb{R}^{e \times k \times n}$：
$$S=\operatorname{Softmax}\left(X \cdot W_g\right), \quad S \in \mathbb{R}^{n \times e} \\
G, I=\operatorname{TopK}\left(S^{\top}, k\right), P=\operatorname{Onehot}(I)$$<span style="color: red">(补充：expert capacity + MoE FFN + Additional Constraint)</span></p>

![MoE Expert Choice Routing](/images/paper_MoE_expert-choice.png)

References
===

1. [Expert Gate: Lifelong Learning with a Network of Experts](https://openaccess.thecvf.com/content_cvpr_2017/html/Aljundi_Expert_Gate_Lifelong_CVPR_2017_paper.html)

2. [Hash Layers For Large Sparse Models](https://proceedings.neurips.cc/paper_files/paper/2021/hash/92bf5e6240737e0326ea59846a83e076-Abstract.html)

3. [Hard Mixtures of Experts for Large Scale Weakly Supervised Vision](https://openaccess.thecvf.com/content_cvpr_2017/html/Gross_Hard_Mixtures_of_CVPR_2017_paper.html)

4. [Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer](https://arxiv.org/abs/170-06538)

5. [Switch transformers: scaling to trillion parameter models with simple and efficient sparsity](https://dl.acm.org/doi/abs/10.5555/3586589.3586709)

6. [GLaM: Efficient Scaling of Language Models with Mixture-of-Experts](https://proceedings.mlr.press/v162/du22c.html)

7. [GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding](https://arxiv.org/abs/2006.16668)

8. [Mixture-of-Experts Meets Instruction Tuning:A Winning Combination for Large Language Models](https://arxiv.org/abs/2305.14705)

9. [Finetuned Language Models Are Zero-Shot Learners](https://arxiv.org/abs/2109.01652)

10. [Scaling vision with sparse mixture of experts](https://proceedings.neurips.cc/paper/2021/hash/48237d9f2dea8c74c2a72126cf63d933-Abstract.html)

11. [Mixture-of-Experts with Expert Choice Routing](https://proceedings.neurips.cc/paper_files/paper/2022/hash/2f00ecd787b432c1d36f3de9800728eb-Abstract-Conference.html)

12. [Language Modeling with Gated Convolutional Networks](https://proceedings.mlr.press/v70/dauphin17a.html?ref=https://githubhelp.com)

13. [Gaussian Error Linear Units (GELUs)](https://arxiv.org/abs/1606.08415)

14. [Mixture of Experts: How an Ensemble of AI Models Decide As One - Blog](https://deepgram.com/learn/mixture-of-experts-ml-model-guide)

15. [Mixture-of-Experts with Expert Choice Routing - Blog](https://blog.research.google/2022/11/mixture-of-experts-with-expert-choice.html?m=1)