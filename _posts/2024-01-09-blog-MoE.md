---
title: 'The Basic Knowledge of MoE'
date: 24-01-09
permalink: /posts/2023/12/blog-moe/
star: superior
tags:
  - 深度学习基本知识
---

<p style="text-align:justify; text-justify:inter-ideograph;">这篇博客主要讲解了使用 Mixture of Experts (MoE) 将多个模型进行组合的原理。</p>

MoE 的基本原理
===

<p style="text-align:justify; text-justify:inter-ideograph;">Mixture of Experts (MoE)，即<b>混合专家模型</b>，旨在增加少量的计算量和内存量下，尽可能地扩大模型规模，提高模型性能。
一般来说，CNN 的模型规模和计算量/内存量呈线性关系；而 Transformer 的模型规模更是和计算量呈二次关系 (在模型宽度，即输入序列上)。
</p>

MoE 在 Deep Learning 的应用
===

1. <p style="text-align:justify; text-justify:inter-ideograph;"><a href="https://openaccess.thecvf.com/content_cvpr_2017/html/Gross_Hard_Mixtures_of_CVPR_2017_paper.html" target="_blank" title="Hard Mixtures of Experts for Large Scale Weakly Supervised Vision">k-means</a>：该方法使用 k-means 聚类的方法来实现 Gating Model。
它将每个 expert 模型所映射的特征空间视为 $1$ 个簇，则 $N$ 个 expert 模型分别表示 $N$ 个簇。
接着通过计算新数据 $x$ 的特征值到各个簇之间的最小距离，即可确定该数据应该由哪个 expert 模型进行处理。
同时，它使用分离式训练方式，先训练 Gating Model，在使用训练完成的 Gating Model 训练 $N$ 个 expert 模型。
具体而言，假设有数据集 $\mathcal{D} = \{x_i, y_i\}_N$。首先使用数据集 $\mathcal{D}$ 训练 $1$ 个 $L$ 层的分类器模型 $T$ (即 Gating Model)。
然后将其最后一层(即 $L-1$ 层)的输出值 $z_i = T^{L-1}(x_i)$ 作为数据 $x_i$ 的特征值，
则对于数据集 $\mathcal{D}$ 中的每个数据 $x_j$ 都能生成一个对应的特征值 $z_j$，组成特征集 $\mathcal{F} = \{x_j, z_j\}_N$。
接着针对特征集 $\mathcal{F}$ 进行 <b>k-means</b> 聚类，聚类个数为 $K$，则最终会获得 $K$ 个聚类中心 $c_1,...c_K$。
这些聚类中心即可视为不同的数据分布，需要不同的 expert 模型进行学习处理，即 $1$ 个聚类中心表示一个 expert 模型。
因此需要 $K$ 个 expert 模型，分别为 $H_1(x) \sim H_K(x)$。
接着对于训练集中的每个数据 $x$，计算其到各个聚类中心 $c_j$ 的距离，并选择最短距离的聚类中心 $c_i$ 所对应的 expert 模型 $H_{i}(x)$ 作为处理 $x$ 的模型。
在具体实现上，可以使用二值函数来实现对每个 expert 模型的选择，即：
$$T(x)_i= \begin{cases}1, & \text { if } i=\operatorname{argmin}_j\left\|\tilde{T}^{L-1}(x)-c_j\right\|_2 \\ 0, & \text { otherwise }\end{cases} \\
O = \sum_{j=1}^KT(x)_jH_j(x) = T(x)_iH_i(x)$$
由于对于每个数据 $x$，$T(x)$ 仅有第 $i$ 个位置为 $1$，其余位置均为 $0$，再使用加权求和方式进行计算，即可表示选择第 $i$ 个 expert 模型 $H_{i}(x)$。
由于<b>显式</b>使用 k-means 进行分类，保证每个 expert 模型 $H_i(·)$ 都只训练特定的数据，与 MoE 最原始的目的相一致。
而在训练完成后，对于一个新数据 $\bar{x}$，首先通过分类器模型 $T$ 输出其特征值：$\bar{z} = T^{L-1}(\bar{x})$。
然后使用公式 $(1)$ 计算需要使用的 expert 模型 $H_{i}(\bar{x})$，并最终获得输出 $\bar{O}$。</p>

![MoE k-means](/images/paper_MoE_k-means.png)

2. <p style="text-align:justify; text-justify:inter-ideograph;"><a href="https://proceedings.neurips.cc/paper_files/paper/2021/hash/92bf5e6240737e0326ea59846a83e076-Abstract.html" target="_blank" title="Hash Layers For Large Sparse Models">hash</a>：该方法使用 hash 的方法来实现 Gating Model。
它使用预定义的 hash 函数，将输入数据 $x$ 映射为 $1 \sim N$ 之间的数字 $i$，表示 $x$ 由第 $i$ 个 expert 模型处理。
具体而言，假设有 $K$ 个 expert 模型，分别为 $H_1(x) \sim H_K(x)$。对于输入数据 $x$，首先使用给定的 hash 函数将其映射为数字 $i$，
然后使用第 $i$ 个 expert 模型 $H_i(x)$ 处理数据 $x$，最终得到输出 $O$：
$$O = H_{hash(x)}(x)$$。</p>

![MoE hash](/images/paper_Hash-MoE.png)

References
===

1. [Expert Gate: Lifelong Learning with a Network of Experts](https://openaccess.thecvf.com/content_cvpr_2017/html/Aljundi_Expert_Gate_Lifelong_CVPR_2017_paper.html)

2. [Hash Layers For Large Sparse Models](https://proceedings.neurips.cc/paper_files/paper/2021/hash/92bf5e6240737e0326ea59846a83e076-Abstract.html)

3. [Hard Mixtures of Experts for Large Scale Weakly Supervised Vision](https://openaccess.thecvf.com/content_cvpr_2017/html/Gross_Hard_Mixtures_of_CVPR_2017_paper.html)

4. [Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer](https://arxiv.org/abs/1701.06538)

5. [Switch transformers: scaling to trillion parameter models with simple and efficient sparsity](https://dl.acm.org/doi/abs/10.5555/3586589.3586709)

6. [GLaM: Efficient Scaling of Language Models with Mixture-of-Experts](https://proceedings.mlr.press/v162/du22c.html)

7. [GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding](https://arxiv.org/abs/2006.16668)

8. [Mixture-of-Experts Meets Instruction Tuning:A Winning Combination for Large Language Models](https://arxiv.org/abs/2305.14705)

9. [Finetuned Language Models Are Zero-Shot Learners](https://arxiv.org/abs/2109.01652)

10. [Scaling vision with sparse mixture of experts](https://proceedings.neurips.cc/paper/2021/hash/48237d9f2dea8c74c2a72126cf63d933-Abstract.html)

11. [Mixture-of-Experts with Expert Choice Routing](https://proceedings.neurips.cc/paper_files/paper/2022/hash/2f00ecd787b432c1d36f3de9800728eb-Abstract-Conference.html)

12. [Language Modeling with Gated Convolutional Networks](https://proceedings.mlr.press/v70/dauphin17a.html?ref=https://githubhelp.com)

13. [Gaussian Error Linear Units (GELUs)](https://arxiv.org/abs/1606.08415)

14. [Mixture of Experts: How an Ensemble of AI Models Decide As One - Blog](https://deepgram.com/learn/mixture-of-experts-ml-model-guide)

15. [Mixture-of-Experts with Expert Choice Routing - Blog](https://blog.research.google/2022/11/mixture-of-experts-with-expert-choice.html?m=1)