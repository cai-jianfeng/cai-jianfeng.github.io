---
title: 'The Basic Knowledge of Torch Train Pipeline'
date: 24-05-05
update: 24-05-14
permalink: /posts/2024/05/blog-torch-pipeline/
star: superior
tags:
  - æ·±åº¦å­¦ä¹ åŸºæœ¬çŸ¥è¯†
---

<p style="text-align:justify; text-justify:inter-ideograph;">è¿™ç¯‡åšå®¢ä¸»è¦è®²è§£ PyTorch è®­ç»ƒæ¨¡å‹çš„æ•´ä¸ªæµç¨‹çš„å…·ä½“ç»†èŠ‚ï¼Œ
åŒ…æ‹¬å¦‚ä½•åœ¨å‰å‘è¿‡ç¨‹ä¸­æ„å»ºè®¡ç®—å›¾ï¼›åå‘ä¼ æ’­è¿‡ç¨‹ä¸­å¦‚ä½•è®¡ç®—å¹¶ä¿å­˜æ¢¯åº¦ï¼›ä¼˜åŒ–å™¨å¦‚ä½•æ ¹æ®æ¢¯åº¦æ›´æ–°æ¨¡å‹å‚æ•°ã€‚(å»ºè®®å…ˆé˜…è¯»æˆ‘ä¹‹å‰å…³äº torch.autograd çš„åšå®¢ <a href="https://cai-jianfeng.github.io/posts/2023/12/blog-code-pytorch-autograd/" target="_blank">The Basic Knowledge of PyTorch Autograd</a> )</p>

# Torch è®­ç»ƒçš„æ•´ä½“æµç¨‹

<p style="text-align:justify; text-justify:inter-ideograph;">æˆ‘ä»¬ä»¥æœ€ç®€å•çš„ä¹˜æ³•ä¸ºä¾‹ï¼šä¸¤ä¸ªæ ‡é‡ $x_1$ å’Œ $x_2$ ç›¸ä¹˜å¾—åˆ° $v$ï¼›ç„¶åä½¿ç”¨<code style="color: #B58900">v.backward()</code>å‡½æ•°åå‘è®¡ç®— $x_1$ å’Œ $x_2$ çš„æ¢¯åº¦ï¼›æœ€åä½¿ç”¨ SGD ä¼˜åŒ–å™¨æ›´æ–° $x_1$ å’Œ $x_2$ã€‚ä»£ç å¦‚ä¸‹ï¼š</p>

![simple torch mul pipeline](/images/simple_torch_pipeline.png)

<p style="text-align:justify; text-justify:inter-ideograph;">æ¥ç€æˆ‘ä»¬ä½¿ç”¨<code style="color: #B58900">torchviz</code>çš„<code style="color: #B58900">make_dot</code>å‡½æ•°è·å– PyTorch æ„å»ºçš„è®¡ç®—å›¾ï¼š</p>

![simple torch DAG](/images/simple_torch_DAG.png)

<p style="text-align:justify; text-justify:inter-ideograph;">å¯ä»¥çœ‹åˆ°ï¼Œè®¡ç®—å›¾çš„æ–¹å‘ä¸å‰å‘è®¡ç®—è¿‡ç¨‹åˆšå¥½ç›¸åã€‚è¿™é‡Œï¼Œæˆ‘ä»¬å°†ç®€å•æè¿° Torch è®­ç»ƒçš„æ•´ä½“æµç¨‹ï¼šåœ¨æ‰§è¡Œä¹˜æ³•è¿‡ç¨‹ä¸­ï¼ŒTorch åˆ†åˆ«ä¸º $x_1$ å’Œ $x_2$ æ„å»ºä¸€ä¸ª<code style="color: #B58900">AccumulateGrad</code>èŠ‚ç‚¹ï¼Œå¹¶å°† $x_1$ / $x_2$ å­˜å‚¨åœ¨å¯¹åº”çš„<code style="color: #B58900">AccumulateGrad</code>èŠ‚ç‚¹çš„<code style="color: #B58900">variable</code>å±æ€§ä¸­ï¼›ç„¶åæ ¹æ®<code style="color: #B58900">*</code>çš„ä¹˜æ³•æ“ä½œä¸º $v$ æ„å»ºä¸€ä¸ª<code style="color: #B58900">MulBackwrad0</code>èŠ‚ç‚¹ï¼Œå¹¶å°†å…¶å­˜å‚¨åœ¨ $v$ çš„<code style="color: #B58900">grad_fn</code>å±æ€§ä¸­ã€‚</p>

<p style="text-align:justify; text-justify:inter-ideograph;">è€Œåœ¨åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦çš„è¿‡ç¨‹ä¸­ï¼Œæ‰§è¡Œ<code style="color: #B58900">v.backward()</code>å‡½æ•°æ—¶ï¼ŒTorch é¦–å…ˆä¼šè·å–åˆ°å­˜å‚¨åœ¨ $v$ çš„<code style="color: #B58900">grad_fn</code>å±æ€§ä¸­<code style="color: #B58900">MulBackwrad0</code>èŠ‚ç‚¹ï¼Œç„¶åå°†åˆå§‹æ¢¯åº¦<code style="color: #B58900">gradient</code>ä½œä¸ºè¾“å…¥ä¼ é€’ç»™å…¶<code style="color: #B58900">.backward()</code>å‡½æ•°è®¡ç®—è¯¥èŠ‚ç‚¹çš„è¾“å…¥çš„æ¢¯åº¦ï¼Œå³ $x_1$ å’Œ $x_2$ çš„æ¢¯åº¦ï¼›æ¥ç€å°† $x_1$ å’Œ $x_2$ çš„æ¢¯åº¦ä½œä¸ºè¾“å…¥ä¼ é€’ç»™å„è‡ªå¯¹åº”çš„<code style="color: #B58900">AccumulateGrad</code>èŠ‚ç‚¹çš„<code style="color: #B58900">.backward()</code>å‡½æ•°å®ç°å°†æ¢¯åº¦ç´¯åŠ åˆ° $x_1$ å’Œ $x_2$ çš„<code style="color: #B58900">.grad</code>å±æ€§ä¸­ã€‚</p>

<p style="text-align:justify; text-justify:inter-ideograph;">åœ¨ SGD ä¼˜åŒ–å™¨æ›´æ–° $x_1$ å’Œ $x_2$ çš„è¿‡ç¨‹ä¸­ï¼ŒSGD çš„<code style="color: #B58900">step()</code>å‡½æ•°éå†åˆå§‹åŒ–æ—¶ä¼ å…¥çš„<code style="color: #B58900">params</code>å‚æ•°ï¼Œåˆ¤æ–­å…¶<code style="color: #B58900">required_grad</code>å±æ€§æ˜¯å¦ä¸º<code style="color: #B58900">True</code>ï¼Œè‹¥ä¸º<code style="color: #B58900">True</code>ï¼Œåˆ™å–å‡ºå…¶<code style="color: #B58900">data</code>å±æ€§å’Œ<code style="color: #B58900">grad</code>å±æ€§ï¼Œå°†<code style="color: #B58900">data</code>å‡å»<code style="color: #B58900">grad</code>ï¼Œå¾—åˆ°æ›´æ–°åçš„å‚æ•°<code style="color: #B58900">params</code>ã€‚</p>

# å‰å‘è¿‡ç¨‹æ„å»ºè®¡ç®—å›¾

<p style="text-align:justify; text-justify:inter-ideograph;">ä»‹ç»åŸºæœ¬çŸ¥è¯†ï¼šNode $\rightarrow$ Edge $\rightarrow$ MulBackward0</p>

<p style="text-align:justify; text-justify:inter-ideograph;">å™è¿°è¿‡ç¨‹ï¼šTensor.mul $\rightarrow$ torch._C._TensorBase.__mul__ $\rightarrow$ mul_Tensor $\rightarrow$ collect_next_edges $\rightarrow$ gradient_edge $\rightarrow$ set_next_edges $\rightarrow$ set_history $\rightarrow$ set_gradient_edge</p>

<p style="text-align:justify; text-justify:inter-ideograph;">ä¸çŸ¥é“ä½ ä»¬æœ‰æ²¡æœ‰è¿™æ ·çš„ç–‘æƒ‘ï¼šåœ¨æˆ‘ä»¬çš„ä»£ç ä¸­ï¼Œåªæ˜¯ç®€å•çš„ç¼–å†™äº†ä¸¤ä¸ª<code style="color: #B58900">tensor</code>çš„çŸ©é˜µç›¸ä¹˜ï¼š<code style="color: #B58900">tensor = tensor1 * tensor2</code>ï¼›è€Œ PyTorch ä¾¿è‡ªåŠ¨ä¸ºæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªè®¡ç®—å›¾(å¯ä»¥çœ‹åˆ°<code style="color: #B58900">tensor</code>çš„<code style="color: #B58900">.grad_fn</code>å±æ€§ä¸º<code style="color: #B58900">MulBackward0</code>ï¼›å¦‚æœ<code style="color: #B58900">tensor1 / tensor2</code>çš„<code style="color: #B58900">.required_grad</code>å±æ€§ä¸º<code style="color: #B58900">True</code>)ã€‚è¿™æ˜¯å¦‚ä½•å®ç°çš„ï¼Ÿè™½ç„¶æˆ‘ä»¬åœ¨å‰é¢çš„åšå®¢ <a href="https://cai-jianfeng.github.io/posts/2023/12/blog-code-pytorch-autograd/" target="_blank">The Basic Knowledge of PyTorch Autograd</a> ä¸­è®²äº†å…³äº PyTorch è‡ªåŠ¨æ±‚å¯¼çš„è¿‡ç¨‹ï¼ŒçŸ¥é“äº†æ¯ä¸ªèŠ‚ç‚¹æ˜¯åœ¨åˆç­‰å‡½æ•°æ‰§è¡Œæ—¶ç«‹å³åˆ›å»ºçš„ï¼Œä½†å¹¶æ²¡æœ‰æ¶‰åŠåˆ°å…·ä½“çš„ä»£ç å¯¹åº”è¿‡ç¨‹(å³åœ¨<code style="color: #B58900">tensor = tensor1 * tensor2</code>èƒŒåç©¶ç«Ÿæ˜¯å“ªäº›ä»£ç å®ç°äº†è®¡ç®—å›¾çš„åˆ›å»º)ã€‚å®é™…ä¸Šï¼ŒPyTorch åœ¨<code style="color: #B58900">Tensor</code>ç±»ä¸­å®ç°äº†å¯¹æ¯ä¸ªåˆç­‰å‡½æ•°çš„<b>é‡è½½</b>ï¼Œä½¿å¾—æ¯ä¸ªåˆç­‰å‡½æ•°æ“ä½œå¹¶ä¸åªæ˜¯ç®€å•çš„å®ç°åˆç­‰å‡½æ•°è€Œå·²ã€‚ä¾‹å¦‚å¯¹äº<code style="color: #B58900">mul</code>æ“ä½œï¼Œ<code style="color: #B58900">Tensor</code>ç±»å†…çš„é‡è½½å®ç°ä¸ºï¼š</p>

![tensor mul operation](/images/tensor_mul.png)

<p style="text-align:justify; text-justify:inter-ideograph;">å¯ä»¥çœ‹åˆ°ï¼Œå…¶å†…éƒ¨å®ç°æ˜¯ä½¿ç”¨ C++ è¯­è¨€æ¥ç¼–å†™çš„ï¼Œç»§ç»­è¿½æº¯åˆ° C++ æºä»£ç ä¸­ï¼Œå¯ä»¥çœ‹åˆ°<code style="color: #B58900">mul</code>æ“ä½œçš„å…·ä½“å®ç°ä¸ºï¼š</p>

![tensor mul operation in C++](/images/tensor_mul_c++.png)

<p style="text-align:justify; text-justify:inter-ideograph;">è¿™ä¸ªä»£ç æœ‰ç‚¹å“äººï¼Œè®©æˆ‘ä»¬ä¸€æ­¥æ­¥æ¥ã€‚å…¶ä¸­ï¼Œ<code style="color: #B58900">self, other</code>åˆ†åˆ«æ˜¯<code style="color: #B58900">mul</code>æ“ä½œçš„ç¬¬ä¸€ä¸ª<code style="color: #B58900">tensor</code>å’Œç¬¬äºŒä¸ª<code style="color: #B58900">tensor</code>ã€‚é¦–å…ˆï¼Œç¬¬ $4$ è¡Œä»£ç çš„<code style="color: #B58900">compute_requires_grad()</code>å‡½æ•°åˆ¤æ–­<code style="color: #B58900">self/other</code>çš„<code style="color: #B58900">required_grad</code>å±æ€§æ˜¯å¦ä¸º<code style="color: #B58900">True</code>ï¼Œåªè¦æœ‰ä¸€ä¸ªä¸º<code style="color: #B58900">True</code>ï¼Œåˆ™<code style="color: #B58900">_any_requires_grad</code>ä¸º<code style="color: #B58900">True</code>ï¼Œè¡¨ç¤ºæ­¤æ—¶çš„<code style="color: #B58900">mul</code>æ“ä½œéœ€è¦ç”ŸæˆèŠ‚ç‚¹ï¼ŒåŒæ—¶å…¶ç”Ÿæˆçš„è¾“å‡ºçš„<code style="color: #B58900">required_grad</code>ä¹Ÿä¸º<code style="color: #B58900">True</code>ã€‚åœ¨å¾—åˆ°<code style="color: #B58900">_any_requires_grad</code>ä¸º<code style="color: #B58900">True</code>å(ç¬¬ $6$ è¡Œä»£ç )ï¼Œä»£ç ä¼šåˆ›å»ºä¸€ä¸ª<code style="color: #B58900">MulBackward0</code>ä½œä¸ºè¯¥<code style="color: #B58900">mul</code>æ“ä½œåœ¨è®¡ç®—å›¾ä¸Šçš„èŠ‚ç‚¹(ç¬¬ $8$ è¡Œä»£ç )ï¼ŒåŒæ—¶å°†å…¶èµ‹å€¼ç»™<code style="color: #B58900">grad_fn</code>ï¼›è€Œ<code style="color: #B58900">set_next_edges()</code>åˆ™æ˜¯è®¾ç½®å½“å‰çš„<code style="color: #B58900">MulBackward0</code>èŠ‚ç‚¹ä¸ä¹‹å‰æ“ä½œç”Ÿæˆçš„èŠ‚ç‚¹çš„è¿æ¥ã€‚</p>

<p style="text-align:justify; text-justify:inter-ideograph;">æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬ç»§ç»­æ·±å…¥æ¯ä¸ªéƒ¨åˆ†ã€‚é¦–å…ˆï¼Œ<code style="color: #B58900">self/other</code>æ˜¯ä¸€ä¸ª<code style="color: #B58900">Tensor</code>ï¼Œå½“è®¾ç½®å…¶<code style="color: #B58900">required_grad</code>çš„å±æ€§ä¸º True æ—¶ï¼Œä¼šæ‰§è¡Œä¸‹é¢çš„<code style="color: #B58900">set_requires_grad()</code>å‡½æ•°ï¼š</p>

![set requires grad](/images/required_grad_set.png)

<p style="text-align:justify; text-justify:inter-ideograph;">å…¶ä¼šä¸º<code style="color: #B58900">self/other</code>åˆ›å»ºä¸€ä¸ªæ–°çš„å±æ€§<code style="color: #B58900">autograd_meta_</code>(<code style="color: #B58900">AutogradMeta</code>ç±»)ï¼Œè¯¥å±æ€§ç”¨äºå­˜å‚¨<code style="color: #B58900">self/other</code>çš„æ¢¯åº¦(<code style="color: #B58900">grad_</code>)å’ŒèŠ‚ç‚¹(<code style="color: #B58900">grad_fn_</code>)ç­‰)ï¼Œå¯¹åº”äº Python ä»£ç é‡Œçš„<code style="color: #B58900">.grad</code>å’Œ<code style="color: #B58900">grad_fn</code>å±æ€§ã€‚(å½“ç„¶å…¶è¿˜æœ‰æ¢¯åº¦ç´¯åŠ å™¨(<code style="color: #B58900">grad_accumulator_</code>ç”¨äºç´¯åŠ å¤šä¸ªçˆ¶èŠ‚ç‚¹ä¼ é€’çš„æ¢¯åº¦)</p>

<p style="text-align:justify; text-justify:inter-ideograph;">å…¶æ¬¡ï¼Œè®¡ç®—å›¾çš„æ¯ä¸ªèŠ‚ç‚¹çš„ç±»å‹å‡ä¸º<code style="color: #B58900">Node</code>ç»“æ„ä½“(å¯¹åº”äº Python ä»£ç ä¸­çš„<code style="color: #B58900">Function</code>ç±»)ã€‚ä¸‹å›¾æ˜¯<code style="color: #B58900">Node</code>ç»“æ„ä½“çš„å…·ä½“å†…å®¹ï¼š</p>

![Node structure](/images/Node_class_c++.png)

<p style="text-align:justify; text-justify:inter-ideograph;">å…¶ä¸­ï¼Œ<code style="color: #B58900">operator()</code>å’Œ<code style="color: #B58900">apply()</code>åˆ†åˆ«æ˜¯èŠ‚ç‚¹çš„å‰å‘å’Œåå‘è®¡ç®—å‡½æ•°(å¯¹åº”äº Python ä»£ç ä¸­çš„<code style="color: #B58900">forward()</code>å’Œ<code style="color: #B58900">backward()</code>å‡½æ•°)ï¼Œä¸åŒçš„èŠ‚ç‚¹å¯ä»¥é‡å†™å®ƒä»¬ä»¥å®ç°ä¸åŒçš„è®¡ç®—è¿‡ç¨‹ã€‚è€Œ<code style="color: #B58900">next_edges_</code>åˆ™æ˜¯å­˜å‚¨èŠ‚ç‚¹æ‰€è¿æ¥çš„å‰å‘èŠ‚ç‚¹(å¯¹åº”äº Python ä»£ç ä¸­çš„<code style="color: #B58900">next_functions</code>)ã€‚å› æ­¤ï¼Œ<code style="color: #B58900">next_edges_</code>ä¸­çš„æ¯æ¡è¾¹éƒ½æ˜¯<code style="color: #B58900">Edge</code>ç»“æ„ä½“ï¼Œç»“æ„ä½“ä¸­å­˜å‚¨æ‰§è¡Œå‰å‘èŠ‚ç‚¹çš„æŒ‡é’ˆã€‚ä¸‹å›¾æ˜¯<code style="color: #B58900">Edge</code>ç»“æ„ä½“çš„å…·ä½“å†…å®¹ï¼š</p>

![edge structure](/images/edge_class_c++.png)

<p style="text-align:justify; text-justify:inter-ideograph;">æ‰€ä»¥ï¼Œ<code style="color: #B58900">mul_Tensor()</code>å‡½æ•°ä¸­çš„<code style="color: #B58900">MulBackward0</code>æ“ä½œå³æ˜¯<code style="color: #B58900">Node</code>ç»“æ„ä½“çš„å­ç»“æ„ä½“ï¼Œå…¶ä¸»è¦é‡å†™äº†<code style="color: #B58900">apply()</code>æ–¹æ³•ç”¨äºè®¡ç®—<code style="color: #B58900">mul</code>æ“ä½œçš„åå‘è¿‡ç¨‹(å…¶æ²¡æœ‰é‡å†™<code style="color: #B58900">operator()</code>æ–¹æ³•ï¼Œå› ä¸º<code style="color: #B58900">mul</code>æ“ä½œçš„å‰å‘è¿‡ç¨‹åœ¨<code style="color: #B58900">mul_Tensor()</code>å‡½æ•°ä¸­å®ç°)ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š</p>

![mulbackward structure](/images/multibackward0_c++.png)

<p style="text-align:justify; text-justify:inter-ideograph;">äº†è§£äº†å„ä¸ªå˜é‡çš„åŸºæœ¬ç»“æ„åï¼Œæˆ‘ä»¬å›åˆ°<code style="color: #B58900">mul_Tensor()</code>å‡½æ•°ä¸­ã€‚å¯ä»¥çŒœåˆ°ï¼Œ<code style="color: #B58900">set_next_edges()</code>åº”è¯¥æ˜¯è¦å°†ä¹‹å‰æ“ä½œç”Ÿæˆçš„èŠ‚ç‚¹èµ‹å€¼åˆ°å½“å‰çš„<code style="color: #B58900">MulBackward0</code>èŠ‚ç‚¹çš„<code style="color: #B58900">next_edges_</code>ä¸­ã€‚é¦–å…ˆéœ€è¦è·å–ä¹‹å‰æ“ä½œç”Ÿæˆçš„èŠ‚ç‚¹ï¼Œé€šè¿‡<code style="color: #B58900">collect_next_edges()</code>å‡½æ•°å®ç°ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š</p>

![collect next edges](/images/collect_next_edges_function_c++.png)

<p style="text-align:justify; text-justify:inter-ideograph;">è¿™ä¸ªä»£ç æ›´å“äººï¼Œè¿˜æ˜¯è®©æˆ‘ä»¬ä¸€æ­¥æ­¥æ¥ï¼é¦–å…ˆï¼Œ<code style="color: #B58900">collect_next_edges()</code>å‡½æ•°æ˜¯é€šè¿‡è¾“å…¥<code style="color: #B58900">mul</code>æ“ä½œçš„è¾“å…¥æ•°æ®ï¼Œå³<code style="color: #B58900">self, other</code>ï¼›ç„¶ååˆ›å»º<code style="color: #B58900">MakeNextFunctionList</code>ç»“æ„ä½“çš„å®ä¾‹<code style="color: #B58900">make</code>ï¼Œå¹¶è°ƒç”¨å…¶<code style="color: #B58900">apply()</code>æ–¹æ³•(å³<code style="color: #B58900">MakeNextFunctionList</code>çš„<code style="color: #B58900">operator()</code>æ–¹æ³•)å®ç°çš„è·å–ä¹‹å‰æ“ä½œç”Ÿæˆçš„èŠ‚ç‚¹ã€‚è€Œ<code style="color: #B58900">MakeNextFunctionList</code>çš„<code style="color: #B58900">operator()</code>æ–¹æ³•åŒæ ·è¾“å…¥<code style="color: #B58900">mul</code>æ“ä½œçš„è¾“å…¥æ•°æ®ï¼Œç„¶åæ„å»º<code style="color: #B58900">next_edges</code>æ•°ç»„ï¼Œæ¥ç€é€šè¿‡è°ƒç”¨<code style="color: #B58900">gradient_edge()</code>æ–¹æ³•è·å–æ¯ä¸ªè¾“å…¥æ•°æ®é‡Œä¿å­˜çš„ä¹‹å‰æ“ä½œç”Ÿæˆçš„èŠ‚ç‚¹(ä½¿ç”¨<code style="color: #B58900">Edge</code>ç»“æ„ä½“åŒ…è£…)ï¼Œå¹¶å°†å…¶å­˜å‚¨åœ¨<code style="color: #B58900">next_edges</code>æ•°ç»„ä¸­ï¼Œæœ€åå°†<code style="color: #B58900">next_edges</code>æ•°ç»„è¿”å›ç»™<code style="color: #B58900">collect_next_edges()</code>å‡½æ•°ã€‚è€Œ<code style="color: #B58900">gradient_edge()</code>æ–¹æ³•è¾“å…¥<code style="color: #B58900">mul</code>æ“ä½œçš„è¾“å…¥æ•°æ®ï¼Œåˆ¤æ–­å…¶æ˜¯å¦ä¿å­˜çš„ä¹‹å‰æ“ä½œç”Ÿæˆçš„èŠ‚ç‚¹<code style="color: #B58900">gradient = self.grad_fn()</code>ï¼šè‹¥æœ‰ï¼Œåˆ™è¯´æ˜è¯¥è¾“å…¥æ•°æ®å±äºä¸­é—´æ•°æ®ï¼Œåˆ™å°†å…¶åŒ…è£…æˆ<code style="color: #B58900">Edge</code>ç»“æ„ä½“åè¿”å›ï¼›è‹¥æ²¡æœ‰ï¼Œåˆ™è¯´æ˜è¯¥è¾“å…¥æ•°æ®å±äºæœ€åŸå§‹çš„è¾“å…¥æ•°æ®ï¼Œåˆ™å°†å…¶ä¿å­˜çš„èŠ‚ç‚¹è®¾ç½®ä¸º<code style="color: #B58900">AccumulateBackward</code>èŠ‚ç‚¹(é€šè¿‡è°ƒç”¨<code style="color: #B58900">grad_accumulator()</code>å‡½æ•°è·å¾—)ï¼Œå¹¶å…¶åŒ…è£…æˆ<code style="color: #B58900">Edge</code>ç»“æ„ä½“åè¿”å›ï¼ˆè¿™å°±æ˜¯ä¸ºä»€ä¹ˆæ¯ä¸ªå¶å­èŠ‚ç‚¹å‰éƒ½æœ‰ä¸€ä¸ª<code style="color: #B58900">AccumulateBackward</code>èŠ‚ç‚¹çš„åŸå› ï¼‰ã€‚ä»<code style="color: #B58900">gradient_edge()</code>æ–¹æ³•è¿”å›åˆ°<code style="color: #B58900">MakeNextFunctionList</code>çš„<code style="color: #B58900">operator()</code>æ–¹æ³•ï¼Œå†è¿”å›åˆ°<code style="color: #B58900">collect_next_edges()</code>å‡½æ•°ï¼Œå³å¯å¾—åˆ°å½“å‰çš„<code style="color: #B58900">MulBackward0</code>èŠ‚ç‚¹çš„ä¹‹å‰æ“ä½œç”Ÿæˆçš„èŠ‚ç‚¹ã€‚ç„¶åé€šè¿‡<code style="color: #B58900">set_next_edges()</code>å°†å…¶èµ‹å€¼åˆ°<code style="color: #B58900">next_edges_</code>ä¸­ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š</p>

![set next edges](/images/set_next_edges_c++.png)

<p style="text-align:justify; text-justify:inter-ideograph;">å®Œæˆäº†åœ¨<code style="color: #B58900">set_next_edges()</code>åï¼Œæ¥ä¸‹æ¥ä¾¿éœ€è¦è®¡ç®—å‰å‘è¿‡ç¨‹(å¯¹åº”äº Python ä»£ç ä¸­çš„<code style="color: #B58900">Function</code>ç±»çš„<code style="color: #B58900">forward()</code>æ–¹æ³•)ï¼Œè·å¾—è®¡ç®—ç»“æœ<code style="color: #B58900">result</code>(<code style="color: #B58900">mul_Tensor()</code>å‡½æ•°ä¸­çš„ç¬¬ $15 \sim 20$ è¡Œ)ã€‚</p>

<p style="text-align:justify; text-justify:inter-ideograph;">æœ€åï¼Œéœ€è¦å°†ç”Ÿæˆçš„<code style="color: #B58900">MulBackward0</code>èŠ‚ç‚¹ä¿å­˜åˆ°è¾“å‡º<code style="color: #B58900">result</code>ä¸­(å¯¹åº” Python ä»£ç çš„<code style="color: #B58900">outputs.grad_fn = now_fn</code>)ï¼Œå…¶é€šè¿‡<code style="color: #B58900">set_history()</code>å‡½æ•°å®ç°ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š</p>

![set_history_c++](/images/set_history_c++.png)

<p style="text-align:justify; text-justify:inter-ideograph;">é¦–å…ˆï¼Œ<code style="color: #B58900">set_history()</code>å‡½æ•°æ˜¯é€šè¿‡è¾“å…¥å‰å‘è¿‡ç¨‹çš„è¾“å‡º<code style="color: #B58900">result</code>å’Œç”Ÿæˆçš„<code style="color: #B58900">MulBackward0</code>èŠ‚ç‚¹ï¼Œç„¶åè°ƒç”¨<code style="color: #B58900">set_gradient_edge()</code>æ–¹æ³•å®ç°çš„å°†<code style="color: #B58900">MulBackward0</code>èŠ‚ç‚¹ä¿å­˜åœ¨è¾“å‡º<code style="color: #B58900">result</code>çš„<code style="color: #B58900">AutogradMeta</code>å±æ€§çš„<code style="color: #B58900">grad_fn_</code>ä¸­ã€‚è€Œ<code style="color: #B58900">set_gradient_edge()</code>æ–¹æ³•åˆ™æ˜¯é€šè¿‡è¾“å…¥åŒæ ·çš„å‰å‘è¿‡ç¨‹çš„è¾“å‡º<code style="color: #B58900">result</code>å’Œç”Ÿæˆçš„<code style="color: #B58900">MulBackward0</code>èŠ‚ç‚¹ï¼Œå–å‡º<code style="color: #B58900">result</code>çš„<code style="color: #B58900">AutogradMeta</code>å±æ€§<code style="color: #B58900">meta</code>ï¼Œå°†<code style="color: #B58900">MulBackward0</code>èµ‹å€¼åœ¨å…¶<code style="color: #B58900">grad_fn_</code>å±æ€§ä¸­ã€‚</p>

<p style="text-align:justify; text-justify:inter-ideograph;">è‡³æ­¤ï¼Œæˆ‘ä»¬ç»ˆäºâ€œç¨å¾®â€ææ‡‚äº† PyTorch è‡ªåŠ¨åŒ–æ„å»ºè®¡ç®—å›¾çš„è¿‡ç¨‹ã€‚åŸæ¥åœ¨æˆ‘ä»¬å†™äº†ä¸€ä¸ªç®€å•çš„<code style="color: #B58900">tensor = tensor1 * tensor2</code>ä»£ç èƒŒåï¼ŒPyTorch æ‰§è¡Œäº†å¦‚æ­¤å¤šçš„é¢å¤–ä»£ç æ“ä½œæ¥å®ç°è®¡ç®—å›¾çš„æ„å»ºã€‚</p>

# åå‘ä¼ æ’­è¿‡ç¨‹è®¡ç®—å¹¶ä¿å­˜æ¢¯åº¦

<p style="text-align:justify; text-justify:inter-ideograph;">æ•¬è¯·æœŸå¾…ï¼</p>

# ä¼˜åŒ–å™¨æ ¹æ®æ¢¯åº¦æ›´æ–°æ¨¡å‹å‚æ•°

<p style="text-align:justify; text-justify:inter-ideograph;">ä¸åŒäºå‰å‘è¿‡ç¨‹å’Œåå‘è¿‡ç¨‹ï¼Œå…¶ä»£ç éœ€è¦æ·±å…¥åˆ°åº•å±‚çš„ C++ æºä»£ç è¿›è¡Œç†è§£ï¼Œä¼˜åŒ–å™¨åˆ©ç”¨è®¡ç®—å¾—åˆ°çš„æ¢¯åº¦æ›´æ–°æ¨¡å‹å‚æ•°çš„è¿‡ç¨‹ä¸»è¦åœ¨ Python æºä»£ç ä¸­å®ç°ã€‚ç°åœ¨è®©æˆ‘ä»¬ä»¥æœ€ç®€å•çš„ SGD ä¼˜åŒ–å™¨ä¸ºä¾‹ï¼šé¦–å…ˆæˆ‘ä»¬éœ€è¦åˆå§‹åŒ–ä¸€ä¸ª SGD ä¼˜åŒ–å™¨å®ä¾‹ï¼Œå®ƒè‡³å°‘éœ€è¦è¾“å…¥ä¸¤ä¸ªå‚æ•°ï¼ˆæ¨¡å‹å‚æ•°<code style="color: #B58900">params</code>(å³ $x_1$ å’Œ $x_2$)å’Œåˆå§‹å­¦ä¹ ç‡<code style="color: #B58900">lr</code>ï¼‰ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š</p>

![SGD optimizer](/images/SGD_optimizer_construction_py.png)

<p style="text-align:justify; text-justify:inter-ideograph;">åœ¨ç»è¿‡å‰å‘è¿‡ç¨‹(<code style="color: #B58900">v = x1 * x2</code>)å’Œåå‘è¿‡ç¨‹(<code style="color: #B58900">v.backward()</code>)åï¼Œæ­¤æ—¶ $x_1$ å’Œ $x_2$ çš„<code style="color: #B58900">grad</code>å±æ€§å†…å·²ç»å­˜å‚¨äº†è®¡ç®—å¾—åˆ°çš„æ¢¯åº¦ã€‚å› æ­¤ï¼Œæˆ‘ä»¬èƒ½æƒ³åˆ°çš„æœ€ç›´æ¥çš„åšæ³•å°±æ˜¯éå†<code style="color: #B58900">params</code>çš„æ¯ä¸€ä¸ªå‚æ•°ï¼Œåˆ¤æ–­æ¯ä¸ªå‚æ•°çš„<code style="color: #B58900">required_grad</code>å±æ€§æ˜¯å¦ä¸º<code style="color: #B58900">True</code>ï¼›è‹¥æ˜¯ï¼Œåˆ™å–å‡ºå…¶å¯¹åº”çš„<code style="color: #B58900">grad</code>å±æ€§å†…å­˜å‚¨çš„æ¢¯åº¦ï¼Œå¹¶å°†è¯¥å‚æ•°ä¸å…¶æ¢¯åº¦(ä¹˜ä»¥å­¦ä¹ ç‡)è¿›è¡Œç›¸å‡å³å¯å®ç°å‚æ•°æ›´æ–°ã€‚å› æ­¤ SGD ç±»çš„ç®€å•å®ç°åº”è¯¥å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š</p>

![SDG class](/images/SGD_class_py.png)

<p style="text-align:justify; text-justify:inter-ideograph;">ä½†æ˜¯è¿™é‡Œæœ‰ä¸ªé—®é¢˜ï¼Œå‰é¢æˆ‘ä»¬è¯´è¿‡ï¼ŒPyTorch é‡è½½äº†<code style="color: #B58900">Tensor</code>ç±»çš„æ‰€æœ‰åˆç­‰å‡½æ•°æ“ä½œï¼›å› æ­¤ï¼Œå½“æˆ‘ä»¬æ‰§è¡Œ<code style="color: #B58900">param -= grad * self.lr</code>æ“ä½œæ—¶ï¼Œæˆ‘ä»¬å®é™…ä¸Šä¼šåœ¨åŸæœ‰è®¡ç®—å›¾çš„åŸºç¡€ä¸Šå†æ„å»ºä¸€ä¸ª<code style="color: #B58900">SubBackward0</code>èŠ‚ç‚¹åˆ†æ”¯ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š</p>

![simple SGD problem](/images/simple_SGD_problem.png)

<p style="text-align:justify; text-justify:inter-ideograph;">å› æ­¤ï¼Œä¸ºäº†ä¸è®© PyTorch ç»§ç»­æ„å»ºè®¡ç®—å›¾ï¼Œæˆ‘ä»¬éœ€è¦è®¾ç½®<code style="color: #B58900">with torch.no_grad()</code>æ¥â€œå‘Šè¯‰â€ PyTorch ä¸‹é¢çš„æ“ä½œä¸éœ€è¦æ„å»ºè®¡ç®—å›¾ï¼Œæ­¤æ—¶<code style="color: #B58900">Tensor</code>ç±»çš„æ‰€æœ‰åˆç­‰å‡½æ•°æ“ä½œå°±ä¸ä¼šæ„å»ºè®¡ç®—å›¾ã€‚å› æ­¤ï¼Œæ”¹è¿›çš„SGD ç±»ä»£ç å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š</p>

![advance SGD class](/images/SGD_class_advance_py.png)

<p style="text-align:justify; text-justify:inter-ideograph;">è€Œåœ¨ SGD çš„æºä»£ç ä¸­ï¼ŒPyTorch ä½¿ç”¨å¦ä¸€ç§æ–¹å¼æ¥é¿å…è®¡ç®—å›¾çš„æ„å»ºï¼Œé€šè¿‡ä½¿ç”¨<code style="color: #B58900">torch._dynamo.graph_break()</code>å®ç°è®¡ç®—å›¾çš„è„±ç¦»æ¥ç¡®ä¿åˆç­‰å‡½æ•°æ“ä½œå°±ä¸ä¼šç»§ç»­æ„å»ºè®¡ç®—å›¾ã€‚</p>

<p style="text-align:justify; text-justify:inter-ideograph;">äº†è§£äº†å¦‚ä½•ç®€å•å®ç° SGD åï¼Œæ¥ä¸‹æ¥è®©æˆ‘ä»¬è¿›å…¥ SGD çš„æºä»£ç æ¥éªŒè¯æˆ‘ä»¬çš„å®ç°æ˜¯å¦æ­£ç¡®ã€‚é¦–å…ˆæ˜¯ SGD å¦‚ä½•ä¿å­˜è¾“å…¥è¿›æ¥çš„<code style="color: #B58900">params</code>å‚æ•°ï¼Œä¸‹å›¾ä¸º SGD çš„<code style="color: #B58900">__init__()</code>å‡½æ•°éƒ¨åˆ†ä»£ç ï¼š</p>

![SGD source code init](/images/SGD_source_code_init.png)

<p style="text-align:justify; text-justify:inter-ideograph;">å¯ä»¥çœ‹åˆ°ï¼ŒSGD æ˜¯é€šè¿‡è°ƒç”¨å…¶çˆ¶ç±»<code style="color: #B58900">Optimizer</code>çš„<code style="color: #B58900">__init__()</code>å‡½æ•°å°†è¾“å…¥è¿›æ¥çš„å‚æ•°ä¿å­˜åœ¨<code style="color: #B58900">self.param_groups</code>åˆ—è¡¨å†…ã€‚æ¥ä¸‹æ¥å°±æ˜¯ SGD çš„<code style="color: #B58900">step()</code>å‡½æ•°ï¼Œä¸‹å›¾ä¸º SGD çš„<code style="color: #B58900">step()</code>å‡½æ•°éƒ¨åˆ†ä»£ç ï¼š</p>

![SGD source code step](/images/SGD_source_code_step.png)

<p style="text-align:justify; text-justify:inter-ideograph;">é¦–å…ˆï¼Œ<code style="color: #B58900">step()</code>å‡½æ•°å¯¹æ¯ä¸ª<code style="color: #B58900">self.param_groups</code>åˆ—è¡¨å†…çš„æ¯ä¸ªå‚æ•°ç»„<code style="color: #B58900">group</code>ï¼Œè°ƒç”¨<code style="color: #B58900">self._init_group()</code>åˆ¤æ–­å…¶æ¯ä¸ªå‚æ•°<code style="color: #B58900">p</code>çš„<code style="color: #B58900">grad</code>å±æ€§æ˜¯å¦ä¸º<code style="color: #B58900">None</code>ï¼šå¦‚æœä¸æ˜¯ï¼Œåˆ™è¡¨ç¤ºéœ€è¦æ›´æ–°è¯¥å‚æ•°ï¼Œåˆ™å°†å…¶å­˜å‚¨åœ¨<code style="color: #B58900">params_with_grad</code>åˆ—è¡¨ä¸­ï¼ŒåŒæ—¶ä½¿ç”¨<code style="color: #B58900">d_p_list</code>åˆ—è¡¨å­˜å‚¨å…¶å¯¹åº”çš„æ¢¯åº¦<code style="color: #B58900">p.grad</code>ã€‚</p>

<p style="text-align:justify; text-justify:inter-ideograph;">æ¥ç€ï¼Œå¯¹äºé‚£äº›éœ€è¦æ›´æ–°çš„å‚æ•°<code style="color: #B58900">params_with_grad</code>ï¼Œè°ƒç”¨<code style="color: #B58900">sgd()</code>å‡½æ•°è¿›è¡Œå‚æ•°æ›´æ–°ã€‚åœ¨<code style="color: #B58900">sgd()</code>å‡½æ•°ä¸­ï¼Œè¿›è¡Œä¸€ç³»åˆ—çš„æ£€æŸ¥åï¼Œè°ƒç”¨<code style="color: #B58900">_single_tensor_sgd()</code>å‡½æ•°è¿›è¡Œå‚æ•°æ›´æ–°ã€‚
è€Œ<code style="color: #B58900">_single_tensor_sgd()</code>å‡½æ•°åˆ™æ˜¯éå†<code style="color: #B58900">params_with_grad</code>åˆ—è¡¨ä¸­çš„æ‰€æœ‰å‚æ•°ï¼Œå¯¹äºæ¯ä¸ªå‚æ•°<code style="color: #B58900">param</code>åˆ—è¡¨ï¼Œå–å‡ºå…¶åœ¨<code style="color: #B58900">d_p_list</code>åˆ—è¡¨ä¸­çš„å¯¹åº”çš„æ¢¯åº¦<code style="color: #B58900">d_p</code>ï¼Œå¹¶ä½¿ç”¨<b>åŸåœ°æ›´æ–°</b>çš„æ–¹å¼è¿›è¡Œå‚æ•°æ›´æ–°ï¼š<code style="color: #B58900">param.add_(d_p, alpha=-lr)</code>ã€‚ç”±äºæ˜¯åŸåœ°æ›´æ–°ï¼Œä¸”ä¼ å…¥ä¼˜åŒ–å™¨çš„å‚ç…§å³ä¸ºæ¨¡å‹å‚æ•°ï¼Œå› æ­¤å¯¹åº”çš„æ¨¡å‹ä¸­çš„å‚æ•°ä¹Ÿä¼šåŒæ­¥è¿›è¡Œæ›´æ–°ã€‚</p>

<p style="text-align:justify; text-justify:inter-ideograph;">è‡³æ­¤ï¼Œæˆ‘ä»¬ç»ˆäºå®Œæˆäº† PyTorch è®­ç»ƒæ¨¡å‹çš„æ•´ä¸ªæµç¨‹çš„å…·ä½“ç»†èŠ‚ï¼ˆæ·±å…¥åˆ°åº•å±‚ä»£ç ï¼‰ï¼ŒåŒ…æ‹¬å¦‚ä½•åœ¨å‰å‘è¿‡ç¨‹ä¸­æ„å»ºè®¡ç®—å›¾ï¼›<text style="background-color:gray; color:white">åå‘ä¼ æ’­è¿‡ç¨‹ä¸­å¦‚ä½•è®¡ç®—å¹¶ä¿å­˜æ¢¯åº¦ï¼›</text>ä¼˜åŒ–å™¨å¦‚ä½•æ ¹æ®æ¢¯åº¦æ›´æ–°æ¨¡å‹å‚æ•°ã€‚ï¼ˆ<text style="background-color:gray; color:white">ç°åº•ç™½å­—</text>éƒ¨åˆ†è¡¨ç¤ºå°šæœªå®Œæˆéƒ¨åˆ†ğŸ¤ªï¼‰</p>

<!-- 1. optimizer ä¸­çš„ self.param_groups å’Œ self.states çš„ keys éƒ½æ˜¯ä¸ model.parameters() å…±äº«å†…å­˜ç©ºé—´ï¼Œå³å®ƒä»¬éƒ½æŒ‡å‘åŒä¸€ä¸ªå†…å­˜åŒºåŸŸ

1. dict çš„ keys(), values() å’Œ items() çš„è¿”å›å€¼ä¸ dict å…±äº«å†…å­˜ç©ºé—´ï¼Œå¯¹å…¶å€¼è¿›è¡Œâ€œåŸåœ°â€æ“ä½œä¼šåŒæ­¥ä¿®æ”¹ dict å†…çš„å€¼

2. torch.autograd ä¸ä¿å­˜ä¸­é—´å˜é‡ (å³å¯¹äº z = (x + y) ** 2ï¼Œtorch ä¸ä½¿ç”¨ä¸€ä¸ªé¢å¤–çš„å˜é‡ä¿æŒ x + y çš„å€¼)

3. torch.autograd.funtions.Function çš„é‡è¦å±æ€§ï¼š

-------------------------------------
_save_self / _save_other ä¸€èˆ¬æ˜¯ä¸ºäº†åå‘è¿‡ç¨‹æ—¶è®¡ç®—æ¢¯åº¦è€Œä¿æŒçš„å¿…è¦è¾“å…¥
_save_self
_save_other
-------------------------------------
variable -> åªåœ¨ AccumulateGrad ä¸­å‡ºç°


<p style="text-align:justify; text-justify:inter-ideograph;"><code style="color: #B58900">torch.autograd</code>ç†è®ºä¸Šéœ€è¦å¯å¾®å‡½æ•°æ‰èƒ½è®¡ç®—æ¢¯åº¦ï¼Œä½†æ˜¯å¹¶ä¸æ˜¯æ‰€æœ‰çš„å‡½æ•°åœ¨å…¶å®šä¹‰åŸŸå†…éƒ½æ˜¯å¯å¾®çš„ï¼Œä¾‹å¦‚ $ReLU$ åœ¨ $x=0$ æ—¶ä¸å¯å¾®ã€‚
ä¸ºæ­¤ï¼ŒPyTorch ä½¿ç”¨å¦‚ä¸‹çš„ä¼˜å…ˆçº§æ¥è®¡ç®—ä¸å¯å¾®å‡½æ•°çš„æ¢¯åº¦: </p>

1. <p style="text-align:justify; text-justify:inter-ideograph;">If the function is differentiable and thus a gradient exists at the current point, use it.</p>
2. <p style="text-align:justify; text-justify:inter-ideograph;">If the function is convex (at least locally), use the sub-gradient of minimum norm (it is the steepest descent direction).</p>
3. <p style="text-align:justify; text-justify:inter-ideograph;">If the function is concave (at least locally), use the super-gradient of minimum norm (consider -f(x) and apply the previous point).</p>
4. <p style="text-align:justify; text-justify:inter-ideograph;">If the function is defined, define the gradient at the current point by continuity (note that inf is possible here, for example for sqrt(0)). If multiple values are possible, pick one arbitrarily.</p>
5. <p style="text-align:justify; text-justify:inter-ideograph;">If the function is not defined (sqrt(-1), log(-1) or most functions when the input is NaN, for example) then the value used as the gradient is arbitrary (we might also raise an error but that is not guaranteed). Most functions will use NaN as the gradient, but for performance reasons, some functions will use other values (log(-1), for example).</p>
6. <p style="text-align:justify; text-justify:inter-ideograph;">If the function is not a deterministic mapping (i.e. it is not a mathematical function), it will be marked as non-differentiable. This will make it error out in the backward if used on tensors that require grad outside of a no_grad environment.</p>

Torch Grad Mode
===

<p style="text-align:justify; text-justify:inter-ideograph;"><code style="color: #B58900">torch.autograd</code> tracks operations on all tensors which have <code style="color: #B58900">requires_grad</code> flag set to True. 
For tensors that donâ€™t require gradients, setting this attribute to False excludes it from the gradient computation DAG.</p>

<p style="text-align:justify; text-justify:inter-ideograph;"><code style="color: #B58900">torch.no_grad()</code>: In this mode, the result of every computation will have <code style="color: #B58900">requires_grad=False</code>, 
even when the inputs have <code style="color: #B58900">requires_grad=True</code>. 
All factory functions, or functions that create a new Tensor and take a <code style="color: #B58900">requires_grad</code> kwarg, will <b>NOT</b> be affected by this mode.</p>

<p style="text-align:justify; text-justify:inter-ideograph;">Locally disabling gradient computation: requires_grad, grad mode, no_grad mode, inference mode:</p>

1. <p style="text-align:justify; text-justify:inter-ideograph;"><code style="color: #B58900">requires_grad</code> is a flag, defaulting to false unless wrapped in a <code style="color: #B58900">nn.Parameter</code>. 
During the forward pass, an operation is only recorded in the backward graph if at least one of its input tensors require grad. 
During the backward pass (<code style="color: #B58900">.backward()</code>), 
only leaf tensors with <code style="color: #B58900">requires_grad=True</code> will have gradients accumulated into their <code style="color: #B58900">.grad</code> fields.
Setting <code style="color: #B58900">requires_grad</code> only makes sense for leaf tensors (tensors that do not have a <code style="color: #B58900">grad_fn</code>, 
e.g., a <code style="color: #B58900">nn.Module</code>â€™s parameters),
all non-leaf tensors will automatically have <code style="color: #B58900">require_grad=True</code>.
apply <code style="color: #B58900">.requires_grad_(False)</code> to the parameters / <code style="color: #B58900">nn.Module</code>.</p>

2. <p style="text-align:justify; text-justify:inter-ideograph;">grad mode (default) is the only mode in which <code style="color: #B58900">requires_grad</code> takes effect.</p>

3. <p style="text-align:justify; text-justify:inter-ideograph;">no_grad mode: computations in no-grad mode are never recorded in the backward graph even if there are inputs that have <code style="color: #B58900">requires_grad=True</code>.
can use the outputs of these computations in grad mode later.
optimizer: when performing the training update youâ€™d like to update parameters in-place without the update being recorded by autograd. 
You also intend to use the updated parameters for computations in grad mode in the next forward pass.
torch.nn.init: rely on no-grad mode when initializing the parameters as to avoid autograd tracking when updating the initialized parameters in-place.</p>

4. <p style="text-align:justify; text-justify:inter-ideograph;">inference mode: computations in inference mode are not recorded in the backward graph. 
tensors created in inference mode will not be able to be used in computations to be recorded by autograd after exiting inference mode.</p>

5. <p style="text-align:justify; text-justify:inter-ideograph;">evaluation mode(<code style="color: #B58900">nn.Moudle.eval()</code> equivalently <code style="color: #B58900">module.train(False)</code>): 
<code style="color: #B58900">torch.nn.Dropout</code> and <code style="color: #B58900">torch.nn.BatchNorm2d</code> that may behave differently depending on training mode. </p>

|   Mode    | Excludes operations from being recorded in backward graph | Skips additional autograd tracking overhead | Tensors created while the mode is enabled can be used in grad-mode later |             Examples              |
|:---------:|:---------------------------------------------------------:|:-------------------------------------------:|:------------------------------------------------------------------------:|:---------------------------------:|
|  default  |                             Ã—                             |                      Ã—                      |                                    âˆš                                     |           Forward pass            |
|  no-grad  |                             âˆš                             |                      Ã—                      |                                    âˆš                                     |         Optimizer updates         |
| inference |                             âˆš                             |                      âˆš                      |                                    Ã—                                     | Data processing, model evaluation |

<p style="text-align:justify; text-justify:inter-ideograph;"></p>

<p style="text-align:justify; text-justify:inter-ideograph;"></p>

Appendix
===

## torch.autograd

<p style="text-align:justify; text-justify:inter-ideograph;">computational graph: input data (tensor) & executed operations (elementary operations, Function) in DAG, leaves are input tensors, roots are output tensors.</p>

<p style="text-align:justify; text-justify:inter-ideograph;">In a forward pass, autograd does two things simultaneously:</p>

- <p style="text-align:justify; text-justify:inter-ideograph;">run the requested operation to compute a resulting tensor; </p>

- <p style="text-align:justify; text-justify:inter-ideograph;">maintain the operationâ€™s gradient function in the DAG, the .grad_fn attribute of each torch.Tensor is an entry point into this graph. </p>

<p style="text-align:justify; text-justify:inter-ideograph;">The backward pass kicks off when <code style="color: #B58900">.backward()</code> is called on the DAG root. autograd then trace DAG from roots to leaves to compute gradient: </p>

- <p style="text-align:justify; text-justify:inter-ideograph;">computes the gradients from each <code style="color: #B58900">.grad_fn</code>; </p>

- <p style="text-align:justify; text-justify:inter-ideograph;">accumulates them in the respective tensorâ€™s <code style="color: #B58900">.grad</code> attribute; </p>

- <p style="text-align:justify; text-justify:inter-ideograph;">using the chain rule, propagates all the way to the leaf tensors. </p>

<p style="text-align:justify; text-justify:inter-ideograph;">DAGs are dynamic in PyTorch. An important thing to note is that the graph is recreated from scratch; after each <code style="color: #B58900">.backward()</code> call, 
autograd starts populating a new graph.</p>

## torch.autograd.Function

<p style="text-align:justify; text-justify:inter-ideograph;">Function objects (really expressions), which can be <code style="color: #B58900">apply()</code> ed to compute the result of evaluating the graph. </p>

<p style="text-align:justify; text-justify:inter-ideograph;">Some operations need intermediary results to be saved during the forward pass in order to execute the backward pass ($x \mapsto x^2$).
When defining a custom Python Function, you can use <code style="color: #B58900">save_for_backward()</code> to save tensors during the forward pass and <code style="color: #B58900">saved_tensors to</code> retrieve them during the backward pass.</p>

<p style="text-align:justify; text-justify:inter-ideograph;">You can explore which tensors are saved by a certain <code style="color: #B58900">grad_fn</code> by looking for its attributes starting with the prefix <code style="color: #B58900">_saved</code> (<code style="color: #B58900">_saved_self</code> / <code style="color: #B58900">_saved_result</code>).
To create a custom <code style="color: #B58900">autograd.Function</code>, subclass this class and implement the <code style="color: #B58900">forward()</code> and <code style="color: #B58900">backward()</code> static methods. 
Then, to use your custom op in the forward pass, call the class method <code style="color: #B58900">apply()</code>: </p>

![exp Function](/images/torch_autograd_Function.png)

<p style="text-align:justify; text-justify:inter-ideograph;">You can control how saved tensors are packed / unpacked by defining a pair of <code style="color: #B58900">pack_hook</code> / <code style="color: #B58900">unpack_hook</code> hooks.</p>

<p style="text-align:justify; text-justify:inter-ideograph;">The <code style="color: #B58900">pack_hook</code> function should take a tensor as its single argument but can return any python object (e.g. another tensor, 
a tuple, or even a string containing a filename). 
The <code style="color: #B58900">unpack_hook</code> function takes as its single argument the output of <code style="color: #B58900">pack_hook</code> and should return a tensor to be used in the backward pass. 
The tensor returned by <code style="color: #B58900">unpack_hook</code> only needs to have the same content as the tensor passed as input to <code style="color: #B58900">pack_hook</code>. </p>

![pack / unpack](/images/torch_autograd_pack.png)

<p style="text-align:justify; text-justify:inter-ideograph;">the <code style="color: #B58900">unpack_hook</code> should not delete the temporary file because it might be called multiple times: 
the temporary file should be alive for as long as the returned <code style="color: #B58900">SelfDeletingTempFile</code> object is alive.
register a pair of hooks on a saved tensor by calling the <code style="color: #B58900">register_hooks()</code> method on a SavedTensor object.</p>

<p style="text-align:justify; text-justify:inter-ideograph;"><code style="color: #B58900">
param.grad_fn._raw_saved_self.register_hooks(pack_hook, unpack_hook)
</code></p>

<p style="text-align:justify; text-justify:inter-ideograph;">use the context-manager <code style="color: #B58900">saved_tensors_hooks</code> to register a pair of hooks which will be applied to all saved tensors that are created in that context.
The hooks defined with this context manager are thread-local, using those hooks disables all the optimization in place to reduce Tensor object creation.</p>

![torch pack](/images/torch_autograd_pack_DDP.png) -->


# Reference

1. [Understanding PyTorch with an example: a step-by-step tutorial](https://towardsdatascience.com/understanding-pytorch-with-an-example-a-step-by-step-tutorial-81fc5f8c4e8e)

2. [The SGD source code of PyTorch](https://github.com/pytorch/pytorch/blob/cd9b27231b51633e76e28b6a34002ab83b0660fc/torch/optim/sgd.py#L63)

3. [A lightweight package to create visualizations of PyTorch execution graphs and traces](https://github.com/szagoruyko/pytorchviz)

4. [Overview of PyTorch Autograd Engine](https://pytorch.org/blog/overview-of-pytorch-autograd-engine/)

5. [How Computational Graphs are Constructed in PyTorch](https://pytorch.org/blog/computational-graphs-constructed-in-pytorch/)

6. [How Computational Graphs are Executed in PyTorch](https://pytorch.org/blog/how-computational-graphs-are-executed-in-pytorch/)

7. [PyTorch internals](http://blog.ezyang.com/2019/05/pytorch-internals/)

8. [Ultimate guide to PyTorch Optimizers](https://analyticsindiamag.com/ultimate-guide-to-pytorch-optimizers/)

9. [torch.optim](https://pytorch.org/docs/stable/optim.html)

10. [What is a PyTorch optimizer?](https://www.educative.io/answers/what-is-a-pytorch-optimizer)