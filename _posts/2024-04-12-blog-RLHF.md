---
title: 'The Basic Knowledge of RLHF (Reinforce Learning with Human Feedback)'
date: 24-04-12
update: 24-04-13
permalink: /posts/2024/04/blog-rlhf/
star: superior
tags:
  - æ·±åº¦å­¦ä¹ åŸºæœ¬çŸ¥è¯†
---

<p style="text-align:justify; text-justify:inter-ideograph;">è¿™ç¯‡åšå®¢ä¸»è¦è®²è§£å…³äº RLHF çš„åŸºç¡€çŸ¥è¯†å’Œè®­ç»ƒ LLM çš„å…·ä½“(ç®€æ˜“)ä»£ç å®ç°. </p>

# RLHF çš„åŸºæœ¬åŸç†

<p style="text-align:justify; text-justify:inter-ideograph;">é¦–å…ˆï¼ŒRLHF åˆ†ä¸º $3$ ä¸ªéƒ¨åˆ†ï¼š<b>é¢„è®­ç»ƒ LLM æ¨¡å‹ $M_\theta$ </b>ï¼›<b>é¢„è®­ç»ƒå¥–åŠ±æ¨¡å‹ $r_\theta$ </b> ä»¥åŠ <b>ä½¿ç”¨ PPO å¾®è°ƒ $M_\theta$</b>. </p>

<p style="text-align:justify; text-justify:inter-ideograph;"><b>ç¬¬ä¸€æ­¥ï¼šé¢„è®­ç»ƒ LLM æ¨¡å‹ $M_\theta$ </b> (å°±æ˜¯ä¸€ä¸ª Transformer Decoder å‡å» cross-attention)ï¼š$M_\theta$ è¾“å…¥ä¸€ä¸ª prompt (å³è¾“å…¥çš„æ–‡æœ¬) $x \in \mathbb{R}^{L_I \times 1}$, è¾“å…¥ä¸€ä¸ª response (å³è¾“å‡ºçš„æ–‡æœ¬) $y \in \mathbb{R}^{L_O \times 1}$. å’Œæ­£å¸¸çš„é¢„è®­ç»ƒ LLM è¿‡ç¨‹ä¸€æ ·ï¼Œæ”¶é›†å¤§é‡çš„æ–‡æœ¬æ•°æ®(ç§°ä¸ºè¯­æ–™åº“ corpus)ï¼Œä½¿ç”¨ predict-next-token çš„æ–¹å¼è¿›è¡Œé¢„è®­ç»ƒ LLM æ¨¡å‹ $M_\theta$. åœ¨é¢„è®­ç»ƒå®Œæˆåï¼Œä¸ºäº†æ¨¡å‹ $M_\theta$ åœ¨ç¬¬ $3$ æ­¥æœ‰ä¸€ä¸ªæ›´å¥½çš„åˆå§‹åŒ–åŸºç¡€ï¼Œå¯ä»¥å†æ”¶é›†ä¸€äº›ç»è¿‡<b>äººä¸ºæ ‡æ³¨é«˜è´¨é‡</b>çš„ Q & A æ•°æ®è¿›è¡Œå¾®è°ƒï¼Œç§°ä¸º <b>SFT</b> (Supervised Fine Tuning). </p>

<p style="text-align:justify; text-justify:inter-ideograph;"><b>ç¬¬äºŒæ­¥ï¼šé¢„è®­ç»ƒå¥–åŠ±æ¨¡å‹ $r_\theta$</b> (å’Œ $M_\theta$ ä¸€æ ·çš„æ¶æ„)ï¼šå®ƒçš„ä½œç”¨æ˜¯æ ¹æ®äººç±»çš„å–œå¥½å¯¹ $M_\theta$ çš„è¾“å‡ºè¿›è¡Œæ‰“åˆ†(score)ï¼Œè¿™ä¸€æ­¥å°±æ˜¯éœ€è¦<b>äººå·¥æ ‡æ³¨</b>çš„æ­¥éª¤. å…·ä½“è€Œè¨€ï¼Œç»™å®š prompt $x$ï¼ŒLLM æ¨¡å‹ $M_\theta$ è¾“å‡º $y$ï¼Œé€šè¿‡äººå·¥æ ‡æ³¨è¯¥è¾“å‡ºçš„å¾—åˆ†(score $s_{GT} \in R^{1 \times 1}$)ä½œä¸º groud-truth (GT)ï¼›ç„¶åå°†ä¸¤è€…è¿›è¡Œæ‹¼æ¥å¾—åˆ° $r_\theta$ çš„è¾“å…¥ï¼Œå³ $r_\theta$ çš„è¾“å…¥ä¸º $\theta$ çš„è¾“å…¥ + è¾“å‡º $z = concat(x, y) \in \mathbb{R}^{(L_I + L_O) \times 1}$, è¾“å‡ºæ¯ä¸ªè¯çš„ score (å³ reward) $r \in \mathbb{R}^{(L_I + L_O) \times 1}$ (æˆ–è€…åªå– $y$ çš„æ¯ä¸ªè¯çš„ score: $r \in \mathbb{R}^{L_O \times 1}$, å› ä¸ºæˆ‘ä»¬ä¸éœ€è¦è¯„ä¼°éƒ¨åˆ† $x$ çš„ score)ï¼Œæœ€åå°† $s_{GT}$ å¤åˆ¶ $(L_I + L_O) \text{or} L_O$ æ¬¡å¾—åˆ° $s_{GT}^{L} \in \mathbb{R}^{(L_I + L_O) \times 1} \text{or} \mathbb{R}^{L_O \times 1}$ï¼Œå°±å¯ä»¥ä½¿ç”¨ Cross-Entropy Loss è¿›è¡Œè®­ç»ƒ. ä½†æ˜¯ä¸€èˆ¬æ¥è¯´ï¼Œäººç±»å¯¹äºç›¸å¯¹åˆ†æ•°æ¯”è¾ƒæ•æ„Ÿ(ä¾‹å¦‚è°æ¯”è°å¥½)ï¼Œè€Œå¯¹äºç»å¯¹åˆ†æ•°ä¸å¤ªæ“…é•¿(æ¯”å¦‚ç»™è°æ‰“å‡ åˆ†). å› æ­¤ï¼Œä¸€èˆ¬ä¸ä½¿ç”¨ç›´æ¥äººå·¥æ ‡æ³¨æ¥è·å¾— $s_{GT}$ï¼Œè€Œæ˜¯è®© $M_\theta$ é’ˆå¯¹ $x$ è¾“å‡ºä¸åŒçš„ $y$ ($y_0,...,y_n$)ï¼Œç„¶åè®©äººç±»å¯¹è¿™ $n$ ä¸ªè¾“å‡ºè¿›è¡Œæ’åºå¾—åˆ°ç”±ä¼˜åˆ°åŠ£çš„è¾“å‡ºé¡ºåº $y_{k_1}, ..., y_{k_n}, k_j \in {1,...,n}$ï¼Œç„¶åä½¿ç”¨é¢„å®šä¹‰çš„æ–¹æ³•(ä¾‹å¦‚ Elo rating)å°†æ’åºè½¬åŒ–ä¸ºè¾“å‡ºçš„ score, åˆ™æ¯ä¸ªè¾“å‡º $y_{k_j}$ éƒ½å¯¹åº”ä¸€ä¸ª score $s_{GT_j}$. ç„¶åå°†è¿™ $n$ ä¸ªæ•°æ®å¯¹ $\{(x, y_{k_j}), s_{GT_j}\}$ ä½œä¸ºå¥–åŠ±æ¨¡å‹ $r_\theta$ çš„è®­ç»ƒæ•°æ®. </p>

<p style="text-align:justify; text-justify:inter-ideograph;">åœ¨æ­£å¼å¼€å§‹ PPO ç®—æ³•ä¹‹å‰ï¼Œå…ˆç®€å•è®²ä¸€ä¸‹ RL æ–¹æ³•ï¼ŒRL æ–¹æ³•ä¸»è¦åŒ…æ‹¬ $6$ ä¸ªéƒ¨åˆ†ï¼šenvironment $e$ï¼Œaction $a$ï¼Œstate $s$ï¼Œpolicy $\pi_\theta(\cdot)$ï¼Œreward $r$ å’Œ value $V(\cdot)$. å…¶<b>åŸºæœ¬æ¡†æ¶</b>ä¸ºï¼šç»™å®šä¸€ä¸ªåˆå§‹ state $s_0$ï¼Œpolicy æ ¹æ® $s_0$ é¢„æµ‹å¯¹åº”çš„ action $a_0 = argmax_{a \in \text{action space}}\pi_\theta(a|s_0)$ï¼Œç„¶åæ‰§è¡Œ action $a_0$ ä¸ environment è¿›è¡Œäº¤äº’ï¼Œå¾—åˆ°æ–°çš„ state $s_1$ å’Œåé¦ˆçš„ reward $r_0$ï¼›æ¥ç€åŸºäº state $s_1$ é¢„æµ‹æ–°çš„ actionï¼Œå¾ªç¯å¾€å¤ï¼Œç›´åˆ°è¾¾åˆ°ç»ˆæ­¢ stateï¼Œæœ€ç»ˆå¾—åˆ° policy é¢„æµ‹çš„ä¸€ä¸ª trajectory $\{(s_0, a_0),...,(s_m, a_m)\}$. è€Œ policy æ ¹æ®æ¯ä¸ª $s$ é€‰æ‹©åˆé€‚ $a$ çš„ç›®çš„æ˜¯ä½¿å¾—æœ€ç»ˆçš„ rewards æœ€å¤§åŒ–ï¼Œå³ $max(\sum_{i=0}{r_i})$. å› æ­¤ï¼Œpolicy ä¸èƒ½ä½¿ç”¨â€œè´ªå¿ƒâ€ç­–ç•¥ï¼Œè€Œæ˜¯éœ€è¦å…¨å±€è€ƒè™‘. ä¸ºäº†ç®€åŒ– reward çš„è¯„ä¼°ï¼Œé€šå¸¸å¼•å…¥ value $V(\cdot)$ï¼Œå…¶ $V(s)$ è¡¨ç¤ºåœ¨ state $s$ ä¸‹çš„<b>æ€»çš„æœªæ¥é¢„æœŸ</b> reward (the estimated expected total future rewards from that state $s$). æ­¤æ—¶ï¼Œåœ¨ state $s_0$ ä¸‹æ‰§è¡Œ action $a_0$ å¾—åˆ°æ–°çš„ state $s_1$ çš„æœªæ¥é¢„æœŸ rewards $\hat{r} = r_0 + V(s_1) \approx max(\sum_{i=0}{r_i})$. å› æ­¤ï¼Œæˆ‘ä»¬åªéœ€è¦åœ¨å½“å‰çŠ¶æ€å°±å¯ä»¥é¢„ä¼°åˆ°ç»ˆæ­¢ state æ—¶çš„å¤§æ¦‚ rewardsï¼Œè€Œä¸ç”¨çœŸçš„æ‰§è¡Œåˆ°ç»ˆæ­¢ state. è¿™æ ·å°±å¯ä»¥å®æ—¶è°ƒæ•´ policy (ä½¿å…¶æ¯æ¬¡å°½é‡é¢„æµ‹èƒ½ä½¿å¾— $argmax_{a}\hat{r}$ çš„ action $a$). åœ¨ç›®å‰çš„ DL èŒƒå¼ä¸‹ï¼Œé€šå¸¸å°† policy $\pi_\theta(\cdot)$ï¼Œreward $r$ å’Œ value $V(\cdot)$ éƒ½å»ºæ¨¡ä¸º NN æ¨¡å‹ $\pi_\theta(a|s)$ï¼Œ$r_\theta(s, a)$ å’Œ $V_\theta(s)$.</p>

<p style="text-align:justify; text-justify:inter-ideograph;"><b>ç¬¬ä¸‰æ­¥ï¼šä½¿ç”¨ PPO ç®—æ³•å¾®è°ƒ LLM æ¨¡å‹ $M_\theta$</b>ï¼šé¦–å…ˆï¼Œéœ€è¦å°†å¾®è°ƒ(FT)é—®é¢˜è½¬åŒ–ä¸ºå¼ºåŒ–å­¦ä¹ (RL)é—®é¢˜ï¼šé¢„è®­ç»ƒå¥½çš„ LLM æ¨¡å‹ $M_\theta$ ç§°ä¸º policy model $\pi_\theta(a|s)$ (å®ƒæ¯æ¬¡è¾“å‡ºä¸€ä¸ªè¯)ï¼›é¢„è®­ç»ƒå¥½çš„å¥–åŠ±æ¨¡å‹ $r_\theta$ ç§°ä¸º reward model $r_\theta(s, a)$. è€Œæ–‡æœ¬æ•°æ®ç§°ä¸ºçŠ¶æ€(state) $s$ï¼Œåˆ™è¾“å…¥çš„æ–‡æœ¬æ•°æ® $x$ ç§°ä¸ºåˆå§‹çŠ¶æ€ $s_0$ï¼›ä¸€ä¸ªè¯ç§°ä¸ºåŠ¨ä½œ(action) $a$. æ‰€ä»¥ï¼Œåˆå§‹æ—¶ policy model ä¸º $\pi_{\theta_0}(\cdot)$ï¼Œstate ä¸º $s_0$ï¼Œå°†çŠ¶æ€ $s_0$ è¾“å…¥ $\pi_{\theta_0}(\cdot)$ å¾—åˆ°é¢„æµ‹çš„ action $a_0 = \pi_{\theta_0}(a_0|s_0)$ï¼Œç„¶åå°† $a_0$ æ‹¼æ¥åˆ°çŠ¶æ€ $s_0$ åé¢ç”Ÿæˆæ–° state $s_1$ï¼Œå¹¶å°†æ–° state $s_1$ æ¨¡å‹ $\pi_{\theta_0}(\cdot)$ ç”Ÿæˆæ–° action $a_1$. å¦‚æ­¤å¾ªç¯å¾€å¤ç”Ÿæˆ policy model $\pi_{\theta_0}(\cdot)$ é¢„æµ‹çš„ trajectory $\{(s_0, a_0), ..., (s_m, a_m)\}$ (è¿™ä¸å°±æ˜¯ LLM æ¨¡å‹çš„ predict-next-token çš„æ–¹å¼å˜›ğŸ˜€ï¼Œæ‰€ä»¥ $\pi_{\theta_0}(\cdot)=M_\theta$ï¼Œ$a_i = y_i$ï¼Œ$s_i = concat(x,y_{[1:i-L_I]})$ï¼Œ$m=L_O$ï¼Œå°±æ˜¯ä¸ºäº†å¯¹é½ PPO ç®—æ³•æ¢äº†ä¸ªåå­—è€Œå·². æ³¨æ„ï¼šæ˜¯æ¯ä¸ªè¯æ˜¯ä¸€ä¸ª actionï¼Œè€Œä¸æ˜¯ä¸€ä¸ª $y$ æ˜¯ä¸€ä¸ª actionï¼Œä¸è¦å’Œç¬¬äºŒæ­¥æ’åºæ··æ·†äº†(æˆ‘å°±ææ··äº†ğŸ˜Ÿ)). </p>

<p style="text-align:justify; text-justify:inter-ideograph;">é¢˜å¤–è¯ï¼šè¿™æ®µè¯å®å±ç»å…¸ï¼Œå»ºè®®åå¤é˜…è¯»æ¥ç†è§£å¦‚ä½•å°† FT é—®é¢˜è½¬åŒ–ä¸º RL é—®é¢˜(from hf blog)ï¼šFirst, the <b>policy</b> is a language model that takes in a prompt and returns a sequence of text (or just probability distributions over text). The <b>action space</b> of this policy is all the tokens corresponding to the vocabulary of the language model (often on the order of 50k tokens) and the <b>observation space</b> is the distribution of possible input token sequences, which is also quite large given previous uses of RL (the dimension is approximately the size of vocabulary $\times$ length of the input token sequence). The <b>reward function</b> is a combination of the preference model and a constraint on policy shift.</p>

![PPO Clip](/images//RLHF_PPO-Clip.png)

<p style="text-align:justify; text-justify:inter-ideograph;">ä¸‹ä¸€æ­¥å°±æ˜¯æ„é€  PPO çš„ä¼˜åŒ–å‡½æ•°æ¥è®­ç»ƒ policy model $\pi_\theta(a|s)$. æ³¨æ„ï¼Œæ¥ä¸‹æ¥çš„ PPO éƒ½æ˜¯ä»¥ä¸€ä¸ª trajectory ä¸ºä¸€ä¸ªè¿­ä»£æ€»ä½“(ç±»ä¼¼ epoch)ï¼Œä»¥ä¸€ä¸ª $(s, a)$ ä¸ºä¸€æ¬¡è¿­ä»£åŸºå…ƒ(ç±»ä¼¼ item)ï¼Œå› æ­¤ä¸‹æ–‡ä¸­çš„ $t \in \{1, ..., m\}$. å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œ<b>åŸå§‹</b>çš„ PPO ä¼˜åŒ–å‡½æ•°ä¸ºï¼š</p>

$$L^{CLIP}(\theta)=\hat{\mathbb{E}}_t\Big[\min(r_t(\theta)\hat{A}_t,\operatorname{clip}(r_t(\theta),1-\epsilon,1+\epsilon)\hat{A}_t)\Big], r_t(\theta)=\frac{\pi_\theta(a_t\mid s_t)}{\pi_{\theta_{\mathrm{old}}}(a_t\mid s_t)}$$

<p style="text-align:justify; text-justify:inter-ideograph;">å…¶ä¸­ $\pi_{\theta_{\mathrm{old}}}(a_t\mid s_t)$ è¡¨ç¤ºä¹‹å‰è¿­ä»£çš„ policy model (å¯èƒ½æ¥è‡ªå‰ä¸€è½® epochï¼Œä¹Ÿå¯èƒ½å°±æ˜¯åˆå§‹åŒ–çš„)ï¼›$\pi_\theta(a_t\mid s_t)$ è¡¨ç¤ºç›®å‰æ­£åœ¨è¿­ä»£çš„ policy modelï¼›$r_t(\theta)$ è¡¨ç¤ºä¹‹å‰çš„ policy model å’Œç›®å‰çš„ policy model çš„è¾“å‡ºçš„å·®å¼‚(ç”¨æ¯”å€¼æ¥è¡¡é‡)ï¼›è€Œ $\operatorname{clip}(r_t(\theta),1-\epsilon,1+\epsilon)$ è¡¨ç¤ºå°† $r_t(\theta)$ çš„å€¼é™åˆ¶åœ¨ $(1-\epsilon,1+\epsilon)$ ä¹‹é—´æ¥ä¿è¯ policy model æ›´æ–°è¿­ä»£çš„å˜åŒ–ä¸èƒ½è¿‡å¤§(å³ä¸èƒ½åç¦» $\pi_{\theta_{\mathrm{old}}}(a_t\mid s_t)$ çš„åˆ†å¸ƒå¤ªå¤šï¼Œæœ‰ç‚¹ smooth-update çš„æ€æƒ³). $\hat{A}_t$ è¡¨ç¤º Advantageï¼Œç”¨äºè¡¡é‡å½“å‰é¢„æµ‹çš„ action $a_t$ å¾—åˆ°çš„ reward æ˜¯å¦æ¯”å…¶ä»–æ‰€æœ‰å¯èƒ½çš„ action çš„å¹³å‡ reward è¦é«˜ï¼šå¦‚æœæ˜¯ï¼Œåˆ™ $\hat{A}_t > 0$ï¼›åä¹‹ï¼Œåˆ™ $\hat{A}_t < 0$. å…¶è®¡ç®—å…¬å¼ä¸ºï¼š</p> 

$$\begin{aligned}&\delta_{t}=r_{t}+\gamma V(s_{t+1})-V(s_{t})\\&\hat{A}_{t}=\delta_{t}+\gamma\lambda\hat{A}_{t+1}\end{aligned}$$

<p style="text-align:justify; text-justify:inter-ideograph;">é¢˜å¤–è¯ï¼šæƒ³è¦äº†è§£ PPO çš„ä¼˜åŒ–å‡½æ•°çš„å…·ä½“æ¨å¯¼è¿‡ç¨‹åŠå…¶æ¯ä¸ªè¡¨è¾¾å¼çš„å«ä¹‰ï¼Œå¯ä»¥å‚è€ƒ <a href="https://huggingface.co/blog/deep-rl-ppo#recap-the-policy-objective-function" target="_blank">huggingface blog</a>. </p>

<p style="text-align:justify; text-justify:inter-ideograph;">å¯ä»¥çœ‹åˆ°ï¼Œ$\hat{A}_t$ çš„è®¡ç®—æ˜¯ä»åå¾€å‰æ¨çš„ï¼Œå®ƒéœ€è¦å…ˆè®¡ç®—å‡ºæ•´ä¸ª trajectoryï¼›å› æ­¤ï¼Œ$\hat{A}_t$ çš„è®¡ç®—æ˜¯åŸºäºä¸Šä¸€æ¬¡è¿­ä»£ç”Ÿæˆçš„ trajectoryï¼šå‡è®¾ä¸Šä¸€æ¬¡è¿­ä»£çš„ trajectory ä¸º $\{(s_0, a_0, V(s_0)), ..., (s_m, a_m, V(s_m))\}$ï¼Œç„¶åé€šè¿‡ä¸Šè¿°è®¡ç®—å…¬å¼å¾—åˆ° $\hat{A} = {\hat{A}_m, ..., \hat{A}_0}$ï¼Œç„¶åå°† $\hat{A}$ è¾“å…¥åˆ°ä¸‹ä¸€æ¬¡è¿­ä»£è¿›è¡Œä¼˜åŒ–å‡½æ•°å€¼çš„è®¡ç®—. </p>

<p style="text-align:justify; text-justify:inter-ideograph;">æ¥ä¸‹æ¥ï¼Œå°†<b>åŸå§‹</b>çš„ PPO ä¼˜åŒ–å‡½æ•°ä¸­çš„æ¯ä¸ªéƒ¨åˆ†éƒ½å¯¹åº”åˆ°å¾®è°ƒ LLM ä¸Š(å…¶å®ä¹Ÿåªæœ‰ $V(\cdot)$ å’Œ $L^{CLIP}(\theta)$ ç•¥æœ‰å·®å¼‚). åœ¨å‰è¿°ä¸­ï¼Œæˆ‘ä»¬æåˆ°é€šå¸¸å°† policy $\pi_\theta(\cdot)$ï¼Œreward $r$ å’Œ value $V(\cdot)$ éƒ½å»ºæ¨¡ä¸º NN æ¨¡å‹ $\pi_\theta(a|s)$ï¼Œ$r_\theta(s, a)$ å’Œ $V_\theta(s)$. ç°åœ¨æˆ‘ä»¬å·²ç»å®ç°äº† $\pi_\theta(a|s)$ï¼Œ$r_\theta(s, a)$ï¼Œé‚£ä¹ˆ $V_\theta(\cdot)$ å¦‚ä½•å®ç°ï¼Ÿå¾ˆç®€å•ï¼Œåªéœ€è¦åœ¨ $\pi_\theta(a|s)$ (å³ Transformer Decoder)çš„æœ€åå†å¹¶è¡Œå¢åŠ ä¸€ä¸ªçº¿æ€§å›å½’ MLPï¼Œä½¿å…¶é¢„æµ‹è¿ç»­å€¼ $v \in \mathbb{R}$ å³å¯ï¼›è€Œè¿™ä¸ª MLP çš„å‚æ•°æ˜¯éšæœºåˆå§‹åŒ–çš„(æ‰€ä»¥ç¬¬ä¸‰æ­¥ä¸­çš„æ‰€æœ‰ $V(\cdot) = V_\theta(\cdot)$). å› æ­¤ $\pi_\theta(\cdot)$ å’Œ $V_\theta(\cdot)$ å…±äº«å¤§éƒ¨åˆ†å‚æ•°. åŒæ—¶ï¼Œ$\pi_{\theta_{\mathrm{old}}}(\cdot)$ ä¸æ˜¯é‡‡ç”¨ä¸Šä¸€æ¬¡è¿­ä»£çš„ policy modelï¼Œè€Œæ˜¯ç›´æ¥é‡‡ç”¨åˆå§‹åŒ–çš„ policy model $\pi_{\theta_{0}}(\cdot)$ï¼Œå¹¶å¯¹å…¶åšäº†è¿›ä¸€æ­¥ç®€åŒ–ï¼Œæœ€ç»ˆå¾—åˆ°(å…¶ä¸­ $k$ è¡¨ç¤ºç¬¬ $k$ æ¬¡è¿­ä»£æ€»ä½“(å³ç¬¬ $k$ ä¸ª epoch))ï¼š</p>

$$\begin{align}
  L(s,a,\theta_k,\theta)&=\min\left(\frac{\pi_\theta(a|s)}{\pi_{\theta_k}(a|s)}A^{\pi_{\theta_k}}(s,a),\operatorname{clip}\left(\frac{\pi_\theta(a|s)}{\pi_{\theta_k}(a|s)},1-\epsilon,1+\epsilon\right)A^{\pi_{\theta_k}}(s,a)\right)\\&=\min\left(\frac{\pi_\theta(a|s)}{\pi_{\theta_k}(a|s)}A^{\pi_{\theta_k}}(s,a),g(\epsilon,A^{\pi_{\theta_k}}(s,a))\right), \left.g(\epsilon,A)=\left\{\begin{array}{ll}(1+\epsilon)A&A\geq0\\(1-\epsilon)A&A<0.\end{array}\right.\right.
\end{align}$$

$$\theta_{k+1}=\arg\max_\theta\sup_{s,a\sim\pi_{\theta_k}}\left[L(s,a,\theta_k,\theta)\right]$$

# RLHF çš„å…·ä½“(ç®€æ˜“)ä»£ç 

[simple pesudocode](/files/RLHF.py)

![RLHF code](/images/RLHF_code.png)

![RLHF code2](/images/RLHF_code2.png)

![RLHF code3](/images/RLHF_code3.png)

# References

1. [Illustrating Reinforcement Learning from Human Feedback (RLHF)](https://huggingface.co/blog/rlhf)

2. [Proximal Policy Optimization (PPO)](https://huggingface.co/blog/deep-rl-ppo#recap-the-policy-objective-function)

3. [Implementing RLHF with Custom Datasets](https://github.com/HumanSignal/RLHF/blob/master/tutorials/RLHF_with_Custom_Datasets.ipynb)

4. [Coding chatGPT from Scratch | Lecture 2: PPO Implementation](https://www.youtube.com/watch?v=CCTRyTAL72U)

5. [PPO_Implementation.ipynb](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbEJmV24ycS1td29UWVdEVl9MbTFOcnR6RlpCd3xBQ3Jtc0tsdXVyak92endpVUxrdFdNdnZWcllKVjBWSjNMZ3ZSbG9oelJ6QkYtMWpXakxHYUpieU1zVm5PYjdLYWxMWXQxc2VMVnNCZGRhXzRRb1JRaUFSR0hmaWxSWkMxRG1qWjFrd3dyVmxiNGx6MkNNa3ZRbw&q=https%3A%2F%2Fcolab.research.google.com%2Fdrive%2F1AmdtDNd_DUVRJlluaKIGT19BzBZ7loVZ%3Fusp%3Dsharing&v=CCTRyTAL72U)

6. [Coding chatGPT from Scratch | Lecture 1: PPO Theory](https://www.youtube.com/watch?v=3uvnoVjM8nY)

7. [RLHF (Reinforcement Learning From Human Feedback): Overview + Tutorial](https://www.v7labs.com/blog/rlhf-reinforcement-learning-from-human-feedback)

8. [The N Implementation Details of RLHF with PPO](https://huggingface.co/blog/the_n_implementation_details_of_rlhf_with_ppo#general-implementation-details)

9. [The 37 Implementation Details of Proximal Policy Optimization](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/)

10. [Approximating KL Divergence](http://joschu.net/blog/kl-approx.html)

11.  [RL â€” Proximal Policy Optimization (PPO) Explained](https://jonathan-hui.medium.com/rl-proximal-policy-optimization-ppo-explained-77f014ec3f12)

12. [Elo rating system](https://en.wikipedia.org/wiki/Elo_rating_system)
