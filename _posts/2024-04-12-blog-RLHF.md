---
title: 'The Basic Knowledge of RLHF (Reinforce Learning with Human Feedback)'
date: 24-04-12
update: 24-04-12
permalink: /posts/2024/04/blog-rlhf/
star: superior
tags:
  - æ·±åº¦å­¦ä¹ åŸºæœ¬çŸ¥è¯†
---

<p style="text-align:justify; text-justify:inter-ideograph;">è¿™ç¯‡åšå®¢ä¸»è¦è®²è§£å…³äº RLHF çš„åŸºç¡€çŸ¥è¯†å’Œè®­ç»ƒ LLM çš„å…·ä½“(ç®€æ˜“)ä»£ç å®ç°. </p>

<p style="text-align:justify; text-justify:inter-ideograph;">é¦–å…ˆï¼ŒRLHF åˆ†ä¸º $3$ ä¸ªéƒ¨åˆ†ï¼š<b>é¢„è®­ç»ƒ LLM æ¨¡å‹ $M_\theta$ </b>ï¼›<b>é¢„è®­ç»ƒå¥–åŠ±æ¨¡å‹ $r_\theta$ </b> ä»¥åŠ <b>ä½¿ç”¨ PPO å¾®è°ƒ $M_\theta$</b>. </p>

<p style="text-align:justify; text-justify:inter-ideograph;"><b>ç¬¬ä¸€æ­¥ï¼šé¢„è®­ç»ƒ LLM æ¨¡å‹ $M_\theta$ </b> (å°±æ˜¯ä¸€ä¸ª Transformer Decoder å‡å» cross-attention)ï¼š$M_\theta$ è¾“å…¥ä¸€ä¸ª prompt (å³è¾“å…¥çš„æ–‡æœ¬) $x \in \mathbb{R}^{L_I \times 1}$, è¾“å…¥ä¸€ä¸ª response (å³è¾“å‡ºçš„æ–‡æœ¬) $y \in \mathbb{R}^{L_O \times 1}$. å’Œæ­£å¸¸çš„é¢„è®­ç»ƒ LLM è¿‡ç¨‹ä¸€æ ·ï¼Œæ”¶é›†å¤§é‡çš„æ–‡æœ¬æ•°æ®(ç§°ä¸ºè¯­æ–™åº“ corpus)ï¼Œä½¿ç”¨ predict-next-token çš„æ–¹å¼è¿›è¡Œé¢„è®­ç»ƒ LLM æ¨¡å‹ $M_\theta$. åœ¨é¢„è®­ç»ƒå®Œæˆåï¼Œä¸ºäº†æ¨¡å‹ $M_\theta$ åœ¨ç¬¬ $3$ æ­¥æœ‰ä¸€ä¸ªæ›´å¥½çš„åˆå§‹åŒ–åŸºç¡€ï¼Œå¯ä»¥å†æ”¶é›†ä¸€äº›ç»è¿‡<b>äººä¸ºæ ‡æ³¨é«˜è´¨é‡</b>çš„ Q & A æ•°æ®è¿›è¡Œå¾®è°ƒï¼Œç§°ä¸º <b>SFT</b> (Supervised Fine Tuning). </p>

<p style="text-align:justify; text-justify:inter-ideograph;"><b>ç¬¬äºŒæ­¥ï¼šé¢„è®­ç»ƒå¥–åŠ±æ¨¡å‹ $r_\theta$</b> (å’Œ $M_\theta$ ä¸€æ ·çš„æ¶æ„)ï¼šå®ƒçš„ä½œç”¨æ˜¯æ ¹æ®äººç±»çš„å–œå¥½å¯¹ $M_\theta$ çš„è¾“å‡ºè¿›è¡Œæ‰“åˆ†(score)ï¼Œè¿™ä¸€æ­¥å°±æ˜¯éœ€è¦<b>äººå·¥æ ‡æ³¨</b>çš„æ­¥éª¤. å…·ä½“è€Œè¨€ï¼Œç»™å®š prompt $x$ï¼ŒLLM æ¨¡å‹ $M_\theta$ è¾“å‡º $y$ï¼Œé€šè¿‡äººå·¥æ ‡æ³¨è¯¥è¾“å‡ºçš„å¾—åˆ†(score $s_{GT} \in R^{1 \times 1}$)ä½œä¸º groud-truth (GT)ï¼›ç„¶åå°†ä¸¤è€…è¿›è¡Œæ‹¼æ¥å¾—åˆ° $r_\theta$ çš„è¾“å…¥ï¼Œå³ $r_\theta$ çš„è¾“å…¥ä¸º $\theta$ çš„è¾“å…¥ + è¾“å‡º $z = concat(x, y) \in \mathbb{R}^{(L_I + L_O) \times 1}$, è¾“å‡ºæ¯ä¸ªè¯çš„ score (å³ reward) $r \in \mathbb{R}^{(L_I + L_O) \times 1}$ (æˆ–è€…åªå– $y$ çš„æ¯ä¸ªè¯çš„ score: $r \in \mathbb{R}^{L_O \times 1}$, å› ä¸ºæˆ‘ä»¬ä¸éœ€è¦è¯„ä¼°éƒ¨åˆ† $x$ çš„ score)ï¼Œæœ€åå°† $s_{GT}$ å¤åˆ¶ $(L_I + L_O) \text{or} L_O$ æ¬¡å¾—åˆ° $s_{GT}^{L} \in \mathbb{R}^{(L_I + L_O) \times 1} \text{or} \mathbb{R}^{L_O \times 1}$ï¼Œå°±å¯ä»¥ä½¿ç”¨ Cross-Entropy Loss è¿›è¡Œè®­ç»ƒ. ä½†æ˜¯ä¸€èˆ¬æ¥è¯´ï¼Œäººç±»å¯¹äºç›¸å¯¹åˆ†æ•°æ¯”è¾ƒæ•æ„Ÿ(ä¾‹å¦‚è°æ¯”è°å¥½)ï¼Œè€Œå¯¹äºç»å¯¹åˆ†æ•°ä¸å¤ªæ“…é•¿(æ¯”å¦‚ç»™è°æ‰“å‡ åˆ†). å› æ­¤ï¼Œä¸€èˆ¬ä¸ä½¿ç”¨ç›´æ¥äººå·¥æ ‡æ³¨æ¥è·å¾— $s_{GT}$ï¼Œè€Œæ˜¯è®© $M_\theta$ é’ˆå¯¹ $x$ è¾“å‡ºä¸åŒçš„ $y$ ($y_0,...,y_n$)ï¼Œç„¶åè®©äººç±»å¯¹è¿™ $n$ ä¸ªè¾“å‡ºè¿›è¡Œæ’åºå¾—åˆ°ç”±ä¼˜åˆ°åŠ£çš„è¾“å‡ºé¡ºåº $y_{k_1}, ..., y_{k_n}, k_j \in {1,...,n}$ï¼Œç„¶åä½¿ç”¨é¢„å®šä¹‰çš„æ–¹æ³•(ä¾‹å¦‚ Elo rating)å°†æ’åºè½¬åŒ–ä¸ºè¾“å‡ºçš„ score, åˆ™æ¯ä¸ªè¾“å‡º $y_{k_j}$ éƒ½å¯¹åº”ä¸€ä¸ª score $s_{GT_j}$. ç„¶åå°†è¿™ $n$ ä¸ªæ•°æ®å¯¹ $\{(x, y_{k_j}), s_{GT_j}\}$ ä½œä¸ºå¥–åŠ±æ¨¡å‹ $r_\theta$ çš„è®­ç»ƒæ•°æ®. </p>

<p style="text-align:justify; text-justify:inter-ideograph;"><b>ç¬¬ä¸‰æ­¥ï¼šä½¿ç”¨ PPO ç®—æ³•å¾®è°ƒ LLM æ¨¡å‹ $M_\theta$</b>ï¼šé¦–å…ˆï¼Œéœ€è¦å°†å¾®è°ƒ(FT)é—®é¢˜è½¬åŒ–ä¸ºå¼ºåŒ–å­¦ä¹ (RL)é—®é¢˜ï¼šé¢„è®­ç»ƒå¥½çš„ LLM æ¨¡å‹ $M_\theta$ ç§°ä¸º policy model $\pi_\theta(a|s)$ (å®ƒæ¯æ¬¡è¾“å‡ºä¸€ä¸ªè¯)ï¼›é¢„è®­ç»ƒå¥½çš„å¥–åŠ±æ¨¡å‹ $r_\theta$ ç§°ä¸º reward model $r_\theta(s, a)$. è€Œæ–‡æœ¬æ•°æ®ç§°ä¸ºçŠ¶æ€(state) $s$ï¼Œåˆ™è¾“å…¥çš„æ–‡æœ¬æ•°æ® $x$ ç§°ä¸ºåˆå§‹çŠ¶æ€ $s_0$ï¼›ä¸€ä¸ªè¯ç§°ä¸ºåŠ¨ä½œ(action) $a$. æ‰€ä»¥ï¼Œåˆå§‹æ—¶ policy model ä¸º $\pi_{\theta_0}(\cdot)$ï¼Œstate ä¸º $s_0$ï¼Œå°†çŠ¶æ€ $s_0$ è¾“å…¥ $\pi_{\theta_0}(\cdot)$ å¾—åˆ°é¢„æµ‹çš„ action $a_0 = \pi_{\theta_0}(a_0|s_0)$ï¼Œç„¶åå°† $a_0$ æ‹¼æ¥åˆ°çŠ¶æ€ $s_0$ åé¢ç”Ÿæˆæ–° state $s_1$ï¼Œå¹¶å°†æ–° state $s_1$ æ¨¡å‹ $\pi_{\theta_0}(\cdot)$ ç”Ÿæˆæ–° action $a_1$. å¦‚æ­¤å¾ªç¯å¾€å¤ç”Ÿæˆ policy model $\pi_{\theta_0}(\cdot)$ é¢„æµ‹çš„ trajectory $\{(s_0, a_0), ..., (s_m, a_m)\}$ (è¿™ä¸å°±æ˜¯ LLM æ¨¡å‹çš„ predict-next-token çš„æ–¹å¼å˜›ğŸ˜€ï¼Œæ‰€ä»¥ $\pi_{\theta_0}(\cdot)=M_\theta$ï¼Œ$a_i = y_i$ï¼Œ$s_i = concat(x,y_{[1:i-L_I]})$ï¼Œ$m=L_O$ï¼Œå°±æ˜¯ä¸ºäº†å¯¹é½ PPO ç®—æ³•æ¢äº†ä¸ªåå­—è€Œå·². æ³¨æ„ï¼šæ˜¯æ¯ä¸ªè¯æ˜¯ä¸€ä¸ª actionï¼Œè€Œä¸æ˜¯ä¸€ä¸ª $y$ æ˜¯ä¸€ä¸ª actionï¼Œä¸è¦å’Œç¬¬äºŒæ­¥æ’åºæ··æ·†äº†(æˆ‘å°±ææ··äº†ğŸ˜Ÿ)). </p>

<p style="text-align:justify; text-justify:inter-ideograph;">é¢˜å¤–è¯ï¼šè¿™æ®µè¯å®å±ç»å…¸ï¼Œå»ºè®®åå¤é˜…è¯»æ¥ç†è§£å¦‚ä½•å°† FT é—®é¢˜è½¬åŒ–ä¸º RL é—®é¢˜(from hf blog)ï¼šFirst, the <b>policy</b> is a language model that takes in a prompt and returns a sequence of text (or just probability distributions over text). The <b>action space</b> of this policy is all the tokens corresponding to the vocabulary of the language model (often on the order of 50k tokens) and the <b>observation space</b> is the distribution of possible input token sequences, which is also quite large given previous uses of RL (the dimension is approximately the size of vocabulary $\times$ length of the input token sequence). The <b>reward function</b> is a combination of the preference model and a constraint on policy shift.</p>

<p style="text-align:justify; text-justify:inter-ideograph;">è€Œ PPO çš„ä¼˜åŒ–å‡½æ•°ä¸ºï¼š</p>

$L(s,a,\theta_k,\theta)=\min\left(\frac{\pi_\theta(a|s)}{\pi_{\theta_k}(a|s)}A^{\pi_{\theta_k}}(s,a),\operatorname{clip}\left(\frac{\pi_\theta(a|s)}{\pi_{\theta_k}(a|s)},1-\epsilon,1+\epsilon\right)A^{\pi_{\theta_k}}(s,a)\right)$

$\begin{aligned}&\delta_{t}=r_{t}+\gamma V(s_{t+1})-V(s_{t})\\&\hat{A}_{t}=\delta_{t}+\gamma\lambda\hat{A}_{t+1}\end{aligned}$

<p style="text-align:justify; text-justify:inter-ideograph;">é‚£ä¹ˆ $V(\cdot)$ å¦‚ä½•å®ç°ï¼Ÿ</p>

$\begin{aligned}&\delta_{t}=r_{t}+\gamma V(s_{t+1})-V(s_{t})\\&\hat{A}_{t}=\delta_{t}+\gamma\lambda\hat{A}_{t+1}\end{aligned}$

![PPO Clip](/images//RLHF_PPO-Clip.png)

$\theta_{k+1}=\arg\max_\theta\sup_{s,a\sim\pi_{\theta_k}}\left[L(s,a,\theta_k,\theta)\right], \\
L(s,a,\theta_k,\theta)=\min\left(\frac{\pi_\theta(a|s)}{\pi_{\theta_k}(a|s)}A^{\pi_{\theta_k}}(s,a),\operatorname{clip}\left(\frac{\pi_\theta(a|s)}{\pi_{\theta_k}(a|s)},1-\epsilon,1+\epsilon\right)A^{\pi_{\theta_k}}(s,a)\right),$

$L(s,a,\theta_k,\theta)=\min\left(\frac{\pi_\theta(a|s)}{\pi_{\theta_k}(a|s)}A^{\pi_{\theta_k}}(s,a),g(\epsilon,A^{\pi_{\theta_k}}(s,a))\right), \left.g(\epsilon,A)=\left\{\begin{array}{ll}(1+\epsilon)A&A\geq0\\(1-\epsilon)A&A<0.\end{array}\right.\right.$

Advantage is positive: $L(s,a,\theta_k,\theta)=\min\left(\frac{\pi_\theta(a|s)}{\pi_{\theta_k}(a|s)},(1+\epsilon)\right)A^{\pi_{\theta_k}}(s,a).$

Advantage is negative: $L(s,a,\theta_k,\theta)=\max\left(\frac{\pi_\theta(a|s)}{\pi_{\theta_k}(a|s)},(1-\epsilon)\right)A^{\pi_{\theta_k}}(s,a).$