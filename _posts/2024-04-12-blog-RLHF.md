---
title: 'The Basic Knowledge of RLHF (Reinforce Learning with Human Feedback)'
date: 24-04-12
update: 24-04-12
permalink: /posts/2024/04/blog-rlhf/
star: superior
tags:
  - 深度学习基本知识
---

<p style="text-align:justify; text-justify:inter-ideograph;">这篇博客主要讲解关于 RLHF 的基础知识和训练 LLM 的具体(简易)代码实现. </p>

<p style="text-align:justify; text-justify:inter-ideograph;">首先，RLHF 分为 $3$ 个部分：<b>预训练 LLM 模型 $M_\theta$ </b>；<b>预训练奖励模型 $r_\theta$ </b> 以及 <b>使用 PPO 微调 $M_\theta$</b>. </p>

<p style="text-align:justify; text-justify:inter-ideograph;"><b>第一步：预训练 LLM 模型 $M_\theta$ </b> (就是一个 Transformer Decoder 减去 cross-attention)：$M_\theta$ 输入一个 prompt (即输入的文本) $x \in \mathbb{R}^{L_I \times 1}$, 输入一个 response (即输出的文本) $y \in \mathbb{R}^{L_O \times 1}$. 和正常的预训练 LLM 过程一样，收集大量的文本数据(称为语料库 corpus)，使用 predict-next-token 的方式进行预训练 LLM 模型 $M_\theta$. 在预训练完成后，为了模型 $M_\theta$ 在第 $3$ 步有一个更好的初始化基础，可以再收集一些经过<b>人为标注高质量</b>的 Q & A 数据进行微调，称为 <b>SFT</b> (Supervised Fine Tuning). </p>

<p style="text-align:justify; text-justify:inter-ideograph;"><b>第二步：预训练奖励模型 $r_\theta$</b> (和 $M_\theta$ 一样的架构)：它的作用是根据人类的喜好对 $M_\theta$ 的输出进行打分(score)，这一步就是需要<b>人工标注</b>的步骤. 具体而言，给定 prompt $x$，LLM 模型 $M_\theta$ 输出 $y$，通过人工标注该输出的得分(score $s_{GT} \in R^{1 \times 1}$)作为 groud-truth (GT)；然后将两者进行拼接得到 $r_\theta$ 的输入，即 $r_\theta$ 的输入为 $\theta$ 的输入 + 输出 $z = concat(x, y) \in \mathbb{R}^{(L_I + L_O) \times 1}$, 输出每个词的 score (即 reward) $r \in \mathbb{R}^{(L_I + L_O) \times 1}$ (或者只取 $y$ 的每个词的 score: $r \in \mathbb{R}^{L_O \times 1}$, 因为我们不需要评估部分 $x$ 的 score)，最后将 $s_{GT}$ 复制 $(L_I + L_O) \text{or} L_O$ 次得到 $s_{GT}^{L} \in \mathbb{R}^{(L_I + L_O) \times 1} \text{or} \mathbb{R}^{L_O \times 1}$，就可以使用 Cross-Entropy Loss 进行训练. 但是一般来说，人类对于相对分数比较敏感(例如谁比谁好)，而对于绝对分数不太擅长(比如给谁打几分). 因此，一般不使用直接人工标注来获得 $s_{GT}$，而是让 $M_\theta$ 针对 $x$ 输出不同的 $y$ ($y_0,...,y_n$)，然后让人类对这 $n$ 个输出进行排序得到由优到劣的输出顺序 $y_{k_1}, ..., y_{k_n}, k_j \in {1,...,n}$，然后使用预定义的方法(例如 Elo rating)将排序转化为输出的 score, 则每个输出 $y_{k_j}$ 都对应一个 score $s_{GT_j}$. 然后将这 $n$ 个数据对 $\{(x, y_{k_j}), s_{GT_j}\}$ 作为奖励模型 $r_\theta$ 的训练数据. </p>

<p style="text-align:justify; text-justify:inter-ideograph;">首先，预训练好的 LLM 模型 $M_\theta$ 称为 policy model $\pi_\theta(a|s)$ (也就是输出的词的最大概率), 预训练好的奖励模型 $r_\theta$ 称为 reward model. 而</p>

<p style="text-align:justify; text-justify:inter-ideograph;">注意：每个 action 是一个词，因此对于一个输入 $x$，policy model $\theta$ 预测的 trajectory 就是 $y$. 而 PPO 的优化函数为：</p>

$L(s,a,\theta_k,\theta)=\min\left(\frac{\pi_\theta(a|s)}{\pi_{\theta_k}(a|s)}A^{\pi_{\theta_k}}(s,a),\operatorname{clip}\left(\frac{\pi_\theta(a|s)}{\pi_{\theta_k}(a|s)},1-\epsilon,1+\epsilon\right)A^{\pi_{\theta_k}}(s,a)\right)$

$\begin{aligned}&\delta_{t}=r_{t}+\gamma V(s_{t+1})-V(s_{t})\\&\hat{A}_{t}=\delta_{t}+\gamma\lambda\hat{A}_{t+1}\end{aligned}$

<p style="text-align:justify; text-justify:inter-ideograph;">那么 $V(\cdot)$ 如何实现？</p>

<p style="text-align:justify; text-justify:inter-ideograph;">First, the policy is a language model that takes in a prompt and returns a sequence of text (or just probability distributions over text). The action space of this policy is all the tokens corresponding to the vocabulary of the language model (often on the order of 50k tokens) and the observation space is the distribution of possible input token sequences, which is also quite large given previous uses of RL (the dimension is approximately the size of vocabulary ^ length of the input token sequence). The reward function is a combination of the preference model and a constraint on policy shift.</p>

$\begin{aligned}&\delta_{t}=r_{t}+\gamma V(s_{t+1})-V(s_{t})\\&\hat{A}_{t}=\delta_{t}+\gamma\lambda\hat{A}_{t+1}\end{aligned}$

![PPO Clip](/images//RLHF_PPO-Clip.png)

$\theta_{k+1}=\arg\max_\theta\sup_{s,a\sim\pi_{\theta_k}}\left[L(s,a,\theta_k,\theta)\right], \\
L(s,a,\theta_k,\theta)=\min\left(\frac{\pi_\theta(a|s)}{\pi_{\theta_k}(a|s)}A^{\pi_{\theta_k}}(s,a),\operatorname{clip}\left(\frac{\pi_\theta(a|s)}{\pi_{\theta_k}(a|s)},1-\epsilon,1+\epsilon\right)A^{\pi_{\theta_k}}(s,a)\right),$

$L(s,a,\theta_k,\theta)=\min\left(\frac{\pi_\theta(a|s)}{\pi_{\theta_k}(a|s)}A^{\pi_{\theta_k}}(s,a),g(\epsilon,A^{\pi_{\theta_k}}(s,a))\right), \left.g(\epsilon,A)=\left\{\begin{array}{ll}(1+\epsilon)A&A\geq0\\(1-\epsilon)A&A<0.\end{array}\right.\right.$

Advantage is positive: $L(s,a,\theta_k,\theta)=\min\left(\frac{\pi_\theta(a|s)}{\pi_{\theta_k}(a|s)},(1+\epsilon)\right)A^{\pi_{\theta_k}}(s,a).$

Advantage is negative: $L(s,a,\theta_k,\theta)=\max\left(\frac{\pi_\theta(a|s)}{\pi_{\theta_k}(a|s)},(1-\epsilon)\right)A^{\pi_{\theta_k}}(s,a).$