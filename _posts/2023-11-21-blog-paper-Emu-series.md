---
title: 'Emu series (Emu & Emu Edit & Emu Video)'
date: 23-11-21
permalink: /posts/2023/11/blog-paper-emu-series/
tags:
  - 论文阅读
---

<p style="text-align:justify; text-justify:inter-ideograph;">本文主要对近期 Meta 发表的三篇关于视觉处理的文章(Emu 系列)进行论文解读(按照它们的发布顺序)：
首先是 SOTA 的 text-to-image 生成模型 Emu；接着以它为 baseline，进行 image edit 的研究改进，提出了一个大一统的图像编辑模型 Emu Edit，
这基本上就把图像领域主流的任务都刷了个遍。最后又提出了 Emu Video 模型，利用 Emu 完成了对 text-to-video 生成模型的改进，也获得了 SOTA。
(ps：我猜下一步应该就是 video edit 的研究改进了🙂)</p>

<p style="text-align:justify; text-justify:inter-ideograph;"> 论文题目：<a href="https://arxiv.org/abs/2309.15807" target="_blank" title="Emu">Emu: Enhancing Image Generation Models Using Photogenic Needles in a Haystack</a></p>

<p style="text-align:justify; text-justify:inter-ideograph;"> 论文题目：<a href="https://arxiv.org/abs/2311.10089" target="_blank" title="Emu Edit">Emu Edit: Precise Image Editing via Recognition and Generation Tasks</a></p>

<p style="text-align:justify; text-justify:inter-ideograph;"> 论文题目：<a href="https://arxiv.org/abs/2311.10709" target="_blank" title="Emu Video">Emu Video: Factorizing Text-to-Video Generation by Explicit Image Conditioning</a></p>

<p style="text-align:justify; text-justify:inter-ideograph;">发表会议：三篇论文都发表在 Conference of Computer Vision and Pattern Recognition (CVPR 2023)</p>

<p style="text-align:justify; text-justify:inter-ideograph;">第一作者：Xiaoliang Dai & Shelly Sheynin & Rohit Girdhar (GenAI, Meta)</p>

<h2>Emu</h2>

<h3>Question</h3>

<h2>Emu Edit</h2>

<h2>Emu Video</h2>

